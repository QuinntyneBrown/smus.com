<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"
      xml:lang="en"
      xml:base="http://smus.com">
  <title>Boris Smus</title>
  <link href="http://smus.com/atom.xml" rel="self"/>
  <link href="http://smus.com"/>
  <updated>2015-11-04T10:39:55-00:00</updated>
  <id>http://smus.com/atom.xml</id>
  
  <entry>
    <title>Hot bread: delicious or deadly?</title>
    <author><name>Boris Smus</name></author>
    <link href="/hot-bread-delicious-deadly"/>
    
    <updated>2015-09-23T09:00:00-00:00</updated>
    
    <id>http://smus.com/hot-bread-delicious-deadly</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Despite free access to information via the Internet and an increasingly global
world, people still seem to have all sorts of divergent ideas about how the world
works. For example, did you know that eating hot bread and pastries is
incredibly unhealthy? Indeed, it can often even lead to complete bowel
obstruction! I learned this fact as a kid, while growing up in the Soviet Union.
Understandably, I have been very careful to avoid eating hot baked goods.
That is, until recently, when my American girlfriend questioned the validity of my
belief and I began to harbor some doubts. I decided to check if it was actually
true, and asked Google. The results were very clear: I had fallen prey to an old
wives tale. My worldview, shattered.</p>

<p>Incredulous, I searched for the same thing in Russian and arrived at the
opposite conclusion. "What's up with that?" I thought, and wrote this post.</p>

<!--more-->

<h2>Asking in different languages</h2>

<p>I searched Google for "hot bread unhealthy", and tallied up the top 5 results:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>davidwalbert.com</td><td>No</td></tr>
<tr><td>chestofbooks.com</td><td>Maybe</td></tr>
<tr><td>gurumagazine.org</td><td>For some</td></tr>
<tr><td>lthforum.com</td><td>No</td></tr>
<tr><td>answers.yahoo.com</td><td>No</td></tr>
</table>

<p>I then compared it to an equivalent Russian search string: "горячий хлеб
вреден". The following are my results in English:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>useful-food.ru</td><td>Yes</td></tr>
<tr><td>foodblogger.ru</td><td>Yes</td></tr>
<tr><td>hlebopechka.ru</td><td>Yes</td></tr>
<tr><td>otvet.mail.ru</td><td>Maybe</td></tr>
<tr><td>otvet.mail.ru</td><td>Yes</td></tr>
</table>

<p>My <a href="https://goo.gl/ltPefm">working spreadsheet</a> contains more colorful details
if you are interested.</p>

<h2>Language shapes your... search results?</h2>

<p>No English language site suggested that eating hot bread was unhealthy. Three of
the top five results explicitly point it out as an old wives tale. The first hit,
<a href="http://goo.gl/Cj9jKS">the most skeptical of the bunch</a> even cites articles from
the 18th and 19th centuries which have since been refuted.</p>

<p>In stark contrast, no Russian language site suggested that eating fresh
bread was totally fine. Four of five of the top results explicitly said that it
was unhealthy, suggesting that fresh bread is difficult to digest, encourages
swallowing without chewing, and eating it leads to all sorts of gastrointestinal
trouble like stomach pain, inflammation, constipation and full on bowel
obstruction. Oh my!</p>

<p>One possibility is that the environments of the Russian and English speaker are
in fact completely different. The bread making processes in Russia could differ
from other places in the world. Many Russians favor rye bread, which takes some
effort to find in North America, for example. The main reason for unhealthiness
of fresh bread seems to be related to it being undercooked, with the yeast still
being active until it cools. Maybe rye better protects the yeast, or takes less
time or heat to cook? </p>

<p>This and other theories are possible, though not likely. My intuition suggests a
simpler explanation.</p>

<h2>Nothing Is True and Everything Is Possible</h2>

<p>There is an expression in Russian: "умом Россию не понять", which roughly
translates as "Russia cannot be understood with the mind". There is a certain 
mystery deeply ingrained in the national character which, fascinatingly, has
always been a point of pride.  The heading of this section is actually taken
from the title of a <a href="http://www.amazon.com/Nothing-Is-True-Everything-Possible/dp/1610394550">book about modern Russia</a>, subtitled "The Surreal
Heart of the New Russia". In Russia, rationalism and skepticism is on the
decline, in favor of traditionalism and magical thinking. Given a rich tradition
of <a href="https://en.wikipedia.org/wiki/Russian_traditions_and_superstitions">traditions, superstitions, and beliefs</a> in Russian culture, there is a
large pool of absurdity to pick from.</p>

<p>Given that, and my recent search history, you can imagine what I now believe
about the harmful effects of eating freshly baked bread. I don't much care
whether or not eating fresh bread is healthy, especially since as a card
carrying Celiac, I can't even enjoy the delicious kind. The fascinating
conclusion from my multilingual sojourn is this:</p>

<p><strong>Having searched for the same thing in their native languages, a Russian
speaker and an English speaker would have arrived at a completely different
world-view.</strong></p>

<p>The Russian language <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_Russian_is_an_official_language">maps closely</a> to Russia and Russian culture, certainly
more so than <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_English_is_an_official_language">English does</a> to any particular country and culture. The
result is that queries in Russian are suspect to a very natural echo chamber,
echoing and amplifying deeply held beliefs with the help of our supposedly
normalizing open Internet. </p>

<h2>Translated foreign pages</h2>

<p>There are hundreds of other examples of queries that when translated will yield
dramatically different results much like "hot bread unhealthy"/"горячий хлеб
вреден". There's a simple formula for finding more. Pick a language and write a
query string, translate it into another language, perform both searches and
analyze the top results.</p>

<p>This sounds a lot like something that can be automated. Indeed, Google used to
automatically translate queries, perform searches with translated queries, and
surface them to the user. Unfortunately this "Translated foreign pages" feature
was <a href="https://productforums.google.com/forum/#!topic/websearch/tYo0LpcVobI/discussion">removed several years ago</a>, due to lack of usage. Also, there are
difficulties with automating the process. The Google Translation of "hot bread
unhealthy" is "горячий хлеб нездоровый", which in Russian sounds like the bread
itself is ill, and yields less relevant search results.</p>

<p>It's surprising how clearly this cultural difference can be seen through the
simple example of warm bread and a search engine. The initial surprise can be
easily explained though, since the search engine crawls a naturally insular
corpus of articles in the same language. Many of the search results in English
cite the same sources. The same is true for search results in Russian. The key
point, though, is that there is very little shared linking between the English
and Russian sites, especially since <a href="https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population">only 5% of Russians speak English</a>.
The language corpuses seem to be almost completely insulated from one another.
Inevitably, confirmation bias kicks in and you end up with the polarized world
we live in today.</p>

<p>I'd love to see what similar analyses on other search queries. For instance,
there is a Russian gadget called a <a href="http://www.amazon.com/Dark-Blue-Lamp-Minin-Reflector/dp/B00RPG6UTW">Minin Reflector</a>, which consists of a
lamp with a blue filter. You simply shine it onto the part of your body that
ails you, and presto, instant pain relief... sigh!</p>

<p>Wrapping up this blog, I am enjoying some delicious, fresh from the oven, hot
muffins. I'll keep you posted with the definitive truth!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UbiComp and ISWC 2015</title>
    <author><name>Boris Smus</name></author>
    <link href="/ubicomp-iswc-2015"/>
    
    <updated>2015-09-16T09:00:00-00:00</updated>
    
    <id>http://smus.com/ubicomp-iswc-2015</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I recently returned from ISWC 2015, where I presented the <a href="/magnetic-input-mobile-vr/">Cardboard
Magnet</a> paper. In addition to seeing old friends,
meeting new ones, and being inspired by some interesting research, it was an
excellent excuse to visit Osaka, Japan! This year, ISWC was co-located with
UbiComp, and the combined conference had four tracks. This post is by no means
exhaustive, just some of the more interesting work I got a chance to see.</p>

<!--more-->

<p><strong>Opening keynote: Visualizing and Manipulating Brain Dynamics</strong>. <a href="http://www.cns.atr.jp/~kawato/">Mitsuo
Kawato</a> showed some impressive <a href="https://ieeetv.ieee.org/conference-highlights/cb-exploring-neuroscience-withhumanoid-research-platform?">self-balancing
robots</a>.
It seems that we've <a href="http://tumblr.forgifs.com/post/111425301004/robot-soccer-kick-fail">come a long
way</a>. Most
of what he showed was around deep brain stimulation, artificial cochleas and
retinas, old work but mostly new to me. This is a recent, very impressive and
somewhat terrifying paper on reconstructing low-resolution grayscale <a href="http://neurosurgery.washington.edu/Lectures/science.1234330.full.pdf">imagery
from
dreams</a>.</p>

<h2>Novel input technology</h2>

<p><strong>SoQr: Sonically Quantifying the Content Level inside Containers</strong> is a
convoluted way of determining if you're out of milk. Inspired by acoustically
checking ripeness of watermellons, the idea is to use <a href="https://goo.gl/photos/BVhjSZXyn7MJV2sv7">contact speaker and mic
pair</a> to determine how full a container
is. The method's efficacy depends a lot on the placement of the sensor,
properties of the container, and other environmental factors, like whether or
not any other items are touching the container. Seems overly complex to me, you
could use another approach to reach higher fidelity (eg. a scale). That said,
maybe this can be done very inexpensively?</p>

<p><strong>MagnifiSense: Inferring Device Interaction Using Wrist-Worn Passive
Magneto-Inductive Sensors</strong> is about determining which electronic device is
being used. The idea is to use an inductor coil to detect nearby electromagnetic
radiation. They built their own hardware for the purpose which samples at a very
high frequency (44.1 KHz). They detect <a href="https://goo.gl/photos/RQXQ8fBJpJU4f8wn6">unique EM radiation
patterns</a> for each type of device.
Supposedly they can do the same using a regular smartphone magnetometer, but I'm
very skeptical. They also claim to be able to determine who is using the device,
but that part wasn't very clear from the talk.</p>

<p><strong>DoppleSleep: A Contactless Unobtrusive Sleep Sensing System Using Short-Range
Doppler Radar</strong> uses a 24 GHz doppler radar typically mounted near the bedside
to detect sleep patterns. The benefits are huge: you don't have to wear anything
or instrument the bed. <a href="https://en.wikipedia.org/wiki/Polysomnography">Medical sleep
trackers</a> require
7 electrodes, and <a href="https://en.wikipedia.org/wiki/Actigraphy">consumer ones</a>
don't work well.  They also had a demo where you just sit at your desk and the
doppler tracks your heart rate, breathing rate, as well as more macro
movements. It didn't work as well as I had hoped, but being a research demo, I
remain hopeful!</p>

<p><strong>Activity tracking and indoor positioning with a wearable magnet</strong> was a poster
showing a very cheap way of tracking just by placing magnetometers in strategic
locations and giving the user a magnet. More details <a href="https://goo.gl/photos/2un4nc5nrE7evC4D8">on the
poster</a>.</p>

<p><strong>IDyLL: Indoor Localization using Inertial and Light Sensors on Smartphones</strong>
attempts to solve indoor localization using the light sensor, and lamps as
features for tracking. They extract features from the lights using peak finding,
not absolute intensity. Then they wrote a kalman filter to fuse the IMU and
light-derived features. There's a lot of problems, like needing to have
structured light (eg. in a hallway with a low ceiling), and identify ambiguity
(ie. you're under a light, but which one?).</p>

<p><strong>Monitoring Building Door Events using Barometer Sensor in Smartphones</strong> used
the ubiquitous smartphone barometer, which is currently used to get faster GPS
lock and assist in weather forecasting, to determine if a door opens in a
building. This only works in buildings with HVAC systems, but it was pretty
clever, and they found that it can work reliably, even for multiple doors. Basic
idea <a href="https://goo.gl/photos/yKPbBYPv2RayMrcf6">described in this slide</a>.</p>

<p><strong>ProximityHat - A Head-Worn System for Subtle Sensory Augmentation with Tactile
Stimulation</strong> reminded me of various <a href="http://www.cc.gatech.edu/~acosgun3/papers/cosgun2014guidance.pdf">vibro-tactile belt
projects</a>, and
served a similar purpose: to exploit the sense of touch to give the wearer
another sense. This has many benefits like not blocking other senses. Anyway,
they built a hat and gave it ultrasonic sensors all around and inward-facing
linear actuators, not vibrator motors.  They studied sensitivity around the head
and found high variation around users, and that the forehead was generally less
sensitive. Main application appears to be navigation, and they did some blind
user studies.</p>

<p><strong>Controlling Stiffness with Jamming for Wearable Haptics</strong> makes it easier and
harder to move sliders with the help of a pneumatic bladder, and layered
material. As the bladder inflates, the additional force on the layered material
causes increased friction. Previous layer jamming had low fidelity (binary), so
this is a big improvement. They are currently using sandpaper, so it's unclear
how robust the effect would be over time.</p>

<p><strong>PneuHaptic: Delivering Haptic Cues with a Pneumatic Armband</strong> used a wearable
<a href="https://goo.gl/photos/XkFSKrbRmpiZgzCu9">pneumatic band with 2 pumps and 3
valves</a> to give haptic feedback. This
is a nice alternative to vibrating motors and linear actuators, but not sure how
miniaturizable in practice.</p>

<p><strong>Fast Blur Removal for Wearable QR Code Scanners</strong> is an image processing paper
for improving QR code detection on wearable devices. The proposed method uses
un-blurring techniques which involve predicting the blur direction and applying
de-convolutions. They also use an IMU to better guess the direction of movement.
However <a href="http://picturesofpeoplescanningqrcodes.tumblr.com/">QR codes are dead to
me</a>.</p>

<h2>Gadgets and fads</h2>

<p><strong>Why we use and abandon smart devices</strong> tried to answer the question of why
people abandon their various health and tracking devices so quickly. Basically,
people are motivated by curiosity and novelty, and these health trackers are too
gimmicky. Studies of Fitbit trackers saw majority of them abandoned (65%
abandoned in 2 weeks). Design implications are that encouraging routines
(changing behavior) and minimizing maintenance (charging) are the critical
things.This study had participants come up with a goal, and $1K to buy devices,
so quite contrived given that people didn't even choose to use the devices on
their own, but motivated by a study.</p>

<p>In a less contrived study about the same thing, <strong>No Longer Wearing:
Investigating the Abandonment of Personal Health-Tracking Technologies on
Craigslist</strong> scraped Craigslist for this data. They found that only 25% of
people sell their devices just for abandonment reasons. In many other cases,
they upgrade to something else, or reach their goals. That said, it's very
biased sample, since these people are selling (many just abandon, and don't sell
on CL).</p>

<h2>Machine learning</h2>

<p><strong>DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic
Environments Using Deep Learning</strong> used RNNs to learn whatever sound the user is
interested in. They did an interesting comparison to similar specialized systems
(eg. those that do speaker identification, stress detection, emotion, etc) and
claim to do better. Also, their RNN runs in hardware on a chip, which I thought
was super impressive.</p>

<p>In <strong>Sensor-based Stroke Detection and Stroke Type Classification in Table
Tennis</strong>, the authors instrumented paddles with IMUs and got people to perform
various strokes (in a somewhat controlled environment). They performed stroke
detection through peak recognition and thresholding, and then had a classifier
for stroke type determination. 97% detection and classification rates!
Impressive, but contrived. Wondering how it would do for a full game?</p>

<p><strong>Recognizing New Activities with Limited Training Data</strong> was an interesting
paper about recognizing new activities based on small amounts of labeled data.
Their idea was to leverage "semantic attributes" from core activities to learn
a new activity.  Example: biking is like sitting (body is not changing angle),
running (legs move up and down) and driving (hands are steering). They proposed
an <a href="https://goo.gl/photos/Rn2BbvQjhU3Lhv1K7">Activity-Attribute matrix</a>, and a
cascaded classifier. Problem is that multiple activities can share the same
attributes. So they combine this with a traditional approach.</p>

<p><strong>When Attention is not Scarce - Detecting Boredom from Mobile Phone Usage</strong>
predicted boredom with higher accuracy than I predicted. They collected a ground truth
of boredom data by polling users multiple times a day, asking if they were
bored, and collected activity traces (semantic location, demographics, network
usage, recent number of notifications, sensor data). They managed to detect
boredom with 73% accuracy. They then built an app which sent buzzfeed articles
when bored and compared engagement and click ratio to the random condition.
CTR was 8% for random, 20% when bored, and people were much more engaged.</p>

<h2>Virtual and augmented reality</h2>

<p><strong>Wearing Another Personality: A Human-Surrogate System with a Telepresence
Face</strong> was probably the most bizarre paper at the conference. This work
basically proposes to use a human surrogate instead of a telepresence robot. The
surrogate wears an HMD with pass-through camera feed and a tablet on their face.
The tablet shows the face of the director. The director gets audio and video
feed from the surrogate, and the surrogate gets audio instructions from the
director. They did creepy user studies like going to a city office to get a
public document (friend as surrogate), or meeting your grandmother (mother as
surrogate). Surprisingly, many participants liked being surrogates. The big
technical problem is camera pass through latency. If you go through the whole
Java stack it's something crazy like 300ms. Here's a <a href="https://goo.gl/photos/tZPoR4wvQiDekzg86">video from the
conference</a> to give you a better sense.</p>

<p><strong>Comparing Order Picking Assisted by Head-Up Display versus Pick-by-Light with
Explicit Pick Confirmation</strong> compared two order picking methods in warehouses.
The current method is via digital labels on each tray that count how many items
you're supposed to take from that tray. The new method is to show <a href="https://www.youtube.com/watch?v=yUZFaCP6rP4">which trays
to pick from using augmented
reality</a>. The benefit is that you
don't need an instrumented warehouse, so it's much cheaper. This was interesting
because it was a specific, potentially useful application for a Google
Glass-type device. At the same time, it may be an obsolete problem since aren't
robots supposed to automate that sort of thing pretty soon?</p>

<p><strong>ConductAR: An AR based tool for iterative design of conductive ink circuits</strong>
is a project that validates hand drawn circuits using augmented reality. You
sketch your circuit with a conductive pen, and then the tool takes a picture and
gives you the right voltage drops etc. The presenter showed resistance
calculation (the thicker the line, the more resistive), using <a href="https://goo.gl/photos/wLaMQZqoqf66ap477">a FEM
method</a>. But I wasn't convinced that
this is worthwhile. Sketching circuits should be exploratory and does not need
to be precise, that's sort of the point.</p>

<p><strong>An Approach to User Identification for Head-Mounted Displays</strong> uses blink and
head movements to identify users. They play a particular video and track your
patterns using Google Glass. They extract blinks using IR peaks, and head track
using the IMU. It takes about 30s to verify uniqueness, but not sure how large
their user base is. Results are good: 94% balanced accuracy, and blink features
are most important.</p>

<p><strong>Glass-Physics: Using Google Glass for Physics Experiments</strong> compared using
Google Glass to just a tablet for assisting students doing physics experiments.
The idea is to remove drudgery from data collection. The experiment was to
determine <a href="https://goo.gl/photos/kcxvxouYtDs6CfQv7">effect of fill level in a water glass on frequency of
sound</a> when the vessel was hit with a
fork. A Google Glass app did automatic collection of frequency and of water
level. People liked the wearable version more, but the tablet app involved
manual input. My theory is that a tablet app with AR features to auto-measure
fill level would do as well as an HMD.</p>

<p><strong>WISEglass: Multi-purpose Context-aware Smart Eyeglasses</strong> was like Google
Glass, except without the display. The main contribution was a light sensor on
the bridge of the nose, which could reliably determine when you are at the
computer (from the screen update frequency). Other than that, seems pretty much
the same as wearing an IMU anywhere else (eg. smartwatch).</p>

<h2>E-textiles are impressive</h2>

<p>I saw some nice demos of <a href="https://goo.gl/photos/cui8ucmGYjdXq2tj9">stretch-sensitive fabric
(video)</a>, and <a href="https://goo.gl/photos/HbXp9xK2GtBgcoEo8">pressure/capacitative
fabric (video)</a>. The real question is
where to embed the controller, and what to do about battery life (their stats
were pretty bad). E-textiles are interesting because everybody wears clothing,
which is not true for glasses or watches.</p>

<p><strong>Closing keynote: Behind the scenes</strong> delivered by <a href="http://www.daito.ws/en/">Daito
Manabe</a> was sequentially translated, which was
initially jarring, but the talk was so visually stimulating, it didn't really
matter. Daito walked through a lot of his data arts work, mind-blowingly
impressive art pieces involving drones, 3D graphics, depth cameras, etc. A nice,
if somewhat non-sequitur ending to the conference.</p>

<p>Signing off. Arigatou gozaimasu!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Magnetic Input for Mobile VR</title>
    <author><name>Boris Smus</name></author>
    <link href="/magnetic-input-mobile-vr"/>
    
    <updated>2015-09-07T09:00:00-00:00</updated>
    
    <id>http://smus.com/magnetic-input-mobile-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's easy to do, just follow these steps:</p>

<ol>
<li>Cut two holes in a box</li>
<li>Put your phone in that box</li>
<li>Look inside the box</li>
</ol>

<p><a href="https://www.youtube.com/watch?v=ABrSYqiqvzc&amp;t=1m42s">And that's the way you do it</a>.</p>

<p>Your smartphone is now in a box, so how do you do input? Now that we have a
<a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">paper</a> accepted to <a href="http://iswc.net/">ISWC 2015</a>, I can tell you!</p>

<!--more-->

<h2>It's not easy being in a box</h2>

<p>Let me remind you: your smartphone is still in a box. This means that your
fingers can't reach the touch screen or the volume buttons. Let's consider a
couple of input alternatives:</p>

<ul>
<li>Cameras and microphones require extra app permissions, are inefficient to keep
always on, and may face many false positives.</li>
<li>External electronic devices cost money. Plugging them in and out is a
usability nightmare.</li>
</ul>

<p>Ok, let's nix those. How about permanent magnets? They are inexpensive, robust,
require no power to operate, and do not degrade over time. The vast majority of
smartphones have a magnetometer, which is used for the compass. Intriguing...</p>

<h2>Fun with permanent magnets</h2>

<p>In 2009, Chris Harrison and Scott Hudson published <a href="http://www.chrisharrison.net/index.php/Research/Abracadabra">Abracadabra</a>, a
magnetic ring form factor for finger interactions with small devices:</p>

<p><img src="/magnetic-input-mobile-vr/abra.jpg" alt="Abracadabra" /></p>

<p>In 2010, Hamed Ketabdar and others published <a href="https://www.facebook.com/MagiTact">Magitact</a>, instead using
a magnetic rod for more varied interactions near smartphones:</p>

<p><img src="/magnetic-input-mobile-vr/magitact.jpg" alt="Magitact" /></p>

<p>In 2011, Daniel Ashbrook and others published <a href="http://dl.acm.org/citation.cfm?id=1979238">Nenya</a>, which is similar
to Abracadabra, but focused more on the eyes-free input aspects:</p>

<p><img src="/magnetic-input-mobile-vr/nenya.jpg" alt="Nenya" /></p>

<p>In 2013, Sungjae Hwang and others published <a href="https://www.youtube.com/watch?v=_sSgp0hD-jk">Maggetz</a>, which used
passive magnets to build all sorts of widgets around the device:</p>

<p><img src="/magnetic-input-mobile-vr/maggetz.jpg" alt="Maggetz" /></p>

<h2>Fucking magnets: how do they work?</h2>

<p>Magnets affect the magnetometer in an <a href="https://www.quora.com/Why-does-the-magnetic-field-obey-an-inverse-cube-law">inverse-cubic relationship</a>, so
distance between magnet and magnetometer really makes a dramatic difference in
signal strength. We empirically determined that in most phones, the sensor is
placed at the top of the device, near the earpiece:</p>

<table>
<tr><th>Smartphone Model</th><th>Sensor Location</th></tr>
<tr><td>Moto X</td><td>Top</td></tr>
<tr><td>Nexus 4</td><td>Top</td></tr>
<tr><td>Nexus 5</td><td>Top</td></tr>
<tr><td>Samsung S4</td><td>Top</td></tr>
<tr><td>Galaxy Nexus</td><td>Top</td></tr>
<tr><td>Samsung S3</td><td>Bottom</td></tr>
<tr><td>Moto G</td><td>Bottom</td></tr>
</table>

<p>Some magnetometers are really screwy, like the one found in the first revision
of the HTC M7, or broken, like in some models of the Galaxy Nexus we tested
with. There's little that we can do in these cases, but luckily they are quite
rare.</p>

<p><img src="/magnetic-input-mobile-vr/calibration.png" class="floatright" title="Plot of calibration events"/></p>

<p>The way you access the magnetometer on Android is via the sensor stack,
requesting the <code>TYPE_MAGNETIC_FIELD</code> sensor. This is a calibrated sensor, since
it's primarily used to determine the direction of magnetic north for the
compass. Calibration means that somewhere deep inside Android, software and
hardware periodically calibrates the output of the sensor. When calibration
occurs, magnetometer readings effectively reset to some new coordinate system.</p>

<p>Calibration can happen at any point, and the calibration pattern can look quite
different depending on the device. In some cases, it's a gradual calibration,
not a sudden spike as above. This limitation restricts what we can reliably
detect, which is why we chose a pull-and-release interaction. Android already
has provisions for an uncalibrated magnetometer via
<code>TYPE_MAGNETIC_FIELD_UNCALIBRATED</code>, but this sensor is not nearly as ubiquitous
as its calibrated cousin. Even so, we should be robust to phone insertions and
removals from Cardboard, which can also look like calibration events.</p>

<h2>Magnetic input for VR</h2>

<iframe width="853" height="480" src="https://www.youtube.com/embed/a53a-9FLdL8" frameborder="0" allowfullscreen></iframe>

<p><img src="/magnetic-input-mobile-vr/mechanism.png" class="floatright" title="Interaction mechanism"/></p>

<p>As you can see, the interaction involves pulling the magnet downward, and
releasing it. The magnetic ring automatically returns to its rest position
because of the force exerted on it by an internal magnet. The external magnet is
also held in-place by the same force, and while it's possible to pull the magnet
off the cardboard side, it takes concerted effort to do so. The motion of the
magnet is constrained by a cardboard indentation, so it can only move downward.
The thing I find most elegant about this design is that both the digital signal
to the smartphone and the physical mechanism itself relies on the same
principle: magnetism.</p>

<p>We collected a bunch of data for this pull-and-release interaction from many
devices. We found that most devices behave predictably well. Here's a combined
plot of normalized, superimposed positives and negatives from all phones which
we collected data from, with each dimension of the magnetometer vector plotted
separately.</p>

<p><img src="/magnetic-input-mobile-vr/all_features.png" alt="Image of the true positives and negatives from all phones." /></p>

<p>The detector we built was not based on a template learned from all of the data
above, but a simpler state machine based on thresholding. The thresholds
themselves were learned empirically. Here's the simple state machine:</p>

<p><img src="/magnetic-input-mobile-vr/state_machine.png" alt="State machine of the detector" /></p>

<p>The basic idea is that we take a sliding window approach, normalizing all of the
data relative to the last value in the window. For each window, we calculate <code>min_1</code>,
which is the smallest value of the first half of the window, and <code>max_2</code>, the
largest value in the second half. Next, we compare to empirically determined
thresholds perform the appropriate transition in the state machine. I won't bore
you with details of normalizing the data, etc but you can find all of the
details in <a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">the paper</a>. Oh, and all of the code is also available <a href="https://github.com/dodger487/MIST">on
github</a>.</p>

<h2>What's next?</h2>

<p><img src="/magnetic-input-mobile-vr/joystick.jpg" class="floatright" title="Hypothetical magnetic joystick"/></p>

<p>A lot more can be done using passive magnetic input. With uncalibrated
magnetometers, there is no fear of calibration events, so we could implement a
faster detector based on just the down motion of the magnet. We could reliably
detect long presses and double clicks. Alternatively, extensions to the existing
input can be implemented by simply changing the geometry of the physical
constraints, such as a joystick form factor.</p>

<p>I'm incredibly happy that Cardboard has been doing so well. Thanks to the great
team working so hard on it, there are now <a href="http://techcrunch.com/2015/05/28/google-has-shipped-over-1-million-cardboard-vr-units/">over 1 million units shipped</a>.
The press has been happy with it too, with kind reviews from many tech
publications.</p>

<p><a href="http://techcrunch.com/2014/06/25/hands-on-with-googles-incredibly-clever-cardboard-virtual-reality-headset/">Techcrunch</a> said:</p>

<blockquote>
  <p>This funny little cardboard faux-Rift has something even the original Rift
  itself does not: a built-in button. Your phone is able to sense the magnet’s
  movement, allowing it to act as a ridiculously clever little button. Yeesh.</p>
</blockquote>

<p><a href="http://www.techradar.com/news/phone-and-communications/mobile-phones/google-cardboard-everything-you-need-to-know-1277738">Techradar</a> said:</p>

<blockquote>
  <p>What's also somewhat amazing is the magnet on the side. […] The little magnet
  on the side is actually a quite ingenious design aspect of Google Cardboard.
  It's a button!</p>
</blockquote>

<p><a href="http://www.engadget.com/2014/12/10/google-cardboard/">Engadget</a> said:</p>

<blockquote>
  <p>One of the things I liked most was a switch located on the left temple, which
  consists of just a couple of magnets and a metal ring.</p>
</blockquote>

<p><a href="http://www.google.com/get/cardboard/downloads/wwgc_manufacturers_kit_v2.0.zip">Future versions of Cardboard</a> are switching to a different input method
using a conductive button which brings your body's capacitance to the screen,
similar to how a touch stylus works. It's cheaper without them, and the new
input works well, but I'll definitely miss the magnets!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Site redesign, version five</title>
    <author><name>Boris Smus</name></author>
    <link href="/design-v5"/>
    
    <updated>2015-06-10T09:00:00-00:00</updated>
    
    <id>http://smus.com/design-v5</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's been over three years since the design of this site has been
updated. Time to change that!</p>

<p><img src="/design-v5/0days.jpg" class="floatright"/></p>

<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous
designs</a>, I've been happier with minimal designs,
especially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired
by many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical
Typography</a>,
<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik
Johansson</a>,
<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>
and <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>

<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use
flexbox</a> in many places, which makes
the CSS far more intuitive. The responsive parts are very simple,
consisting of just ten CSS declarations.</p>

<!--more-->

<p>Rather than subjecting readers to my face on every page, I have a simple
stipple background on the <a href="/about">about page</a>, which I created using the
complex but functional <a href="http://www.evilmadscientist.com/2012/stipplegen2/">StippleGen</a>.</p>

<p>Also, I've started working on a self-hosted visual link blog that you
can check out in under <a href="/inspiration">inspiring clippings</a>. I've
implemented a companion Chrome extension that makes it super easy to
clip inspiring content from anywhere on the web and bring it to that
page.</p>

<p><strong>Typography:</strong> I'm continuing to use Google fonts, which seems to be so
much simpler to use than various competitors. I have not completely
optimized my selection of fonts, but this is satisfactory given my
belief that no design is ever finished. <a href="http://alistapart.com/article/improving-ux-through-front-end-performance">Performance is UX</a> too,
and aesthetic decisions need to be counterbalanced by mundane
considerations like page load time.  Unfortunately <a href="https://www.google.com/fonts/specimen/Dosis">Dosis</a> didn't
make the cut.</p>

<p><strong>Infrastructure:</strong> This site is still built using the <a href="https://github.com/borismus/lightning">lightning static
blog</a> engine, which I'm continuing to improve. On that front,
I've dropped the ambitious goal of being able to edit content from any
device using dropbox, since in practice I always author on my laptop.
Instead, the focus has been on optimizing the edit flow for the local
offline case, and I have built <a href="https://github.com/lepture/python-livereload">livereload</a> into the local preview
server. As far as hosting, I have conceded to GitHub Pages, and have
migrated away from using S3 directly.</p>

<p><strong>Thanks:</strong> to <a href="https://twitter.com/mikemartin604">Mike</a>,
<a href="https://twitter.com/paul_irish">Paul</a>,
<a href="https://twitter.com/smattyang">Seungho</a>,
<a href="https://twitter.com/scottjenson">Scott</a>,
<a href="https://twitter.com/mahemoff">Michael</a>, and other awesome friends that
gave me excellent design suggestions and found bugs!</p>

<p>While I appreciate companies like <a href="http://medium.com">Medium</a> and
<a href="http://svbtle.com/">Svbtle</a> advancing the aesthetics of the web, I
completely <a href="http://practicaltypography.com/billionaires-typewriter.html">agree with Matthew Butterick</a>'s view, and will
continue self-hosting my writings for as long as possible. Long live the
plurality of the web!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Spatial audio and web VR</title>
    <author><name>Boris Smus</name></author>
    <link href="/spatial-audio-web-vr"/>
    
    <updated>2015-03-19T09:00:00-00:00</updated>
    
    <id>http://smus.com/spatial-audio-web-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Last summer I visited Austria, the capital of classical music. I had the
pleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great
<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of
the piece was that the soloists moved between movements, so their voices
and instruments emanated from surprising parts of the great hall.
Inspired, I returned to the west coast and eventually came around to
building a spatial audio prototypes like this one:</p>

<p><a href="http://borismus.github.io/moving-music"><img src="/spatial-audio-web-vr/collage_small.jpg" alt="Screenshot of a demo" /></a></p>

<p>Spatial audio is an important part of any good VR experience, since the
more senses we simulate, the more compelling it feels to our sense
fusing mind. WebVR, WebGL, and WebAudio all act as complementary specs
to enable this necessary experience. As you would expect, because it
uses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on
mobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will
need headphones :)</p>

<!--more-->

<h2>Early spatial music</h2>

<p>One of the things that made my acoustic experience in the Salzburg Dom
so memorable was the beauty of the space in which it was performed. The
potential for awesome sound was staggering, with one massive organ at
the back, and four smaller organs surrounding the nave. During the
performance of the vespers, the thing that struck me the most was that
as the piece transitioned from movement to movement, choreographed
soloists also moved around the cathedral, resulting in haunting acoustic
effects. Sometimes, a voice would appear quietly from the far end of the
cloister, sounding distant and muffled. Other times, it would come from
the balcony behind the audience, full of unexpected reverb. It was a
truly unique acoustic experience that I will never forget, and it made
me wonder about the role of space in music.</p>

<p>As it turns out, there is a rich history on the <a href="http://en.wikipedia.org/wiki/Spatial_music">topic of spatialization
in music</a> going back to the 16th century. For the
purposes of this blog, I am more interested in the present day. In
particular, given the <a href="http://www.w3.org/TR/webaudio/">excellent state of audio APIs</a> on the
web, what follows is a foray into spatial audio with WebVR.</p>

<h2>Experiments in spatial audio</h2>

<p>How does music sound if in addition to pitch, rhythm and timbre, we
could tweak position and velocity as additional expressive dimension?
My demo places you into a virtual listening space, that you look
around into (using whatever means you have available: mouse and
keyboard, mobile phone gyroscope, or cardboard-like device) -- thanks to
<a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>. Each track is visualized as a blob of
particles. These animate according to the instantaneous amplitude of the
track, serving as a per-track visualizer and indicating where the track
is in space.</p>

<p>There is a surprising amount of multi-track music out there, such as
<a href="http://www.cambridge-mt.com/ms-mtk.htm">Cambridge Music Technology's raw recordings archive</a> for aspiring
audio engineers and <a href="https://soundcloud.com/search/sets?q=stems">soundcloud stems</a> which are typically
recorded separately in a studio and then released publicly for <a href="http://www.remixcomps.com/">remix
contests</a>. In the end, I went with a few different sets just to
get a feeling for spatializing a variety of tracks:</p>

<ul>
<li>Multiple people <a href="http://borismus.github.io/moving-music/?set=speech">speaking (demo)</a> simultaneously (the <a href="http://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party effect</a>).</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=jazz">live (demo)</a> recording with many mic'ed instruments.</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=phoenix">studio recording (demo)</a> with cleanly recorded tracks (A <a href="http://en.wikipedia.org/wiki/Phoenix_%28band%29">Phoenix</a> track).</li>
</ul>

<p>In addition to selecting the sounds to spatialize, the demo supports
laying out the tracks in various formations. To cycle between these
modes, hit space on desktop, or tap the screen on mobile:</p>

<ul>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=0">clumped together (demo)</a> in one place.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=1">positioned around the observer (demo)</a>.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=2">moving around the observer (demo)</a>.</li>
</ul>

<p>Given that the code is <a href="https://github.com/borismus/moving-music">open sourced on github</a>, it's pretty
easy to try your own tracks, implement new trajectories or change the
visualizer. Please fork away!</p>

<h2>Implementation details</h2>

<p>In an attempt to eat my own dogfood, this project partly serves as a way
to test the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> to make sure that
it is usable, and provides the functionality that it purports to. I've
made a bunch of changes to the boilerplate in parallel, fixing browser
compatibility issues and resolving bugs. Notable improvements since
inception include the ability to mouselook via <a href="http://www.html5rocks.com/en/tutorials/pointerlock/intro/">pointer
lock</a> in regular desktop mode and improved support for iOS
and Firefox nightly. Thanks to <a href="http://www.antoniocosta.eu/">Antonio</a>, my awesome designer
colleague, the WebVR boilerplate has a new icon!</p>

<p>This project relies heavily on audio, but requires the page to be
running in the foreground for you to enjoy the immersive nature of the
experience. Browsers, especially on mobile devices, can have some weird
behaviors when it comes to backgrounded tabs. It's a safe bet to just
prevent this from happening altogether, so I've been using the <a href="http://www.smashingmagazine.com/2015/01/20/creating-sites-with-the-page-visibility-api/">page
visibility API</a> to mute the music when the tab goes out of
focus, and then resume it when it's back in focus. This works super well
across browsers I've tested in and prevents the page-hunt where you're
trying to find which annoying tab/activity/app is playing!</p>

<p>I toyed a little bit with the doppler effect, but found it to be
terrible for music. Because in the moving case, each track moves with
its own velocity relative to the viewer, frequency shifts are
non-uniform, leading to a cacophany of out-of-tune instruments. For
spoken word, it worked quite well, though. The caveat to all this is that the
current <a href="http://crbug.com/439644">doppler API is deprecated</a>, so I
didn't delve too deeply into doppler until we have a new implementation.</p>

<h2>Pitfalls and workarounds</h2>

<p><strong>Set your listener's up vector properly.</strong> Something you should beware
of is to always set the up vector correctly in the
<code>listener.setOrientation(...)</code> call. Initially, I was only setting the
direction vector, keeping up fixed at <code>(0, 1, 0)</code>, but this yielded
unpredictable results and took a long time to track down.</p>

<p><strong>Streaming is broken in mobile implementations.</strong> A couple of issues
related to loading audio bit me as I was developing, proving to be
nearly show stoppers (please star if you feel strongly):</p>

<ul>
<li>Streaming audio doesn't work on Android (or iOS). This means that
every track we play needs to be first loaded, and then decoded:
<a href="http://crbug.com/419446">http://crbug.com/419446</a></li>
<li>Decoding mp3 on Android takes a very very long time (same in Firefox):
<a href="http://crbug.com/232973">http://crbug.com/232973</a></li>
<li>Though it doesn't directly affect my spatial sound experiments, the
inability to bring in remote WebRTC audio streams into the audio graph
is blocking other ideas: <a href="https://crbug.com/121673">https://crbug.com/121673</a></li>
</ul>

<p>I tried to work around the streaming issue by doing my own chunking
locally and then writing a <code>ChunkedAudioPlayer</code>, but this is harder than
it seems, especially when you want to synchronize multiple chunked
tracks.</p>

<p><strong>Beware of implementation differences.</strong> It's also worth noting that
different browsers have slightly different behaviors when it comes to
PannerNodes. In particular, Firefox spatialization can appear to sound
better, but this is simply because it's louder (the same effect can be
replicated in Chrome by just increasing gain). Also, on iOS, it seems
that the spatialization effect is weaker -- potentially because they are
using a different HRTF, or maybe they are just panning.</p>

<p><strong>Spatialization effect can be subtle.</strong> I found that there wasn't
enough oomph to the effect provided by WebAudio's HRTF. Perhaps it is
acoustically correct, but it just wasn't obvious or compelling enough as
is. I had to fudge the situation slightly, and implement a sound cone
for the observer, so that sources that are within the field of view got
a slight gain boost.</p>

<h2>Parting words and links</h2>

<p><a href="http://www.ee.usyd.edu.au/people/philip.leong/UserFiles/File/papers/errors_hr97.pdf">The nature and distribution of errors in sound localization</a>
is a seminal paper from 1997, giving a thorough psychoacoustic analysis
on our hearing limits. In this web audio context, however, it is unclear
how much of this perceptual accuracy is lost due to variations in
headphone style and quality, and software implementation details.  To
truly bring my Austrian cathedral experience to the web, we would
probably need a personalized HRTF, and also a more sophisticated room
model that could simulate reflections from the walls of the building.
This is concievable on the web in the near future, especially with the
prospect of the <a href="https://plus.sandbox.google.com/+ChrisWilson/posts/QapzKucPp6Y">highly anticipated</a> <a href="http://webaudio.github.io/web-audio-api/#audio-worker-examples">AudioWorker</a>.</p>

<p>Let me conclude by linking you to a couple more spatial audio demos:</p>

<ul>
<li>Ilmari wrote an <a href="http://www.html5rocks.com/en/tutorials/webaudio/positional_audio/">html5rocks post</a> a while back about three.js
and the Web Audio API.</li>
<li>Mozilla built the <a href="https://hacks.mozilla.org/2013/10/songs-of-diridum-pushing-the-web-audio-api-to-its-limits/">Songs of Diridum demo</a>, showing a cute
spatialized jazz band.</li>
<li>Arturo recently emailed me, showing me a <a href="http://inspirit.unboring.net/">pretty compelling WebVR +
Audio project</a>, in the spirit of WebVR.</li>
<li>Not web-based, but a painstakingly recorded and tweaked <a href="https://www.youtube.com/watch?v=IUDTlvagjJA">binaural
haircut simulation</a> just to illustrate the potential.</li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Responsive WebVR, headset optional</title>
    <author><name>Boris Smus</name></author>
    <link href="/responsive-vr"/>
    
    <updated>2015-02-02T09:00:00-00:00</updated>
    
    <id>http://smus.com/responsive-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>VR on the web threatens to cleave the web platform in twain, like mobile
did before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web
Design</a>, which websites to scale well for all form factors.
Similarly, for VR to succeed on the web, we need to figure out how to
make VR experiences that work both in any VR headset, and also without a
VR headset at all.</p>

<p><img src="/responsive-vr/hmds.png" alt="Various head mounted displays." /></p>

<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building
responsive web VR experiences that work on popular VR headsets and
degrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a
simple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>

<!--more-->

<h2>Preview the VR experience for everyone</h2>

<p>Say you visit a webpage, and it opens up in split-screen mode barrel
distortion, chromatic aberration correction, personalized interpupillary
distance, and provides 6 DOF tracking. <a href="https://www.youtube.com/watch?v=PkFyGNjaQ8k">Whoa</a>. You reach for your
VR headset only to find that you forgot it at work! How disappointing!
The vast majority of normal people with no head mounted display lying
around will surely be even more disappointed.</p>

<p>Responsive web design promises content which automatically adapts to
your viewing environment by using fluid layouts, flexible images,
proportional grids; a cocktail of modern web technologies. Similarly,
WebVR experiences need to work even without VR hardware. This has two
obvious advantages:</p>

<ol>
<li>The vast majority of people that don't have VR hardware can still get
a feeling for the experience.</li>
<li>Even if you have VR gear, donning it is a pain. This preview lets you
quickly evaluate whether or not wearing is worth the hassle.</li>
</ol>

<p>What are some reasonable fallbacks to the in-helmet VR experience? The
main question boils down to emulating head tracking without wearing
anything on your head. On mobile phones, the obvious answer is to use
the gyroscope, for a <a href="http://googlespotlightstories.com/">Spotlight Stories</a>-type experience. On
desktop, we use the mouse to free-look, and also support turning using
the arrow keys. This covers enough of the 3DOF orientation that all HMDs
provide. Clearly missing are the three translational degrees of freedom,
but these are provided only by some VR headsets, and we can imagine some
<a href="http://topheman.github.io/parallax/">interesting fallbacks</a> for those too.</p>

<h2>Write once, run in any VR headset</h2>

<p>Remember the old "write once, run anywhere" promise? The web is the
closest thing we have to fulfilling it, but what it actually delivers is
often far from this ideal. The latest VR wave has barely begun and
already the web VR world is fragmented. Case in point,
<a href="http://vr.chromeexperiments.com/">vr.chromeexperiments.com</a> don't work on Oculus, and
<a href="http://mozvr.com/">mozvr.com</a> demos don't work in Cardboard.  The promise of WebVR
is that once it lands, all will be well in the world. However, this
means that we need to wait for WebVR to become fully baked. In other
words, we are blocked on the valiant efforts of our dear <a href="https://twitter.com/vvuk">WebVR</a>
<a href="https://twitter.com/Tojiro">implementers</a> (as well as the heavyweight browser development
process consisting of spec authors, security reviews, binary size, etc).</p>

<p>To speed up the process, we need a polyfill for WebVR which uses web
APIs to provide functionality to the WebVR specification (currently, in
<a href="https://github.com/vvuk/gecko-dev/blob/oculus/dom/webidl/VRDevice.webidl">IDL form</a>). In the absence of a WebVR implementation, the polyfill
kicks in and supports mobile VR headsets like Cardboard and Durovis
Dive, which are passive contraptions that just piggyback on the
smartness found in your smartphone.</p>

<h2>Introducing: WebVR Boilerplate</h2>

<p>The <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> is on github, and consists of
two parts. Firstly, the <a href="https://github.com/borismus/webvr-polyfill">WebVR polyfill</a> provides WebVR
support for Cardboard-compatible devices, and orientation tracking
fallbacks where no headset is available. The WebVR polyfill can also be
installed from npm (available via <code>npm install webvr-polyfill</code>).</p>

<ol>
<li>A <code>CardboardHMDDevice</code>, provides a reasonable default for
interpupillary distance and field of view for cardboard-like devices.</li>
<li>On mobile devices, a <code>GyroPositionSensorVRDevice</code>, which provides
orientation through the <code>DeviceOrientationEvent</code>.</li>
<li>On PCs, a <code>MouseKeyboardPositionSensorVRDevice</code>, which provides
orientation through keyboard and mouse events.</li>
</ol>

<p>It is designed to be used with <a href="http://threejs.org/">THREE.js</a> plugins that are built
for the WebVR API: <a href="https://github.com/mrdoob/three.js/blob/master/examples/js/controls/VRControls.js">VRControls.js</a> and
<a href="https://github.com/mrdoob/three.js/blob/master/examples/js/effects/VREffect.js">VREffect.js</a>, but of course any valid usage of the WebVR API
should work (modulo bugs).</p>

<p>Secondly, the <a href="https://github.com/borismus/webvr-boilerplate/blob/master/js/webvr-manager.js">WebVR manager</a> surfaces VR compatibility
using consistent iconography and simplifies transitioning in and out of
full VR mode. It also contains some of the best practices for making VR
work on the web, for example, using orientation lock to keep the phone
in landscape orientation, and a means of keeping the phone screen on. If
you're ready to dive in, the <a href="https://github.com/borismus/webvr-boilerplate">github has all of the technical
information</a> available.</p>

<p>WebVR boilerplate is meant to make it easy to develop immersive
experiences that run on all VR hardware, including Oculus and Cardboard,
and also provide reasonable fallbacks when no specialized viewer is
available.</p>

<h2>WebVR boilerplate in action</h2>

<p><img src="/responsive-vr/sechelt.png" alt="Screenshot of the mozvr.com Sechelt demo." /></p>

<p>I really liked Mozilla's <a href="http://mozvr.com/projects/sechelt/">Sechelt demo</a>, inspired by the
eponymous town on British Columbia's beautiful Sunshine Coast. I've
<a href="http://borismus.github.io/sechelt">ported it</a> to WebVR Boilerplate. The result is the same
demo, which works in Cardboard, as well as continuing to work on desktop
and mobile devices, and on the Oculus Rift via <a href="https://nightly.mozilla.org/">Firefox Nightly</a> and
<a href="https://drive.google.com/folderview?id=0BzudLt22BqGRbW9WTHMtOWMzNjQ&amp;usp=sharing#list">Brandon's WebVR Chrome builds</a>. It also yielded less confusing
boilerplate code and a <a href="https://github.com/borismus/sechelt/commit/671cff9fc283b58d2ebce0ae6f0dfa3580050aab">simplified code base</a> since
there is no longer need for an unweildy conditional to determine whether
to use <code>DeviceOrientationControls</code>, <code>OrbitControls</code>, or <code>VRControls</code>,
and decide between <code>VREffect</code> and <code>StereoEffect</code>.</p>

<p>As always, <a href="https://twitter.com/borismus">let me know what you think</a> and feel free to point
out (or fix!) my mistakes on <a href="https://github.com/borismus/webvr-boilerplate">the github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web Sensor API: raw and uncut</title>
    <author><name>Boris Smus</name></author>
    <link href="/web-sensor-api"/>
    
    <updated>2014-11-13T09:00:00-00:00</updated>
    
    <id>http://smus.com/web-sensor-api</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Sensors found in smartphones define the mobile experience. GPS and the
magnetometer enable the fluid experience of maps; motion sensing enables
activity recognition and games, and of course the camera and microphone
allow whole categories of rich media applications. Beyond these now
obvious examples, sensors can also enable clever inventions, such as
<a href="http://www.cycloramic.com/">Cycloramic</a>, which used the vibrator motor in iPhones (4 and 5) to
rotate the phone and take a panorama, <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">pushup counters</a> which
use the proximity sensor to count repetitions, and <a href="https://play.google.com/store/apps/details?id=com.carrotpop.www.smth">Send Me To
Heaven</a>, which uses the accelerometer to determine flight time of
a phone thrown vertically as high as possible. I've had some experience
using and abusing sensors too, most recently for the <a href="http://smus.com/talk/2014/io14/">Cardboard magnet
button</a>.</p>

<iframe width="640" height="360" src="https://www.youtube.com/embed/l5775qwORk4" frameborder="0" allowfullscreen></iframe>

<p>However, over the last couple of years, I've had to step away from the
web as a development platform, in part because of the poor state of
sensor APIs.  In this post, I will describe some of the problems, take a
look at sensor APIs on iOS and Android, and suggest a solution in the
spirit of the <a href="https://extensiblewebmanifesto.org/">extensible web manifesto</a>.</p>

<!--more-->

<h2>Existing sensor APIs are underspecified</h2>

<p>One of the most popular sensor APIs on the web is the <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html#devicemotion">DeviceMotion event
API</a>, which is basically always just an opaque abstraction around the
accelerometer. The web, as always, tries to solve the problem in the
most general way possible:</p>

<blockquote>
  <p>This specification provides several new DOM events for obtaining
  information about the physical orientation and movement of the hosting
  device. The information provided by the events is not raw sensor data,
  but rather high-level data which is agnostic to the underlying source
  of information. Common sources of information include gyroscopes,
  compasses and accelerometers.</p>
</blockquote>

<p>This could be fine in theory, except the specs end up being so vague in
their attempt to please everybody, that they under-specify the behavior
of events such as <code>DeviceOrientation</code>. Throw in some rogue implementers,
and you end up with huge discrepancies in browsers, as <a href="http://www.html5rocks.com/en/tutorials/device/orientation/">Pete found back
in 2011</a>:</p>

<blockquote>
  <p>For most browsers, alpha returns the compass heading, so when the
  device is pointed north, alpha is zero. With Mobile Safari, alpha is
  based on the direction the device was pointing when device orientation
  was first requested. The compass heading is available in the
  webkitCompassHeading parameter.</p>
</blockquote>

<p>A useful sensor abstraction would be to build a compass on top of the
magnetometer (and maybe gyro) sensors, and then expose that as a high
level Compass API. Unfortunately many web sensor APIs give us a
mid-level of abstraction. They don't map reliably to particular hardware
sensors, nor do they provide much use. Sensors allow many applications
that were not originally envisioned by the spec writers. By choosing
poorly specified ivory-tower abstractions, the web limits what can be
done on the platform.</p>

<h2>Low level sensor APIs don't exist</h2>

<p>While you can work around the insanity of <code>Device*</code> style events on the
web with platform-specific shims, you cannot work around missing sensor
APIs. Magnetometers, pressure sensors, proximity, light, temperature,
battery, etc. These are mostly missing, and the ones that are specified
are specified in a very narrow way that does not generalize across to
other types of sensors (eg. <a href="http://www.w3.org/TR/2013/CR-ambient-light-20131001/">DeviceLightEvent</a>).</p>

<p>Unfortunately it seems that previous attempts to push for a general low
level sensor API <a href="http://lists.w3.org/Archives/Public/public-geolocation/2011Oct/0000.html">haven't really gotten much traction</a>. In
fact, it's a bit unclear whether or not the <a href="http://www.w3.org/2009/dap/">Device API working
group</a>, is even the right place for sensor APIs, since their
mandate is supposedly more about services than sensors:</p>

<blockquote>
  <p>[To] enable the development of Web Applications and Web Widgets that
  interact with devices services such as Calendar, Contacts, Camera,
  etc.</p>
</blockquote>

<p>Except <a href="https://dvcs.w3.org/hg/dap/raw-file/default/sensor-api/Overview.html">here's a sensor API</a> from the same group, which
seems to be abandoned... I don't even</p>

<p>There are more recent voices (circa 2014) that seem to be pushing in a
generic sensor API direction, from folks like <a href="https://github.com/rwaldron/sensors">Rick Waldron</a>
and <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Sep/0024.html">Tim Volodine</a>. Many of these ideas are still working within
the confines of a sensor API for each type of sensor. This does not
scale well for the web, which tends to take a long time for any new web
standard, but this renewed interest is very exciting and promising!</p>

<h2>Sensors on other platforms</h2>

<p>The web is woefully behind native platforms in almost every regard (with
possibly the exception of audio). Sensors on iOS and Android have a rich
history, and ended up in a pretty similar place as the two platforms
have scrambled to converge. Let's take a look.</p>

<p>iOS started off with a <a href="https://developer.apple.com/LIBRARY/ios/documentation/UIKit/Reference/UIAccelerometer_Class/index.html">UIAccelerometer API</a>, which was
replaced by <a href="https://developer.apple.com/LIBRARY/ios/documentation/CoreMotion/Reference/CoreMotion_Reference/index.html">CoreMotion</a> in iOS 5. Rather than providing a
series of specific APIs for each type of sensor API as it had before,
CoreMotion provides a unified framework for sensor events. Each data
type inherits from a common base class <code>CMLogItem</code>, and most of the API is
encapsulated in <code>CMMotionManager</code>, which explicitly lists accelerometer,
gyroscope and magnetometer-related APIs. iOS went from specific to
generic, which makes it super easy to add new types of sensor data. That
said, the API is generic only for motion sensors, which excludes a bunch
of sensors not directly related to motion like temperature, humidity,
etc.</p>

<p>Android started off right, and hasn't had to change much, providing a
<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">generic API for sensors</a> since API level 3. Android's API
is accessed through a SensorManager, which provides a somewhat overly
abstract API, because of its support for multiple sensors of one type
(eg. two accelerometers) in the same device. Still, the idea is good,
and all of the low level sensor data are well specified (per sensor
type)so the hardware/firmware vendor knows what data format their sensor
should stream. Of course there are still rogue implementations that
don't follow the spec, but that is a perennial problem for any open-ish
ecosystem.</p>

<p>Android also has a <a href="http://developer.android.com/guide/topics/sensors/sensors_overview.html">distinction</a> between software-based sensors and
hardware-based ones. The idea is that the same framework can provide
both the low level data coming directly from the hardware, as well as
useful higher level data obtained through <a href="http://en.wikipedia.org/wiki/Sensor_fusion">sensor fusion</a>. As of
API level 19, Android also provides <a href="http://developer.android.com/reference/android/hardware/SensorManager.html#flush(android.hardware.SensorEventListener)">batch mode</a> for sensor data, which
is very useful for conserving battery and CPU for applications where
some delay is acceptable.</p>

<p>One nice advantage of an iOS style API is that each sensor type has its
own structure (rather than just an amorphous array of floats, as in
Android), which is quite a bit easier to parse. The downside is that
adding new sensor types introduces more overhead, since each one
requires a new structure to be defined and agreed upon. Since we are
talking about web standards, which evolve at a glacial pace, we should
err on a simple API that works well without spec modifications.</p>

<h2>Great artists steal</h2>

<p>There is no need for the web to reinvent the wheel. The wheel has
already been invented by iOS and Android. All we need to do is take the
good parts from these successful sensor platforms, and integrate them
into the web in a way that makes sense. The web is not the place for
innovation, but for standardization.</p>

<p>Conceptually, a sensor provides a stream of data. The developer should
be able to configure the rate at which new data comes in, as well as
batching the data in windows of sensor data (as is customarily done with
audio data, for example). In Android, because of a plurality of devices,
it's important to be able to check if a particular sensor is available.
The same concept maps well to the web.</p>

<h2>Toward A Web Sensor API</h2>

<p>In general, here are the requirements for a Web Sensor API that works:</p>

<ul>
<li>A specification defining the format of the data, similar to
<a href="http://developer.android.com/reference/android/hardware/SensorEvent.html">Android</a>.</li>
<li>A way to feature detect for the existence of a particular sensor.</li>
<li>A way to request (and revoke) a stream of sensor data.</li>
<li>A way to specify how often to poll the sensor.</li>
<li>Bonus: A way to request sensor data in batch form.</li>
</ul>

<p>While bringing an API like this to the web is a huge undertaking, there
is a silver lining. The sensors we're talking about are all considered
(at least for now) low-security, in the sense that on native platforms,
there is no extra permission required to access them. This makes it
possible to simply propose an API, convince everybody of it's worth, and
then have it implemented across the web!</p>

<p>I don't have a strong opinion about how the API itself looks like as
long as it fulfils the above requirements. Here's a simple strawman
which should satisfy them:</p>

<pre><code>// Check for magnetometer support.
if (sensors.Magnetometer === undefined) {
  console.error('No magnetometer found');
}

// Start listening for changes to the sensor.
var magnetometer = sensors.Magnetometer;
magnetometer.addEventListener('changed', onMagnetometer, {
  sample_rate: sensors.POLL_FAST, // In hertz, eg. POLL_FAST == 100
  batch: 1 // Number of data points to provide in a single poll.
});

// Handle sensor events.
function onMagnetometer(event) {
  var data = event.data[0];
  // Get the timestamp (in millis).
  var t = data.timestamp;
  // Get the data (in this case µT, as per spec).
  var x = data.values[0];
  var y = data.values[1];
  var z = data.values[2];
  // Process the data.
  superAdvancedSensorFusionThing.addData(t, x, y, z);
}

// Stop listening.
magnetometer.removeEventListener('changed', onMagnetometer);
</code></pre>

<h2>Conclusion</h2>

<p>If you aren't yet convinced that we need access to low level sensors on
the web, recall web developers scoffing at device pixel ratio (DPR),
really questioning the need for to ever go above 2x. Now that <a href="https://developers.google.com/cardboard/">some
screens</a> are ending up 5cm from your face, the current
generation of 4x displays isn't enough. The same exact thing applies to
sensors. The need is there, but it is not seen as enough of a priority
by the web community.</p>

<p>By enabling low level sensor access, we can allow new experiences never
before possible on the web. <a href="https://play.google.com/store/apps/details?id=com.runtastic.android.pushup.pro">Pushup rep counters</a>, the <a href="http://smus.com/talk/2014/io14/">magnet
button</a> in Cardboard, and myriads more applications
yet to be concieved could all be built on the web platform, eliminating
a big reason why the web is increasingly losing its relevance on mobile
devices. Providing low level sensor access is critical and aligns
perfectly with the <a href="https://extensiblewebmanifesto.org/">extensible web vision</a>.</p>

<p><em>Update (Nov 14, 2014): There was a <a href="http://lists.w3.org/Archives/Public/public-device-apis/2014Nov/0018.html">W3C call</a> about this very
topic yesterday! Kicking off efforts in <a href="https://github.com/w3c/sensors">this github repo</a>.
Join us!</em></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2014 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="/uist-2014"/>
    
    <updated>2014-10-14T09:00:00-00:00</updated>
    
    <id>http://smus.com/uist-2014</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>This year's <a href="http://www.acm.org/uist/uist2014/">UIST</a> was held in Waikiki, Honolulu, the undisputed
tourist capital of Hawaii. I've stuck to my now three year old habit of
taking notes and <a href="http://smus.com/uist-2013">posting my favorite work</a>. Since last year,
the conference has grown an extra track. The split was generally OK for
me, with my track mostly dedicated to user interface innovation
(sensors, etc) and another more concerned with crowdsourcing,
visualization, and more traditional UIs.</p>

<p>My overall feeling was that the research was mostly interesting from a
tech perspective, but focused on solving the wrong problem. For example,
at least 5 papers/posters/demos were focused on typing on smartwatches.
The keynotes were very thought provoking, especially when juxtaposed
with one another.</p>

<!--more-->

<h2>Focused Ultrasonic Arrays</h2>

<p>I got to play with a holographic display with touch-feedback. Sounds
crazy, and it is. HaptoMime uses an array of ultrasonic transducers to
beam-form focused ultrasound to a specific target. The touch feedback
feels like a little electric shock, but it's incredible that it works.
The field of view of the holographic screen is a bit limited:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/uARGRlpCWg8" frameborder="0" allowfullscreen></iframe>

<p>As a kid, I loved drawing patterns on my grandma's rug with my finger.
This research team was clearly inspired by the same activity, and they
created several ways of automating the process: using a roller device, a
pen, and an focused ultrasonic array:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/L0hrETGddLQ" frameborder="0" allowfullscreen></iframe>

<h2>Multi-device interactions</h2>

<p>GaussStones built on a bunch of other "Gauss"-prefixed previous work
from the same lab, showing an array of hall sensors used to sense a
variety of shielded magnetic tokens, which can encode an ID using field
strength. You could play physical chess, or even combine magnetic tokens
to create more complex interactions, like a slider or button unit:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/qlr-15Oto6s" frameborder="0" allowfullscreen></iframe>

<p>Another nice example of multi-device interaction came from MIT, where
a group used this extremely clever way of tracking the phone's position
relative to a laptop, using a 2D gradient, where the color of each pixel
maps to a position in space:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/hFH6hJLDoLE" frameborder="0" allowfullscreen></iframe>

<p>The awkwardly named Vibkinesis shows a smartphone case which is
equipped with two vibrator motors which give a phone the ability to
translate and rotate on a flat surface. In one example,
notifications caused the phone to rotate by 90 degrees, which had the added benefit of
notifying the user of a notification even if the battery runs out of
juice. This is apparently funny from a Japanese culture perspective,
where characters often die under strange circumstances, leaving no clue
but a "dying message" on or around their person. Another example
involved a fish-eye lens on the front-facing camera to detect the
position of the user's hand (based on skin color), and then physically
nudging the user to get their attention:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/UlFwVUHotrU" frameborder="0" allowfullscreen></iframe>

<h2>Awesome fabrication techniques</h2>

<p>I'm a huge fan of subtractive techniques (eg. laser cutting) rather than
additive ones (eg. 3D printing). FlatFitFab is a CAD tool for easily
creating balsa dinosaur-style models, and evaluating their stability and
feasibility. Super cool work:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/HeFQw0chSJY" frameborder="0" allowfullscreen></iframe>

<p>Rather than creating PCBs in something like Eagle, why not just sketch
them with a conductive pen instead? ShrinkyCircuits does just this,
following the principles of Shrinky Dinks, which shrinks when heated.
Because the whole board shrinks, it improves conductivity of the
conductive ink, and the contact points with electronics components.</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/4p-l374rb8M" frameborder="0" allowfullscreen></iframe>

<h2>Spatial AR</h2>

<p>Research from Microsoft showed rooms instrumented with multiple
Projector+Depth Camera rigs, which allowed for some interesting
multi-user interactions:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/ILb5ExBzHqw" frameborder="0" allowfullscreen></iframe>

<p>Of course, the setup above doesn't allow perspective-corrected scenes.
To remedy this, they had a companion project which split the room in
two, creating head-tracked scenes for two participants. Pretty cool,
though it does not generalize to more than two people, nor does it
support stereo:</p>

<iframe width="600" height="339" src="//www.youtube.com/embed/Df7fZAYVAIE" frameborder="0" allowfullscreen></iframe>

<p>Still, entering either of these VR rooms feels a lot less dorky than
having to don a VR headset.</p>

<h2>Keynotes</h2>

<p>UIST was punctuated by three keynotes, from Ken Perlin, Mark
Bolas and Bret Victor, all of which were thought provoking and sometimes
frightening, but unfortunately not recorded. When I used to be in
Developer Relations, we would be hardpressed to show up at a conference
if the talk was not recorded because so much of the engagement happened
after the fact online. <strong>Academia needs the same culture</strong>.</p>

<p>Ken Perlin kicked off the co-located conference, SUI (Symposium for Spatial
Interaction), off with a nice talk about making computer use more like
performing a music, and less like writing a musical composition. As he
gave the talk, he very effectively used a tool he created called Chalk
Talk, which lets you sketch objects with behaviors in short hand - very
meta. Unfortunately I've been unable to find anything published about
the tool, as it would be interesting to play with. Ken envisioned a
world where you could do something conceptually similar to Chalk Talk
without a computer in the way. If this "virtual chalk" capability was
available to all humans, it would transform the way we communicate. I
wasn't completely convinced. When I'm discussing something with
colleagues, we only use a whiteboard for only very specialized things
like drawing a diagram of multiple objects. So there are two things that
need to happen:</p>

<ol>
<li>This virtual chalk needs to be <strong>easier to access</strong> than a whiteboard
while in a meeting room.</li>
<li><strong>Expand the set of concepts</strong> that can be expressed with virtual chalk.
Text and speech is <a href="http://graydon.livejournal.com/196162.html">pretty powerful</a>.</li>
</ol>

<p>Mark Bolas started UIST with a pretty terrifying keynote on virtual
reality. His premise was that "we are headed into a virtual future,
whether we want it or not". Terrifyingly, Mark seemed to be okay with
this inevitability, even going as far as discounting augmented reality,
since by the time we've built VR, we'll just want to stay in our
helmets. The real world isn't that great anyway. One thing I liked was
his call for creating more <strong>surreal experiences</strong> in VR rather than
trying to emulate the real world. These types of simulations are
conspicuously missing from <a href="https://share.oculusvr.com/category/all">existing VR demos</a>.</p>

<p>Bret Victor ended the conference with a much needed humanist
counterpoint to Mark' vision. I cannot do the talk justice, and eagerly
await a recording of it to try to understand all of the nuances. The big
idea of the talk was that "knowledge work" which started with the
printing press is tyrannical, reducing all of our senses and abilities
to manipulating symbols on a sheet of paper. So many other things that
evolution has designed for us, like hearing, smell, sense of space,
touch, etc, are all thrown out of the window. This problem only gets
compounded as we move to virtualize everything with touch screens. Bret
thinks we're poised to design the next great "dynamic" medium after the
printing press, something that is always interactive and multimodal, and
takes advantage of a wide array of human capabilities.</p>

<p>According to Mark Bolas, the real world is flawed, and we should build a
better virtual one. Bret Victor's vision is that humans are perfect,
having evolved over thousands of years. Rather than changing what it
means to be human, we should build a new medium that adapts to our
inherent strengths and weaknesses. Ken Perlin's "virtual chalk" is a
great example application for this dynamic medium.</p>

<h2>Tracks I missed</h2>

<p>Because UIST has become a multi-track conference, I inevitably missed
interesting parts. In particular, the collaboration track had some
<a href="https://www.youtube.com/watch?v=QtyO-oFlzGg">awesome</a> <a href="https://www.youtube.com/watch?v=jMH_qQF0vKg">work</a>, and there was one <a href="https://www.youtube.com/watch?v=YMfzAstvij0">music-related paper</a>
paper. It was great to have had a good excuse to go this year, showing
Cardboard to the academic community. Looking forward to next year,
although it is to be held in a somewhat <a href="http://uist.acm.org/about">less glamorous location</a>.</p>

<p><img src="/uist-2014/sunset.jpg" alt="Hawaii sunset." /></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Spectrogram and oscillator</title>
    <author><name>Boris Smus</name></author>
    <link href="/spectrogram-and-oscillator"/>
    
    <updated>2014-06-09T09:00:00-00:00</updated>
    
    <id>http://smus.com/spectrogram-and-oscillator</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A live-input spectrogram written using <a href="http://polymer-project.org">Polymer</a> using the <a href="http://webaudioapi.com">Web
Audio API</a>.</p>

<p><img src="/spectrogram-and-oscillator/screenshot.png" alt="Screenshot of spectrogram" /></p>

<p>If you're running Chrome or Firefox, <a href="http://borismus.github.io/spectrogram">see it in action</a>. Once the
spectrogram is running, see if you can make a pattern with your speech
or by whistling. You can also click anywhere on the page to turn on the
oscillator. For a mind-blowing effect, <a href="https://www.youtube.com/watch?v=M9xMuPWAZW8&amp;t=5m30s">load this</a> in a parallel
tab.</p>

<!--more-->

<h2>Why?</h2>

<p>Having a spectrogram is incredibly handy for a lot of the work I've been
doing recently. So a while ago, I built one that satisfies my needs. It
runs in a full-screen, using the microphone input as the source.</p>

<p>It also includes an oscillator, which plays a sine wave at the frequency
of your pointer. It also shows you the frequency that it plays back, and
plots a short buffer of pointer positions. This is handy for measuring
internal latency:</p>

<p><img src="/spectrogram-and-oscillator/latency.png" alt="Latency estimation" /></p>

<p>Having the oscillator built-in is also pretty fun. You can <a href="/spectrogram-and-oscillator/sounds/morse.wav">send morse
code</a> (short short short, long long, short short long, short
short short), <a href="/spectrogram-and-oscillator/sounds/radio.wav">scan for radio stations</a>, make 8-bit character
<a href="/spectrogram-and-oscillator/sounds/sfx.wav">dying sound effects</a>, simulate <a href="/spectrogram-and-oscillator/sounds/ghosts.wav">aliens, ghosts and
theremins</a>, and annoy <a href="/spectrogram-and-oscillator/sounds/dogs.wav">small, annoying dogs</a>.</p>

<p>I use the tool mostly in Chrome, but it also works in Firefox.
Unfortunately no other browser currently has both <code>getUserMedia</code> and Web
Audio API support.</p>

<h2>Configuration parameters</h2>

<p>The following are HTML attributes of the <code>g-spectrogram</code> component. Many
of them are also configurable via the spectrogram controls component,
which shows up if the <code>controls</code> attribute is set to true.</p>

<ul>
<li><code>controls</code> (boolean): shows a config UI component.</li>
<li><code>log</code> (boolean): enables y-log scale (linear by default).</li>
<li><code>speed</code> (number): how many pixels to move past for every frame.</li>
<li><code>labels</code> (boolean): enables y-axis labels.</li>
<li><code>ticks</code> (number): how many y labels to show.</li>
<li><code>color</code> (boolean): turns on color mode (grayscale by default).</li>
<li><code>oscillator</code> (boolean): enables an oscillator overlay component. When
you click anywhere in the spectrogram, a sine wave plays corresponding
to the frequency you click on.</li>
</ul>

<h2>Using the Polymer component</h2>

<p>If you are inclined to embed this component somewhere, you can,
since it's implemented in Polymer, which, by the way, is an
awesome framework. Once you've <a href="http://www.polymer-project.org/docs/start/getting-the-code.html">gotten started</a>, here's
the simplest possible version:</p>

<pre><code>&lt;g-spectrogram/&gt;
</code></pre>

<p>Enable controls:</p>

<pre><code>&lt;g-spectrogram controls&gt;&lt;/g-spectrogram&gt;
</code></pre>

<p>Pass parameters to the component:</p>

<pre><code>&lt;g-spectrogram log labels ticks="10"&gt;&lt;/g-spectrogram&gt;
</code></pre>

<h2>Future work ideas</h2>

<p>It would be great to add a few things to this tool. If you're interested
in helping, submit your changes as a pull request <a href="https://github.com/borismus/spectrogram">on github</a>.
Some ideas for things that can be done:</p>

<ul>
<li>Improved axis labeling.</li>
<li>Make it work in mobile browsers.</li>
<li>Loading/saving of traces.</li>
<li>Loading audio data from a file.</li>
<li>Zoom support.</li>
<li>Higher precision FFT results (would require writing a custom FFT
rather than using the one built into Web Audio API.)</li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Addressable apps</title>
    <author><name>Boris Smus</name></author>
    <link href="/addressable-apps"/>
    
    <updated>2014-05-21T09:00:00-00:00</updated>
    
    <id>http://smus.com/addressable-apps</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It is human nature to create taxonomies for everything: people, places,
and things.  Without such a system of reference, we become lost and
disoriented.  Imagine your city with street names and addresses blanked
out. Finding your favorite cafe, meeting up with your friend on the
weekend, even locating your own parked car would become incredibly
difficult. Travel outside your city would become far more
challenging.</p>

<p>The web's defining property is addressability. URLs on the web are like
street names and addresses in the physical world. This makes sharing
and cross-linking easy. Non-web platforms are a little bit like our
city with blanked out street names and addresses. There's no good
way of talking about where you currently are, or how to get somewhere
else. These platforms typically give users a crutch to help with the
issue, such as a share button or dialog. But these create an
inherently inferior experience, since addressability is no longer
built-in. Addressability becomes a burden on the app developer, and
as a result, the platform is no longer navigable.</p>

<p>In light of the success of Android and iOS, and given a potential
explosion in new types of lower power computing (wearables, IoT, etc),
it's unclear if <a href="http://smus.com/ebb-of-the-web/">browsers will be as ubiquitous</a> as they are
today (at least in the near term). I'm very interested in seeing if and
how non-web platforms can embrace URLs.  How closely coupled are URLs to
HTML, and do they make sense without a presentation layer?</p>

<!--more-->

<h2>Not all URLs are created equal</h2>

<p>The modern URL can host several very different kinds of entities:</p>

<ol>
<li><strong>Data</strong>: text files, images, audio, movies, JSON, etc.</li>
<li><strong>Hypertext</strong> (content-program hybrid): HTML that can reference
content.</li>
<li><strong>Program</strong>: webapps designed to deliver a bundle of JavaScript that
then constructs the HTML dynamically from other URLs.</li>
</ol>

<p><img src="/addressable-apps/data-vs-program.png" alt="Program vs. data: evolution of web" /></p>

<p>Without an HTML renderer, hypertext and program URLs cannot be
interpreted. Only one of these types of entities makes sense: data. Data
URLs are seen everywhere on the web: whenever you include an <code>&lt;img&gt;</code> tag
on your page, or embed a <code>&lt;video&gt;</code>, reference some CSS, or make an XHR
to fetch some JSON, you are using a data URL.</p>

<p>Apps on other platforms use data URLs too, though not as much. Images
are typically included as part of the app itself, but all API access is
done in exactly the same fashion as on the web: using HTTP requests to
text or binary data.</p>

<p>The similarity isn't entirely superficial. Any sort of web-connected app
can be seen as just a view on top of a series of data URLs (APIs).
However, data URLs are typically hidden from the user. The only types of
URLs that users see are hypertext and program URLs. These are the ones
that are being shared around. But both of these types of URLs ultimately
map to HTML, sometimes via JavaScript. The underlying data URLs  are
concealed inside the page, and aren't exposed to the user.</p>

<h2>The "URL in, URL out" principle</h2>

<p>A user need not understand schemes, domain names, DNS, HTTP or GET
requests. They don't need to think about conceptual distinctions
between URL types to know that a URL is an address that gets you to the
same thing you're looking at right now. Whether it's Android/Java,
Polymer/JS or <em>InsertPlatform/InsertLanguage</em> underneath, the only thing
they want to be able to do is to continue reading their book on whatever
device they happen to be transitioning to. They want to share it with
their friend too, and have them enjoy a good read.</p>

<p>To make a platform URL-friendly, it should satisfy two simple
requirements:</p>

<ol>
<li>The platform should provide a way for apps to reveal the underlying
URL for the view.</li>
<li>Given a URL, the platform should open it in a way that yields the
best available user experience.</li>
</ol>

<p><img src="/addressable-apps/url-in-url-out.png" alt="Platforms handle content URLs and provide them on demand." /></p>

<p>However, to bring URL friendliness to a platform retroactively takes a
lot of effort. Taking a quick look at today's trending web-alternatives,
it's plain to see that Android has some form of URL in (via intent
filters), but no URL out. iOS has neither in, nor out (you're stuck). To
address this lack of URL out in Android, you can imagine all Android
activities having to implement a <code>URLReporter</code> interface like this:</p>

<pre><code>class TwitterProfileViewer extends Activity implements URLReporter {
  @Override
  String reportURL() {
    return String.format("http://twitter.com/%s", username);
  }
}
</code></pre>

<p>Of course, there is the not-unimportant UX question of how to then
reveal the URL and transfer it to other devices and people. But this
question will for now be left unanswered. With this API, a very
tasty carrot, and a very painful stick to force developers to implement
it (and a bit more UX thinking), we can make Android URL-friendly. </p>

<h2>But URLs aren't just identifiers</h2>

<p>You can look at URLs in one of two ways:</p>

<ol>
<li>As a <strong>universal identifier</strong>. The same URL is also the universal and
canonical way of getting to content that you are reading now.</li>
<li>As a <strong>web address</strong>. A URL like <code>http://smus.com/addressable-apps</code>
can be viewed as instructions for getting to a certiain place:
resolve the <code>smus.com</code> to <code>205.251.243.108</code>, connect to port 80 over
TCP, perform a <code>GET /addressable-apps</code> request.</li>
</ol>

<p>The real power of URLs is in both aspects combined. When only one facet
is used, the system is broken. I can't quite put my finger on it, but
something feels wrong in the cases, where the addressability aspect (2)
of URLs are not taken into account:</p>

<ul>
<li>AppLinks: these aren't universal identifiers, but fragile shortcuts to
the platform-specific apps. (violates 1)</li>
<li>Android intent filters: when you register an URL indent filter for
your activity, you aren't actually hosting anything at that URL.
(violates 2)</li>
<li>History API: a hack allowing developers to set the path of the URL to
anything they want. (violates 2)</li>
</ul>

<p>The History API emerged from a trend on the web: highly imperative
applications. These apps have grown so far from being a collection of
hyperlinked markup that they no longer have a natural URL-to-HTML-page
mapping. Because they are so script heavy, they need to be able to
pretend to respond to URLs. The History API is the webapp's hack for
URL-out.</p>

<h2>Mixed feelings</h2>

<p>This is my umpteenth attempt at finishing a post on the complicated
subject of URLs in non-web apps. And you have had the pleasure of
reading it not because the ideas in my head have crystallized into a
something coherent, but because I feel that the topic is difficult and
fundamentally unresolvable. In light of that, this post contains more
questions than answers. Sorry to disappoint :)</p>

<p>I'm still torn between maintaining the ideological and benefits of the
<a href="http://www.polymer-project.org/">mostly-declarative web</a> and the practicality of <a href="http://smus.com/installable-webapps/">jumping out
of the web's sandbox</a>. While I would be happy to see more
native platforms embrace URL in URL out, I don't think that the solution
is a clean one, nor do I think that first-class URLs are likely to
emerge in any platform after-the-fact. Unfortunately I don't think that
there is a clean solution.</p>

<p>However, as an optimist, I must believe in the long-term victory of the
web, though not in the sense that the web will rule over all other
user-facing platforms unopposed. Instead, platforms will continue to
rise and fall; the web's influence will ebb and flow as well. But the
web must be the one that wins out the most, keeping the idea of
addressability alive.</p>

<p>That URLs become a universal address that works across all platforms,
and not just the web, is a proposition worth considering.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>The ebb of the web</title>
    <author><name>Boris Smus</name></author>
    <link href="/ebb-of-the-web"/>
    
    <updated>2014-04-15T09:00:00-00:00</updated>
    
    <id>http://smus.com/ebb-of-the-web</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Tech pundits like to lament that the web has <a href="http://cdixon.org/2014/04/07/the-decline-of-the-mobile-web/">no viable
future</a>, while web idealists hold that in fact the web is
totally fine, with a "too big to fail" <a href="http://schepers.cc/the-recline-of-the-mobile-web">sort of attitude</a>.</p>

<p>At the root of this disagreement are poorly defined terms. The web can
mean many different things to different people. Though it started from a
pretty abstract notion of a series of interlinked documents, it has now
evolved to refer to a very specific technology stack of hyperlinked HTML
documents styled with CSS, enhanced with JavaScript, all served on top
of HTTP. In light of an increasing movement away from desktop-style
computing, we've seen a big shift away from the web in mobile platforms. </p>

<p>Let's take apart this gob of web technology in light of the increasingly
complex landscape of computing and try to make sense of what the web is
and where it's going.</p>

<p><img src="/ebb-of-the-web/webiness.png" alt="A framework for webiness" /></p>

<!--more-->

<h2>Webiness: how far down the rabbit hole?</h2>

<p>I want to introduce the concept of webiness, a framework for evaluating
how deeply an application embraces "the web":</p>

<p>Games are typical examples of apps that are <strong>pure native</strong> and not
webby at all (barring high score servers, etc). Because many are
playable offline, with no added benefit of having an internet
connection, there is obviously no room for web. They are built entirely
on native APIs, benefiting from being as close to the hardware as
possible for performance reasons.</p>

<p>Another class of native apps are Twitter, Facebook and the like, which
essentially act as <strong>specialized browsers</strong>. They largely use HTTP to
access RESTful endpoints that serve up JSON, which is rendered by the
native clients. Because a specialized browser is designed for a specific
use case in mind (eg. interacting with Twitter), it can be streamlined
for that purpose and doesn't need to deal with processing the web's
presentation layer (HTML, CSS, JavaScript). Therefore it presents a
more focused experience and benefits from being faster at start-up and
during interaction, offline support, and a generally better experience.</p>

<p><strong>Embedded browsers</strong> (also called hybrid apps) embed a webview into the
user interface of the page to use the web's presentation layer for part
of the user interface. How much of the user interface is created using
web technologies varies widely. This approach is beneficial because it
allows web developers to be featured in app stores, and also gives them
access to native APIs that are not available on the open web.</p>

<p>Lastly, <strong>browser-based</strong> pages and apps are ones where even the code
itself is fetched over HTTP. The app is written in a cross-browser way
to ensure that regardless of which browser loads the page, the user is
presented with a reasonable experience. The idea of this is to be able
to write once, deploy everywhere. Like the other types of apps on our
spectrum, these also use HTTP to interact with the server. All of the
content is rendered using HTML/CSS/JavaScript. Additionally, the HTML is
often hyperlinked.</p>

<h2>The web as greatest common divisor</h2>

<p>In mathematics, the greatest common divisor (GCD) between multiple
digits is the largest positive integer that divides the numbers without
a remainder. Observe that the GCD of a set of numbers (S) is by
definition less than or equal to each number. Furthermore, if the
numbers in S are not multiples of one another, the inequality
becomes strict. So <code>foreach n in S, gcd(S) &lt; n</code>.</p>

<p>In the webbiest case above, where we write once for all browsers, the
web user interface becomes the GCD for all existing interfaces. It is
guaranteed to be slower, less featureful, etc, than each individual
native platform, but when the web was conceived, the benefits outweighed
the costs. As the web evolved in the 90s, native platforms evolved
alongside it. Computing at the time was very desktop-centric, requiring
a physical keyboard, mouse, and relatively large display at, let's say
1024x768 pixels. At best, variance was between operating systems. The
computer geeks were on the Linux fringe. The art and music geeks used
Macs. But the hardware was pretty much the same.</p>

<p>Because of this uniformity, it was easy to standardize on a set of input
and outputs that would work reasonably well across a bunch of existing
computer configurations and operating systems. Computer hardware was all
very similar, just off by some factor. And the GCD of this orderly set
was pretty large: <code>gcd(200, 300, 400) = 100</code>.</p>

<h2>We're not in Kansas anymore</h2>

<p>Contrast this uniformity to today, when our mots du jour are "mobile
first", or even "mobile only". But even these notions are becoming
passé, as our day-to-day tech encounters start including wearable
sensors for health, chips embedded in your appliances, shoes, and
computers on your face. Even with the most conservative notion of what
mobile means - small screens and touch input - we've really thrown a
wrench into the big-screen, mouse-and-keyboard web. Scott Jenson is
absolutely right in remarking in an <a href="https://www.youtube.com/watch?v=6u03xYkwMVI">Edge conf panel</a>, that the
web hasn't even recovered from the fact that screens have gotten
smaller. With today's extended notion of computing, the GCD of all
of the devices and use cases the web is trying to support becomes very
small: <code>gcd(100, 200, 300, 50, 99, 198, 33) = 1</code>.</p>

<p>At the same Edge conf panel on the future of the web, somebody
asked the question of how the web would work on hardware without a
display. Answers from the panel were incoherent, but it's unclear how
this would be built into today's frankenweb, which is already a snowball
of many, often redundant technologies. And this is largely because of the
notion of <strong>THE WEB</strong> as a single platform. This is both its greatest
strength, and ultimately its tragic flaw, as the legacy of the early 90s
causes the singular web to cave in on itself, as we are experiencing
today.</p>

<p>We can no longer have a one-web-for-all approach. We need to focus on
having many different webs, each specializing on a particular subset of
our universe of devices. Taking our set above, we can split it in two
subsets, <code>S1 = {100, 200, 300, 50}</code>, and <code>S2 = {99, 198, 33}</code>.  Imagine
S1 are the desktop-like devices, and S2 are the phone-like devices. Now,
we have pretty okay GCDs: <code>gcd(S1) = 50</code>, and <code>gcd(S2) = 33</code>. Our worst
case GCD is now 33, which is a lot better than 1!</p>

<h2>The web is dead, long live the web!</h2>

<p>I'm pretty sure that HTTP is here to stay. Our desire for content is
universal, and that content needs to live somewhere. Regardless of how
that content is presented to us, it is likely to be served to us through
the cloud, over HTTP in the forseeable future.</p>

<p>What is indisputably being downplayed in the vibrant and variant near
future of computing, is the web's presentation layer - HTML, JavaScript
and CSS. These comprise the lingua franca of the web, and have no real
competition. However, this browser-served presentation layer of the web
will become less relevant as fewer things are done through the browser,
especially on mobile platforms.</p>

<p>To me, the critical thing is that content be addressable by URL, and
cross-linkable in some reasonable way. This is conventionally achieved
with HTML's <code>&lt;a&gt;</code> elements, but can also be done with JavaScript (eg. a
button that runs <code>javascript:window.location.href = myUrl;</code>, or a
<code>&lt;form&gt;</code> that creates a POST request to some other resource. This can
even be done without HTML at all. As long as we continue using HTTP, we
are guaranteed to have content that is available at a given URL. And as
long as we can guarantee that there's some handler for that content, the
spirit of the web lives on.</p>

<h2>Specialized browsers are a good solution</h2>

<p>RSS readers are good examples of specialized browsers focused on
presenting a good reading experience to the user. They are a single
entry point for all of the interesting things on the internet for the
user to read.</p>

<p>The Twitter app on your mobile device is an example of a specialized
browser designed for reading and sending tweets. This specialized
browser relies on the web's ability to link to various kinds of content
addressable by URLs. Tweets typically include an article or an image
which are served up using HTTP, and sometimes require HTML to render.
Twitter is all about hyperlinked content, without ever using an <code>&lt;a&gt;</code>
tag.</p>

<p>Apple also has several projects that are specialized browsers in spirit,
though they rarely link out to the wild west of the world wide
web. Generally, these specialized browsers focus on giving a great
experience for the user aiming to do something specific. Similar in
function to an RSS reader, Newsstand is an entry point to the magazines
and newspapers you read. Passbook is a specialized browser for tracking
event tickets, boarding passes and coupons. From a developer
perspective, both of these browsers require you to setup a server and
write some iOS code. (Unfortunately you're then stuck in Apple's
ecosystem forever.)</p>

<p>By identifying common patterns of functionality, Apple is able to
successfully introduce a specialized browser notion that spans across
multiple services, unifying it with a consistent user experience. This
begins to address the concern that there are <a href="http://designmind.frogdesign.com/blog/mobile-apps-must-die.html">too many apps for
everything</a>. Of course you don't want separate apps for
NYTimes, WSJ, USA Today, LA Times, etc. And of course you don't want
separate apps for each event booking service, airline and coupon
company.</p>

<p>The problem is endemic to the web community. The standards process
behemoth is slow, heavy and change-averse. Its focus is on a monolothic
web, <strong>THE WEB</strong>. Imagine if we focused on use cases in the same way
that Apple does, and created specialized sub-webs for various related
things. The next big thing is wearable health. Imagine a sub-web for
that, where we get to define the way all of these devices can talk to
one another. There would be a browser for that sub-web, providing a good
experience to see historical data, analyze trends, and plot data over
time. This is not the stuff of a web browser, but a completely different
beast.</p>

<p>Android's intent filters are a step in the direction of a specialized
browser. Intent filters let apps register to handle specific URL
patterns. If the URL pattern is opened by the user, she is presented
with a dialog of all of the possible handlers for that resource, which
may include Chrome, other web browsers, and specialized browsers that
subscribe to that URL. Once you're in an Android app, however, you're
stuck. Unless the app provides the ability to share content, there's no
way to send your state to someone else. In contrast, with a web browser,
you just take the URL and send it to your friend and if they have access
to that content, they get to see it.</p>

<h2>The web, the good parts</h2>

<p>When tech pundits hail the ebb of the web, they mean that mobile native
apps are eating away at the presentation layer on the mobile web and
beyond. After some contemplation, I have made peace with this
possible future. HTML is not the best possible way of creating content,
nor is CSS a reasonable way of laying out content. JavaScript may be
commonly used, but that does not make it a very good language. The
presentation layer is optimized for content consumption using pointers
and keyboard input, or if you're really adventurous, a smaller touch
screen.</p>

<p>Apps are another story. Frameworks like iOS and Android provide a much
more modern, cogent way of developing apps for mobile platforms. But
unfortunately they aren't linkable or portable across platforms. As I
mentioned earlier, there's no need for <code>&lt;a&gt;</code> elements, or anything from
the presentation layer to preserve benefits of linkability. But what
does the web look like once we've gutted the presentation layer?  Here
are some characteristics that we should preserve:</p>

<ul>
<li>The ability to take any content that is currently being shown and
serialize it into a URL.</li>
<li>The ability to open a URL with the right specialized browser of the
user's choosing.</li>
<li>If no specialized browser is installed, some way of presenting the
user with a list of good browsers.</li>
</ul>

<p>By dropping the notion of <strong>THE WEB</strong> (singular), and ushering an era of
specialized browsers, we can split our universe of devices into subsets
and increase the baseline greatest common denominator. Trying to extend
the web to work for every possible case will lead to even more feature
creep in a web platform that is already keeling over.</p>

<p>The web community should take a look at verticals and spec and build
specialized browsers for them. How would a web for boarding passes,
concert tickets and coupons look like? How about a web for personal
health tracking data? A web for content that should be consumed on an
audio-only device?</p>

<p>This has been my slightly edited brain dump on the future of the web.
Thanks for reading, I eagerly await your thoughts :)</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Remote controls for web media</title>
    <author><name>Boris Smus</name></author>
    <link href="/remote-controls-web-media"/>
    
    <updated>2014-01-27T09:00:00-00:00</updated>
    
    <id>http://smus.com/remote-controls-web-media</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>When the world wide web was first conceived, it was as a collection of
interlinked textual documents. Today's web is full of rich media.
YouTube and other video sites alone consume an enormous 53% of all
internet traffic. Web denizens often have an open audio player in one of
their tabs. Web-based photo sharing services such as Flickr are the most
common way of enjoying photos on our computers. The remote control,
foundations of which are attributed to everyone's favorite inventor
Nikola Tesla in <a href="https://www.google.com/patents/US613809">patent US613809</a>, has been the preferred way of
controlling media for over half a century.</p>

<p>Yet the only way we can control all of this web media is via the
on-screen user interfaces that the websites provide. The web has no
remote control, and this is a big usability problem. Many use the
desktop versions of streaming services like Spotify and Rdio rather than
their web player, exclusively because of mac media key support. For
scenarios where you're far from the screen, like showing friends a
slideshow of photos on a TV, the lack of remote controllability is a
non-starter.</p>

<p>This post is a concrete proposal for what a remote controls for the web
should be like. To get a sense for how it might feel, try a <a href="http://borismus.github.io/media-control-prototype/">rough
prototype</a>.</p>

<!--more-->

<p><img src="/remote-controls-web-media/inputs.png" alt="Ways of controlling media: dedicated keyboard buttons,headphone
remotes, hardware remote controls, second-screen remote controls,
camera-based gestures, voice commands" /></p>

<h2>Related attempts to solve this problem</h2>

<p>Many platforms, especially Android, Mac and iOS, do a pretty good job of
supporting some of the inputs from the above image. The web, one of the
most common platforms for consuming media, supports none of them. The
only exception, of course, is the mouse and keyboard, but only when the
player tab is in the foreground.</p>

<p>On the web, there have been a number of proposals and half-solutions to
address this problem. Back in 2011, I shared <a href="http://smus.com/chrome-media-keys-revisited/">KeySocket</a>, a
Menu Bar app for OS X that handles media keys on the mac keyboard and
sends them to a companion Chrome extension that injects content scripts
into web-based media players. A similar project, <a href="http://sway.fm/">Sway.fm</a> built
support for media keys as an NPAPI plugin (a now <a href="http://blog.chromium.org/2013/09/saying-goodbye-to-our-old-friend-npapi.html">deprecated</a>
technology). The <a href="https://flutterapp.com/">Flutter app</a> takes a similar approach (native
app and companion extension), but enables webcam-based gestures for
controlling media.</p>

<p>Recently, I contributed the Mac implementation to the new <a href="https://codereview.chromium.org/60353008/">global
keyboard shortcuts API</a> for Chrome Apps and
Extensions. This API lets developers bind to any global shortcut,
including media keys. This is a good start since it brings the media key
handling infrastructure into Chrome, but does not address the problem
for the web in general.</p>

<h2>Starting with a good user experience</h2>

<p>Since we have a blank slate when it comes to controlling media on the
web, how should media controls behave? Let's start with some
sub-optimal behaviors. Here's one: all media events to get routed to all
open tabs capable of handling them. Imagine the case with many YouTube
tabs open, and the ensuing cacophony! Another bad approach is to route
commands only to the foreground tab, since a very common case for
needing media controls occurs when music is playing in the background.</p>

<p><a href="http://borismus.github.io/media-control-prototype/">This rough prototype</a> illustrates what I think is a pretty
good experience. It follows a focus-based model inspired by mobile
operating systems like iOS and Android. However, the web is messier than
the app model and edge cases like multiple sources of media playing
simultaneously (eg. music player and YouTube video) are likely to
happen, so we need to be careful.</p>

<p>Here is what happens when a user issues a play/pause command. I'll
define the bold terms in a second.</p>

<ol>
<li>If any media is <strong>playing</strong> in a <strong>background tab</strong>, it should pause.</li>
<li>Otherwise, if the <strong>foreground tab</strong> supports <strong>media events</strong>, it
should receive the media control and be pushed to the <strong>media focus
stack</strong>.</li>
<li>Otherwise, if the <strong>media focus stack</strong> is non-empty, the event
should be routed to the tab at the top of the stack.</li>
<li>Otherwise (if the stack is empty), find the first open tab
supporting <strong>media events</strong>, relay the event to that page and push it
on the <strong>media focus stack</strong>.</li>
<li>If there are no open tabs supporting <strong>media events</strong>, do nothing.
Optionally alert the user with a non-modal notification (eg. audible
chime).</li>
</ol>

<p>When a next or previous control is issued, the command should be routed
to the tab with <strong>media focus</strong>. If there are no tabs with <strong>media
focus</strong> and none capable of media control, we drop the event.</p>

<p>If a tab closes, remove it from the <strong>media focus stack</strong> and ensure
that <strong>media focus</strong> is granted to the tab at the top of the stack.</p>

<p>To clarify the description above, here are a few terms:</p>

<ul>
<li><strong>Foreground</strong>: the active tab of the foreground browser window.</li>
<li><strong>Background</strong>: every tab that is not in the foreground.</li>
<li><strong>Media events</strong>: a new event type that a page can listen to,
indicating how to interpret media controls (see the next section).</li>
<li><strong>Playing tab</strong>: a tab that is currently playing audio or video
content.</li>
<li><strong>Media focused tab</strong>: the tab which is the default receiver of media
control events.</li>
<li><strong>Media focus stack</strong>: a stack of tabs where the top-most tab
is the one that currently has media focus. If that tab is
popped off the stack, the next one gets media focus.</li>
</ul>

<p>The dry description above and <a href="http://borismus.github.io/media-control-prototype/">the prototype</a> should give a
sense of what tab should handle basic media controls, regardless of
their origin: keyboard, remote control hardware, gesture, etc.</p>

<p>Now, when a command comes in, how does the page know how to interpret
it? That's up to the web developer, and is done through <code>media</code> events,
described in the next section.</p>

<h2>Enabling media controls using media events</h2>

<p>A fundamental missing piece so far is a way for a web page to indicate
that it can receive media controls, and a way for it to specify how it
wants to handle them. The solution to this is to create a new type of
event, the <code>media</code> event, which are defined on a page-level, bound to
the window object. This suggestion is not new, and first (as far as I
can tell) came up in this <a href="http://paulrouget.com/e/mediaevents/">blog post by Paul Rouget</a>. Here's
how media events work:</p>

<pre><code>// Subscribing to media events.
window.addEventListener('media', function(e) {
  if (e.data == e.MEDIA_PLAY) {
    myPlayer.play();
  } else if (e.data == e.MEDIA_PAUSE) {
    myPlayer.pause();
  } else if (e.data == e.MEDIA_NEXT_TRACK) {
    myPlayer.next();
  } else if (e.data == e.MEDIA_PREVIOUS_TRACK) {
    myPlayer.previous();
  }
});
</code></pre>

<p>This code tells the browser that this page can accept media controls,
and what this page should do when a particular media control is
received.</p>

<h2>Determining user-initiated media playback change</h2>

<p>Another missing piece in the narrative so far is how to populate the
focus stack. So far, we know that a closed tab should be popped from the
stack, and that play/pause sometimes causes a tab to be pushed onto the
stack. But this is not enough, since the user can still interact with
media using the UI of the player. For example, if I start listening to
music through a Spotify tab, and then switch tabs, media commands should
obviously go to the Spotify tab, despite me never having issued any
media controls.</p>

<p>One option is to, as the user navigates between tabs, push any tab with
supporting media events onto the stack. This approach fails for the case
where you are listening to music in the background, and then change
tabs, passing an open YouTube video on the way. In this case, that
YouTube video would become focused and there would be no way to control
the music (until you close the YouTube tab). What we actually need is to
be able to <strong>track when the user interacts with the page using the media
player UI</strong>, in order to then push page to the media focus stack.</p>

<p>A browser can distinguish user-initiated events (like clicks and
keyboard presses) from programatic ones (like a timer firing, or a page
loading). <a href="https://developer.apple.com/library/safari/documentation/AudioVideo/Conceptual/Using_HTML5_Audio_Video/Device-SpecificConsiderations/Device-SpecificConsiderations.html">iOS does this</a> to prevent annoying pages from
autoplaying music (remember the 90s?). Using the same idea, browsers can
also track when a media player's playback state changes due to user
input.</p>

<p>Even so, there may be special cases that aren't perfect. For example,
imagine a music app with media controls and a video ad on the side. If
the user then clicks the video ad, it doesn't mean that the page should
now have media focus. There are other tricky cases such as a page full
of videos. Here, if a user starts playing a particular video, and then
wants to stop it using media controls, the expectation is that the same
video pauses. If the web developer does not handle this case gracefully,
another video may start playing concurrently.</p>

<h2>Response at FOMS was positive</h2>

<p>Pitching this idea at <a href="http://www.foms-workshop.org/foms2013/pmwiki.php/Main/MediaFocus">FOMS 2013</a> a few months ago, folks seemed
receptive. There was an understanding that a lack of media controls on
the web is a genuine problem. Additionally, I got good feedback on the
solution, which helped to iterate and get to this stage. This is all
very encouraging, and I've written this post to keep the discussion
alive and keep the momentum going. To make remote controls for the web a
reality, we need is a critical mass of interested browser implementers.</p>

<p>As always, thanks for reading, and let me know if you have thoughts or
suggestions on this topic, especially if you make browsers for a living
and want to help standardize!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2013 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="/uist-2013"/>
    
    <updated>2013-10-25T09:00:00-00:00</updated>
    
    <id>http://smus.com/uist-2013</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I just got back from Scotland, where I had the pleasure of attending
UIST 2013 in St. Andrews. This was my second time attending, and again
it was incredibly engaging and interesting content. I was impressed
enough to take notes just like <a href="http://smus.com/uist-2011/">my last UIST in 2011</a>. What
follows are my favorite talks with demo videos. I grouped them into
topics of interest: gestural interfaces, tangibles and GUIs.</p>

<!--more-->

<h3>Quadrotor Tricks</h3>

<p>UIST kicked off with a very compelling demos from Rafaello D'Andrea,
professor at ETH, co-founder of Kiva. He currently works on the <a href="http://www.flyingmachinearena.org/">flying
machine arena</a>, a lab at ETH working on quadrotor control systems.</p>

<p>I really liked the flight assembled architecture idea: a building
assembled by quadrotors.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/JnkMyfQ5YfY" frameborder="0" allowfullscreen></iframe>

<p>Rafaello also showed off a kinect controlled quadrotor. A pointing
interface to control quadrotors. Other highlights included the ability
to place the quadrotor with your hand, and simulating environments like
controlled gravity, virtual walls, springs, and damped oscillations.</p>

<h3>Mime: Compact, Low Power 3D Gesture Sensing</h3>

<p>An MIT Media Lab group presented a pretty neat approach for gesture
tracking combining time-of-flight and RGB cameras. The approach is
compact enough to be embedded on a HUD device like Google Glass.</p>

<p>The specs are impressive: 100 FPS, sub-centimeter resolution, low-power
(45 mW). Showed glasses hardware with 3 cameras (baseline = face) and an
IR LED. Here's roughly how it works:</p>

<ol>
<li>Illuminate scene with IR. Backscatter light captured by cameras.</li>
<li>Time-of-flight approach. Source <code>s(t)</code> and response <code>r_n(t)</code>. Look
for time-shifted waveforms.</li>
<li>...Lots of crazy math reducing to convex optimization...</li>
</ol>

<p>Applications presented were a bit limited, mostly focused on in-air
writing and drawing. They also presented some cringe-worthy menu
navigation. The last and most obvious application was games.</p>

<h3>Gaze Locking: Passive Eye Contact Detection for Human–Object Interaction</h3>

<p>Surprisingly insightful project from Columbia based on a simple idea:
gaze tracking is hard. Knowing WHERE the user is looking is very
difficult, but knowing IF the user is looking is much easier. I loved
the approach of <a href="http://blog.kenperlin.com/?p=13296">solving the simpler problem</a>.</p>

<p>Detector approach:</p>

<ol>
<li>Eye corner detection</li>
<li>Geometric rectification</li>
<li>Mask eye area</li>
<li>Extract features from 96x26px rectangle.</li>
<li>PCA + MDA compression</li>
<li>Binary classifier (gaze locked or not).</li>
</ol>

<p>They also generated a Gaze Data set (6K images). The detector actually
does better than human vision. Works well from 18m away, though the
presenter claimed there was no degradation as a function of distance,
which was very suspicious.</p>

<p>They also presented a series of compelling applications:</p>

<ul>
<li>Human-object interaction (very cool video of iPads powering on based
on gaze).</li>
<li>Ad analytics (wow, incredible potential for Google/Signs team).</li>
<li>Sort/filter images by eye contact (as a measure of photo quality).</li>
<li>Gaze-triggered photography (when everyone is looking at the camera).</li>
</ul>

<p>More info on <a href="http://www.cs.columbia.edu/CAVE/projects/gaze_locking/index.php">the lab's site</a>.</p>

<h3>BodyAvatar: Creating 3-D Avatars with Your Body and Imagination</h3>

<p>Setting your avatar in video games is annoying. You basically go through
a wizard based on a GUI. This delightful implementation from Microsoft
Research uses your body to build your character's avatar. Creation
begins from the first person, as you create a general skeleton for the
avatar. Then the perspective changes to third person as you add
customizations using gestures. The final stage lets you paint your
avatar from the third person perspective.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/yU2Ai18tft4" frameborder="0" allowfullscreen></iframe>

<p>They also showed some impressive demos of stepping into limbs for
particularily complex models (eg. butterfly with 6 limbs). Super cool!</p>

<h3>Sauron: Embedded Single-Camera Sensing of Printed Physical User Interfaces</h3>

<p>Excellent work from Berkeley showing how a single camera can drive a
whole printed physical UI. The idea is that you 3D print an object,
insert a camera and have a fully functional input device.</p>

<p>Sauron simulates full motions of all components, ensures that everything
is visible via ray casting. One problem is that you can't always see the
whole interior. So Sauron modifies the design by extruding inputs,
adding mirrors.</p>

<p>A good question was asked about doing the same for output. Using a
transparent material you might also be able to light up specific areas
of the prototype, but apparently 3d printers can't print
transparent/translucent plastics. Cool future work might be to design
mobile tangibles that snap to a phone and use the phone's camera.</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/GNdCnmm-cw8" frameborder="0" allowfullscreen></iframe>

<p>Ok, that's all for vision and gestures. Now on to tangibles:</p>

<h3>PneUI: pneumatically activated soft materials</h3>

<p>Ishii's group presented nature-inspired interfaces that are
transformative and responsive. Using mostly air pockets, they set out to
create tangible UIs inspired by soft marine organisms. Some examples of
the applications:</p>

<ol>
<li><p>Curvature: folding wristband/phone. Wraps up when placed on wrist.
Unwraps when used as a tablet. Pulsates shape changes to indicate
incoming calls.</p></li>
<li><p>Volume-change based interfaces with underlying origami substructure.
Application: origami accordion with variable height and input.</p></li>
<li><p>Micro + macro elastomers to create transformable textures.
Application: "feel" GPS on the steering wheel rather than see/hear.</p></li>
</ol>

<iframe src="//player.vimeo.com/video/63591283" width="500" height="281" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe>

<p>Mind = blown.</p>

<h3>Paper Generators: Harvesting Energy from Touching, Rubbing and Sliding</h3>

<p>Disney research presented a way of harvesting energy from interaction,
primarily for popup book-type applications. Based mostly on static
electricity, they used teflon, which has low electron affinity. Rubbing
it on paper causes a discharge. Rubbing generates 500 µA, 1200 V.
Tapping generates 60 mW.</p>

<p>The approach is easy to build, printable with conductive ink cartridges.
In addition to rubbing, showed a bunch of different widgets that can
generate electricity - buttons, cranks, </p>

<p>Approach 1: direct energy usage. (eg. animations on e-ink displays.)
Approach 2: store and release if more energy is needed. (eg. actuate servos.)</p>

<iframe width="560" height="315" src="//www.youtube.com/embed/4WaUcXSfPTg" frameborder="0" allowfullscreen></iframe>

<h3>Touch &amp; Activate: Adding Interactivity via Active Acoustic Sensing</h3>

<p>Tsukuba University presented a very cool paper on adding acoustic
sensing to hard objects using contact mics and speakers. The basic idea
is that touching an object changes its bounding conditions, depending on
how it is touched. </p>

<p>The way it works is they vibrate objects at a wide frequency range and
capture the response.</p>

<ol>
<li>Attach contact speaker and microphone.</li>
<li>Make the object vibrate, doing a sweep signal from 20-40 KHz (inaudible).</li>
<li>Vibration response determined by object properties.</li>
<li>Extract features via FFT</li>
<li>Classify via SVM</li>
</ol>

<p>Applications:</p>

<ul>
<li>Simple music player based on duplo blocks.</li>
<li>Interactive animal body</li>
<li>Grasp recognition system for phone using a case.</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/XgxXi6w8IQc" frameborder="0" allowfullscreen></iframe>

<p>How cool is that? Anyway, now for something a bit more traditional:</p>

<h3>Transmogrifiers: Casual Manipulations of Visualizations</h3>

<p>University of Calgary presented their awesome visualization toolkit.
Their goal is to enable exploration and manipulation of data that is
stored in images with no underlying data.</p>

<p>The idea is to pick a "lens" shape which acts as a template and is
placed on an image. Also provide an output shape to serve as the target.</p>

<p>Applications:</p>

<ul>
<li><a href="http://upload.wikimedia.org/wikipedia/commons/5/5a/1862_Johnson_and_Ward_Map_or_Chart_of_the_World%27s_Mountains_and_Rivers_-_Geographicus_-_MtsRvrs-j-1861.jpg">Tracing rivers to 1D</a> to compare their lengths.</li>
<li>Mutate data chart types (eg. ring chart ==&gt; bar chart)</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/S1Roi2NOmx8" frameborder="0" allowfullscreen></iframe>

<h3>Content-based Tools for Editing Audio Stories</h3>

<p>A Berkeley PhD student showed his project, which aims to edit audio
stories (radio shows, podcasts, audio books) at a semantic level much
higher than the current industry standard (waveforms). Not that
technically challenging, just a really cool idea. Might be a very
compelling product.</p>

<p>Cool interactions:</p>

<ul>
<li>Edit speech (eg. copy, paste) in a text editor.</li>
<li>Lets you pick sentences from a list of takes.</li>
<li>Insert breaths and pauses where needed.</li>
<li>Retarget music by segmenting song by beats and automatically finding music change points.</li>
<li>Specify speech emphasis points manually, and use them as alignment points to music change points.</li>
</ul>

<iframe width="560" height="315" src="//www.youtube.com/embed/RHtI4G5L31w" frameborder="0" allowfullscreen></iframe>

<p>Here's to <a href="https://twitter.com/ACMUIST/status/390958095939407872">next UIST</a>. Hang loose!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Responsive image workflow</title>
    <author><name>Boris Smus</name></author>
    <link href="/responsive-image-workflow"/>
    
    <updated>2013-09-09T09:00:00-00:00</updated>
    
    <id>http://smus.com/responsive-image-workflow</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>About a year ago, I wrote an overview of many of the different <a href="http://www.html5rocks.com/en/mobile/high-dpi/">responsive
image approaches</a> in an HTML5Rocks article, all of which try to solve the
fundamental problem:</p>

<p><strong>Serve the optimal image to the device.</strong></p>

<p>Sounds simple, but the devil's in the details. For the purposes of this here
discussion, I will focus on optimal image size and fidelity, and much to your
chagrin, will completely ignore the art direction component of the problem.</p>

<p>Even for tackling screen density, a lot of the solutions out there involve a
lot of extra work for web developers. I'll go into two solutions (client and
server side) on the horizon that serve the right density images. In both cases,
all you need to do is:</p>

<pre><code>&lt;img src="img.jpg"/&gt;
</code></pre>

<!--more-->

<h2>Nobody cares about responsive images (that much)</h2>

<p>Let me start with an underlying problem: for one reason or another, most developers
don't really care that much about responsive images. Even if left unsolved,
the images still get to their destination, they're just a little crummier than
they should be. If fidelity doesn't matter much to you and your app, then no
big deal.</p>

<p>Others may not even know about the problem. If you're not a high density screen
user, you may have not been disappointed by the gulf in quality between crisp
images in native apps and blurry images in web apps. Some applications may prioritize
performance over fidelity, and want to deliberately send low resolution images.</p>

<p><strong>A vast majority of devs know about the problem, but are just waiting for a
solution that works well</strong>. We're all inherently lazy and in my opinion, a
reasonable solution is one that requires little to no extra work.</p>

<h2>Good solutions require almost no extra work</h2>

<p>How can we serve the optimal image to the device with as little work as
possible?  One approach is to always serve a highly compressed but high density
image, as I outlined in <a href="http://www.html5rocks.com/en/mobile/easy-high-dpi-images/">Easy High DPI Images</a> on HTML5Rocks. This
approach is better than nothing, but isn't really optimal since you end up
sending high density images to low density screens.</p>

<p>Two promising standards are on the horizon to wider adoption: the <code>srcset</code>
attribute for <code>img</code> elements, and the <code>CH</code> client hint header.</p>

<h3>Solution 1: Client-side build step with srcset &amp; friends</h3>

<p>The <code>srcset</code> attribute <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">recently landed in WebKit</a>, and it looks like
others will follow. Though it's more terse than <code>&lt;picture&gt;</code> and friends, <code>srcset</code>
still requires quite a bit of extra work to implement:</p>

<pre><code>&lt;img src="img.jpg" srcset="img-1.5x.jpg 1.5x, img-2x.jpg 2x, img-3x.jpg 3x"&gt;
</code></pre>

<p><a href="http://www.w3.org/TR/css4-images/#image-set-notation"><code>image-set</code></a> is the CSS equivalent, and looks quite similar.
Unfortunately it requires even more work:</p>

<pre><code>selector {
  background-image: url(img.jpg);
  background-image: -webkit-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -moz-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -ms-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  background-image: -o-image-set(
      url(img-1.5x.jpg) 1.5x, url(img-2x.jpg) 2x, url(img-3x.jpg) 3x);
  /* Hehe, moar prefixes! */
}
</code></pre>

<p>Phew! After you have exploded your markup, you need to generate multiple images
of different sizes and decide on appropriate compression levels for each.</p>

<p>You'll notice that this extra work is very formulaic. It almost looks like it
could be automated! Let's skip the busywork and write our web pages like we do
today, specifying a very high quality asset (eg. 3x), and running a build
script. In your markup, all you need to do is:</p>

<pre><code>&lt;img src="img.jpg" /&gt;
</code></pre>

<p>or</p>

<pre><code>selector {
  background-image: url(img.jpg);
}
</code></pre>

<p>This magic time-saving script would need to do two things. First, it
generates images:</p>

<ol>
<li>Find all image files on the site.</li>
<li>Downsize all image files to the right size depending on desired density breakpoints (eg. <code>1x, 1.5x, 2x, 3x</code>).</li>
<li>Name the images according to some convention (eg. <code>${image}-${density}.${format}</code>).</li>
</ol>

<p>Image resizing already has a <a href="http://addyosmani.com/blog/generate-multi-resolution-images-for-srcset-with-grunt/">grunt-based solution</a>, and many
others will surely follow. The second part is rewriting the HTML and
CSS. Here's how it works:</p>

<ol>
<li>Parse all image references from HTML (eg. <code>img</code>) and CSS (eg. <code>background</code>,
<code>background-image</code>).</li>
<li>Augment all HTML <code>img</code> elements with the right srcset. Augment all CSS
<code>background</code> and <code>background-image</code> properties with the right (and prefixed)
image-set value.</li>
</ol>

<p>Now we're talking! And all you need to do is provide one set of high quality
image assets and add this script to your build step (you have a build step,
right?). Keep writing those <code>&lt;img src&gt;</code>s!</p>

<h3>Solution 2: Server-side build step with Client-Hints</h3>

<p>The <a href="http://tools.ietf.org/html/draft-grigorik-http-client-hints-00">Client-Hints proposal</a> (CH) is another promising (read: minimal
developer effort required) future direction that would help solve the
responsive image problem on the server. Ilya Grigorik goes into much
more detail in <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">his post</a>.</p>

<p>Currently, the main thing a server has to identify a client is its
User-Agent (UA) header. The UA header is insufficient to infer basic
things like display density, even in conjunction with a <a href="http://en.wikipedia.org/wiki/WURFL">UA
database</a>. CH is a new header used to pass information to the
server about the user agent.  With it, you can specify the
<code>devicePixelRatio</code> (DPR) of your device explicitly:</p>

<pre><code>CH: dpr=2
</code></pre>

<p>Once browsers send this CH header, you can imagine some really simple
server-side logic to serve the best asset for the DPR specified. You will need
either a smart image generator (and cache) on the server, or a build script for
generating images at different densities. Luckily this build script is the same
as the first half of solution 2, so less work for us! Once the images are
generated, it's just a matter of producing the right redirects based on the CH
header, which Ilya provides his <a href="http://www.igvita.com/2013/08/29/automating-dpr-switching-with-client-hints/">article</a>.</p>

<p>One benefit of solving this problem server side is that it's universal
and completely transparent to the client. A drawback to the first
(client-side) solution is that it will not work when setting <code>&lt;img src&gt;</code>
with JavaScript, although this can be remedied easily with a loader that
you use to specify the image asset. In practice, instead of specifying
the image asset directly, you would need to go through a small image URL
rewriter. Imagine something like this:</p>

<pre><code>var imagePath = images.get('img.jpg');
// imagePath is now img-2x.jpg if on a 2x display.
imageEl.src = imagePath;
</code></pre>

<p>Another benefit of the server-side approach is that there's no need for
parsing HTML and CSS (the second part of the build step) which can be
tricky and error prone.</p>

<h2>Both solutions are good</h2>

<p>In summary, both solutions have merit, and since <code>srcset</code> has <a href="https://www.webkit.org/blog/2910/improved-support-for-high-resolution-displays-with-the-srcset-image-attribute/">momentum
already</a>, it should be standardized and broadly supported as soon as
possible. Many designers may not have access to server side configuration, so
for them the client-side build script would make sense. Conversely, many
developers that have access to server-side image generators and advanced
caching techniques should take advantage of Client-Hints once it's
available, which <a href="https://groups.google.com/a/chromium.org/forum/#!topic/blink-dev/c38s7y6dH-Q">may be soon</a>!</p>

<p>Now, to write that build script... Any volunteers?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Ultrasonic networking on the web</title>
    <author><name>Boris Smus</name></author>
    <link href="/ultrasonic-networking"/>
    
    <updated>2013-08-08T09:00:00-00:00</updated>
    
    <id>http://smus.com/ultrasonic-networking</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>The phone in your pocket is an amazing, fluid, multi-functional tool.
When it comes to talking to other devices, such as your TV or laptop,
the user experience drops off sharply. Bill Buxton <a href="http://www.youtube.com/watch?v=ZQJIwjlaPCQ&amp;feature=youtu.be&amp;t=21m00">speaks
eloquently</a> on the subject, describing three stages of
high tech evolution:</p>

<ol>
<li>Device works: feature completeness and stability</li>
<li>Device flows: good user experience</li>
<li>Many devices work together</li>
</ol>

<p>But connecting devices is a pain and we have been squarely at stage 2
since the release of the iPhone. There are many competing approaches to
do this: Bluetooth, Bluetooth LE, WiFi direct, discovery over the same
local WiFi network, and many many others. This post is dedicated to
attacking this problem from an unexpected angle: using ultrasound to
broadcast and receive data between nearby devices. Best of all, the
approach uses the Web Audio API, making it viable for pure web
applications:</p>

<iframe width="640" height="360" src="//www.youtube.com/embed/w6lRq5spQmc" frameborder="0" allowfullscreen="true"></iframe>

<!--more-->

<h2>A device tower of babel</h2>

<p><a href="http://www.apple.com/airplay/">Airplay</a> and <a href="http://www.google.com/chromecast">Chromecast</a> are great approaches to a subset of the
problem for devices within the same ecosystem (eg. Apple, or Google),
but the general problem remains hard to solve.</p>

<p>Because there are so many possible technical approaches, chances are
that the pair of devices that you happen to be using don't have a common
language to speak. Even if both devices have Bluetooth, one of them may
require a profile the other doesn't support, or support a different
version of the standard. This is especially common with Bluetooth today,
where many devices have the hardware to support Bluetooth 4.0 (aka
BTLE), but many devices don't currently support the new protocol for
various reasons.</p>

<p>On the web, the problem is even worse, since low level device connection
APIs aren't exposed for <a href="http://smus.com/installable-webapps/">security sandbox reasons</a>. Because of
how slowly the web evolves, it's hard to imagine this changing any time
soon.</p>

<h2>Transmitting data in interesting ways</h2>

<p><a href="http://www.youtube.com/watch?v=sVWlQNzU4Ak">Blinkup from Electric Imp</a> is an interesting approach to
cross-device communication. It uses a series of blinks to transfer
configuration data between a smart phone and an Imp, a small SD-card
shaped device with a light sensor.</p>

<p>Dial-up modems did a similar thing. They encoded and decoded digital
data onto an analog phone line. Remember those annoying connection
noises? Dial-up modems would turn on their speaker to give the user an
idea of how the handshake is progressing. If you don't remember, here's
a <a href="http://www.windytan.com/2012/11/the-sound-of-dialup-pictured.html">refresher</a>. Even today on analog phones, the sounds you hear
when pressing numbers on a dialer correspond to the frequencies the
phone system uses for analog-to-digital conversion. The conversion
happens using <a href="http://en.wikipedia.org/wiki/Dual-tone_multi-frequency_signaling">Dual-tone multi-frequency signaling (DTMF)</a>.</p>

<p>Your phone and a lot of other devices around you has a speaker and a
microphone. These two pieces of hardware can be used for sending and
receiving data using sounds, similar to how modems did it over phone
lines. Better yet, if the OS supports high enough frequency sending and
receiving, we can create an inaudible data channel.</p>

<h2>Transmitting data using sound</h2>

<p>I should note that encoding data in sound is not new. The idea of <a href="http://en.wikipedia.org/wiki/Audio_watermark">audio
watermarking</a> is to encode a signature into music that is not
discernable by the listener (due to the way humans hear), but can be
picked up by a machine. This is used as a clever piracy detection
scheme. </p>

<p>Most commodity speakers are capable of producing sound with a 44.1KHz
sample rate (resulting in a maximum frequency of about 22KHz by the
<a href="http://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Nyquist-Shannon sampling theorem</a>). This lets us encode data
not just as sound, but as sound that adults can't hear. Children and
non-human animals are still susceptible, though :)</p>

<p>One technical caveat is that microphones are sometimes not as capable as
speakers, especially in phones, since they are often optimized for human
speech, which sounds fine with a lower sample rate. In other cases,
even though the hardware is capable, the firmware runs at a lower sample
rate for energy efficiency. If this is the case, one of the devices will
not be able to receive the wave and the sound-based connection will be
one-way only.</p>

<h2>Sonicnet.js, a web audio implementation</h2>

<p>To illustrate these concepts, I built a <a href="https://github.com/borismus/sonicnet.js">JavaScript library</a>
that can send and receive data as sounds. My approach is and not nearly
as sophisticated as the audio watermarking technique, and even simpler
than the DTMF approach. Basically, you can specify a range of
frequencies to use, and an alphabet of characters that can be
transmitted. The frequency spectrum is split into ranges corresponding
to the specified alphabet and start/end codes, with each character/code
corresponding to a part of the full frequency range.</p>

<p>The sending side converts each character of the word to be sent into the
center of the corresponding frequency range, and transmits that
frequency for a certain duration. The receiving side does a continuous
fourier transform of the signal and looks for peaks in the specified
frequency range. Upon finding a peak for a significant duration, it does
the conversion back from frequency to character. This is essentially
a <a href="http://en.wikipedia.org/wiki/Selective_calling#Tone_burst_or_single_tone">single-tone multi-frequency signaling (STMF)</a> scheme.</p>

<p>There is a timing issue: on the sending side, how long should each
character be transmitted for, and on the receiving side, how long should
the listened for? An easy workaround for this is to disallow adjacently
repeated characters.</p>

<p>I built a socket-like API for sonic networking. Client code
looks like this:</p>

<pre><code>ssocket = new SonicSocket({alphabet: '0123456789'});
function onButton() {
  ssocket.send('31415');
}
</code></pre>

<p>And the server can look like this:</p>

<pre><code>sserver = new SonicServer({alphabet: '0123456789'});
sserver.on('message', function(message) {
  // Expect message to be '31415'.
  console.log(message);
});
sserver.start();
</code></pre>

<p>The library is available for use on <a href="https://github.com/borismus/sonicnet.js/tree/master/lib">github</a>.</p>

<p>Of course, using it requires a Web Audio implementation (mostly
<code>OscillatorNode</code> on the sending side, and <code>AnalyserNode</code> on the
receiving side) and good enough hardware. I have experimented with
Chrome-to-Chrome transmission on Mac Books, as well as between Chrome
for Android (beta) and Chrome for Mac.</p>

<p>I wrote a couple of demos to illustrate the idea. These appear in the
<a href="http://www.youtube.com/watch?v=w6lRq5spQmc">video I embedded at the top of the post</a>. The first demo lets
you <a href="http://borismus.github.io/sonicnet.js/emoticons">send emoticons</a> from one device to the other. It uses a
small alphabet of just 6 characters - one for each emoticon. You pick
one of 6 emoticons, and the corresponding character is sent over the
sonic network, received and shown prominently on the other end.</p>

<p>A more realistic use for sonicnet.js is this <a href="http://borismus.github.io/sonicnet.js/chat-pair">chat
application</a>, which generates a non-repeating 5-digit token
and uses it to create connections between two devices. This is done with
the help of a pairing server, which helps establish a proxied connection
between the two devices, over a websocket. Once the connection is
established, the chats themselves are sent through the websocket. The
<a href="https://github.com/borismus/sonicnet.js/tree/master/server">server code</a> is hosted on <a href="https://www.nodejitsu.com/">nodejitsu</a>.</p>

<h2>Conclusions and a request</h2>

<p>It's great to see that the Web Audio API has come far enough that
applications like these are possible. I'm fascinated by the implications
of sonicnet.js for the Web of Things. It is a pure web technology that
can be used to pair devices together. Because of the ubiquity of web
browsers and audio hardware, the combination can be a huge win, even
among commodity hardware, without having to wait for Bluetooth and other
close-range connectivity technology to become available to the web
platform.</p>

<p>If this post has piqued your interest and you are interested in helping,
try writing an app using sonicnet.js. As I mentioned earlier, receiving
high frequency sounds does not work on all devices because of
firmware/hardware limitations so I'd love to know which devices it does
and does not work on. My expectation is that most phones should be able
to send only, and that most laptops should be able to both send and
receive. Please fill out <a href="https://docs.google.com/forms/d/1dAgNdVdhss-QR-Owm556RZch-MV_ntnAMP8_ZJi5XLA/viewform">this form</a> once you try the <a href="http://borismus.github.io/sonicnet.js/emoticons">emoticons
demo</a> on your own hardware. At the time of writing, <a href="http://crbug.com/242894">live
input is not supported</a> in Chrome for Android Beta, so sending
data from mobile device to laptop is the only possible configuration.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Easier link blogging</title>
    <author><name>Boris Smus</name></author>
    <link href="/easier-link-blogging"/>
    
    <updated>2013-07-29T09:00:00-00:00</updated>
    
    <id>http://smus.com/easier-link-blogging</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>As usual, I want two conflicting things. Firstly, I want to own the
content I write, and control how it is authored. My weapon of choice is
MacVim and <a href="https://github.com/borismus/lightning">Lightning</a>, a static blog engine I wrote to
address my <a href="http://smus.com/site/">very specific requirements</a>: </p>

<p><img src="/easier-link-blogging/composing-long-post.png" alt="Composing a blog post with Lightning" /></p>

<p>Secondly, I want people to read the things I write and follow the
stories that I link to, since it feels good, and sometimes generates
interesting discussions. I wrote a Mac GUI that automates link blogging
and <a href="http://indiewebcamp.com/POSSE">POSSE style</a> cross-posting to social networks.</p>

<!--more-->

<p>Before I carry on, let me take you back to a simpler time. A time before
Justin Bieber, Black Eyed Peas and social networks. Here's how blogging
used to work: writers wrote to their blogs, and readers subscribed to
them. They would read blog posts from RSS feed readers. Some people ran
link blogs that were kind of like twitter -- short updates discussing a
link. It's was a very clean, federated model. Unfortunately that's not
how things work today. I have already lamented this fact in a <a href="http://smus.com/really-simple-social-syndication/">previous
blog post</a>.</p>

<p>For this site, RSS feed readership is a tiny fraction of the inbound
traffic for new posts. Most of non-search traffic comes from social
networks. As it turns out, the value of these networks is (wait for it)
in their network effect! It only takes a few prominent reshares to have
a post become relatively widely read. By not seeding your content to
social networks, you lose that benefit.</p>

<h2>Cross-posting to social networks is hard</h2>

<p>To seed content to social networks, we look to POSSE. From the
<a href="http://indiewebcamp.com/POSSE">indiewebcamp.com</a> definition:</p>

<blockquote>
  <p>POSSE is an acronym/abbreviation for Publish (on your) Own Site,
  Syndicate Elsewhere. It's a Syndication Model where the flow involves
  posting your content on your own domain first, then syndicating out
  copies to 3rd party services with perma(short)links back to the
  original version.</p>
</blockquote>

<p>The problem is that posting to social networks is a pain in the rear.
Each network has its own limitations (eg. 140 characters on Twitter,
network-specific markup on G+). There are services which cross-post to
these networks, but they tend to sweep the subtle differences between
the networks under the carpet. For example, services like
<a href="http://manageflitter.com/">ManageFlitter</a> and <a href="http://friendsplus.me/">Friends+Me</a> can cross-post from G+ to
Twitter, but if the post is too long to fit in 140 chars, they include a
link back to the original G+ post. I find cross-linking between social
networks to be questionable, so I have stopped using such tools.</p>

<h2>Making link-style updates easier</h2>

<p>A while ago, I realized that my use of social networks is remarkably
close to a link blog. While I'll sometimes @reply/comment and +1/star
things, I hardly ever post broadcast-style updates without a URL.</p>

<p>About a year ago, I added a special "link" type of content on this blog,
specially for this purpose. This type of content is just like a post,
except one link is prominently shown as the title of the post, and the
post itself is focused on commentary about the link. My plan is to use
this type of content more, whenever I want to broadcast a URL and have
something to say about it.</p>

<p>So to scratch my itch, I made a little GUI that automates creating a
link entry on the local static blog (with commentary). After the link is
posted and deployed publicly, it also broadcasts the content to
supported social networks, sometimes linking back to the link page on
this site.</p>

<p><img src="/easier-link-blogging/lightning-link.png" alt="Lightning linker Mac GUI" /></p>

<p>As you can see, there are three fields: the URL, a title (pre-populated
from the <code>&lt;body&gt;&lt;title&gt;</code> of the URL), and the body of the post (in
Markdown), all of which are clearly visible in the link page.
Technically, posting to social networks is easy enough. If they provide
a write API, it's just a matter of doing the OAuth dance and hanging on
to an access token to authorize requests. A more interesting question is
how to re-arrange the above three fields to form social network updates.</p>

<h2>Posting to other networks</h2>

<p>Once the link is posted on the site, Lightning Link has a set of
heuristics to decide what to post to each supported social network. This
can be quite challenging if you are faced with a strict character limit.</p>

<p>Here are some options I considered for Twitter:</p>

<ol>
<li>Truncated plaintext body, followed by the URL</li>
<li>Title colon space, the truncated plaintext body, and then the URL</li>
<li>Title and URL</li>
</ol>

<p>With Option 1, the truncated plaintext body doesn't necessarily reflect
the main idea of the commentary on the link, since the body can be much
longer than 140 chars, and I might just be warming up :) Option 2 leaves
very little room for the body at all, except for about half of a
sentence. I went with Option 3, which lends itself well to short,
Twitter-style updates.</p>

<p>Posting to G+ is relatively easy: take the title and slap on the
plaintextified (from markdown) body, while attaching the URL. </p>

<p>The other question is about URLs. There are two URLs of relevance in
each link blog post:</p>

<ol>
<li>The one on the link blog (eg. <a href="http://smus.com/link/2013/not-terrible-javascript-modules/">http://smus.com/link/2013/not-terrible-javascript-modules/</a>), and</li>
<li>The linked material (eg. <a href="http://github.com/substack/node-browserify">http://github.com/substack/node-browserify</a>).</li>
</ol>

<p>My approach is to use the linked material directly (2) if the comments
can fit entirely into the space alotted by the social network, falling
back to the link blog URL (1).</p>

<p>The Lightning Link app isn't general enough for me to recommend unless
you either like pain, use lightning already, or have a static blog very
setup similar to mine, with a "link" type of post. If you'd still like
to try it, the <a href="https://github.com/borismus/lightning/tree/master/link">code is on github</a>. If you have a similar
link-blogging approach with automatic syndication to social networks,
tell me about it!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Installable webapps: extend the sandbox</title>
    <author><name>Boris Smus</name></author>
    <link href="/installable-webapps"/>
    
    <updated>2013-06-25T09:00:00-00:00</updated>
    
    <id>http://smus.com/installable-webapps</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><style>
article img.border {
  margin: 0 auto;
  max-width: 100%;
  box-shadow: inset 0 0 10px #999
}
</style>
<a name="problem"></a></p>

<p>Have you seen the <a href="http://extensiblewebmanifesto.org/">extensible web manifesto</a>? It's the
formalization of a recent trend in web standards: a tendency towards
lower level APIs. Lower levels of abstraction enable developers to build
more on top of a solid foundation. By going down a level of abstraction
in the web platform, web developers can contribute to the platform
itself in a more fundamental way, working along with browser vendors and
spec writers. This is <a href="/how-the-web-should-work/">how the web should work</a>.</p>

<p>But there is a big missing piece in the extensible web vision. Our
beloved platform is stuck in a constrictive security sandbox. The "drive
by" web's security philosophy is that users of the web should be able to
feel safe on any webpage they visit. While very important for the well
being of web denizens, it prevents developers from using increasingly
important features enjoyed by native platforms such as access to
contacts, TCP/UDP sockets, interfaces to external USB/Bluetooth devices.
Breaching this sandbox is a huge barrier for the web as a compelling
application platform.</p>

<p>Some recent features, such as <code>getUserMedia</code>, which gives web developers
access to the audio and video streams of your device's camera and
microphone, have started to break out of the sandbox. There are two
approaches to this problem today: (a) infobars and (b) packaged apps. In
the rest of this post I'll describe why these are bad solutions,
deconstruct them down into small pieces and then glue the pieces back
together. The ultimate goal is a modest proposal for installable web
apps. Read on for my take on the background of the problem, or skip
ahead to read my <a href="/installable-webapps/#solution">illustrated proposal</a> for fixing it.</p>

<!--more-->

<h2>The extensible web is a good idea</h2>

<p>There are many recent examples of the extensible web philosophy across
many areas of the web. Because of the low level nature of WebGL and Web
Audio, these technologies open up a wide variety of applications to be
built. Under the hood, these APIs are relatively thin wrappers around
underlying native technology, not compromising performance (much) while
adding developer usability. General purpose low level computing
technologies like <a href="http://asmjs.org/">asm.js</a> and <a href="https://developers.google.com/native-client/">NaCl</a> enable computationally
intensive algorithms to run far more efficiently. </p>

<p>Finally, frameworks like <a href="http://www.polymer-project.org/">Polymer</a> use other kinds of low level
APIs like <a href="http://www.youtube.com/watch?v=fqULJBBEVQE">web components</a> and <a href="https://github.com/Polymer/mdv">model-driven views</a> to let
developers invent new types of HTML elements with custom functionality. </p>

<h2>Sandbox vs. low level APIs</h2>

<p>Restrictive web security makes a lot of sense. You should never have to
worry about malicious or careless developers erasing files from your
local filesystem, even if you frequent the most notorious <code>.ru</code> domains!
That said, if the web is to be a viable application platform that stands
a chance against native platform, it needs to have access to certain
data that is sensitive.</p>

<p>Many of the APIs that align with the extensible web philosophy have
already tested the bounds of the web's sandbox. Some have resulted in
security vulnerabilities, such as 2011's <a href="http://blog.chromium.org/2011/07/using-cross-domain-images-in-webgl-and.html">cross-domain WebGL texture
attack</a>. Others have required extending the web platform
with additional levels of security. The earliest of these is probably
the geolocation API. More recent additions include <code>getUserMedia</code>, which
gives developers a stream of the microphone and camera. These APIs could
obviously lead to very serious privacy breaches if turned on by default
on all web pages. I don't want the Russians knowing where I live, or
eavesdropping on my conversations (NSA already knows).</p>

<p>The "drive-by" web solves this problem through infobars. I will explain
later why this is a terrible idea. The other solution is packaged web
apps. Packaging circumvent the web completely by copying the
distribution model of native apps, bundling your whole application into
a locally downloaded zip. This model also features an installation step
which sometimes also grants additional permissions up front. Both of
these so-called solutions reduce the likelihood of new low level APIs
from coming to the web platform.</p>

<h2>Infobars are a bad user experience</h2>

<p>First, look at this:</p>

<p><img src="/installable-webapps/infobar-apocalypse.png"/></p>

<p>This gem, courtesy of <a href="http://persistent.info">Mihai Parparita</a>, is my favorite
explanation for why infobars suck. You can immediately see several
problems stemming from the obvious fact that the infobar model does not
scale well. In the infobar world, each feature requires its own
permission, leading to far too many stacked dialogs that are just ugly.
From a usability perspective, your users have to click through each one
of the Allow/OK dialogs before they can do anything with your
application. If you then reload the page, many of these infobars will
again return to haunt you, forcing you to click OK five more times
before being able to use the webapp.</p>

<p>In some cases, your browser might remember that you accepted an infobar,
and choose not to show it to you again. For example, this happens if you
grant <code>getUserMedia</code> access on an HTTPS site, selecting the "save this
preference" option. This is remembered on a per-domain basis, and in
Chrome, is available via <code>Preferences -&gt; Content Settings</code>. In general,
conditions for when exactly the browser remembers how you responded to
an infobar are unclear and underdefined.</p>

<p>There are also some less obvious issues with the infobar model. Because
infobars are non-modal, users often don't realize that they have to
accept them before they can use the webapp's functionality. For example,
if you have a <a href="http://webcamtoy.com">photo booth application</a>, it will be
completely useless until you accept the "access to video stream"
infobar, yet many of your users may not notice the infobar at the top of
your browser window. If you attempt to draw your users' attention to the
infobar via some illustration, you may end up pointing to the Deny
button by accident because of variations in placement across various
browsers and browser versions.</p>

<p>To summarize, infobars are broken in the following ways:</p>

<ol>
<li>Does not scale with number of permissions.</li>
<li>Visually jarring at scale. Sometimes not visually obvious enough.</li>
<li>Permission granting should often be modal.</li>
<li>Inconsistent persistence, poor management.</li>
</ol>

<p>Part of the problem might be addressable via a <a href="https://code.google.com/p/chromium/issues/detail?id=250797">visual
refresh</a> of infobars (as Paul Neave suggests), but I
suspect that a broader rethink of the problem is in order. Until this is
resolved, many new low level APIs will increases Mihai's stack of
apocalyptic infobars, reducing their chance of coming to the platform in
the first place.</p>

<h2>Packaged apps...</h2>

<p>Packaged apps are an odd marriage between native app distribution and
web technologies. The packaged web app model consists of a few moving
parts:</p>

<ul>
<li>A directory for discovering and installing apps (eg. Firefox
Marketplace, Chrome Web Store).</li>
<li>A set of platform-specific APIs built on top of the web platform for
use in these apps.</li>
<li>A manifest describing each app, which can specify permissions to
enable either the above platform specific APIs or restricted open web APIs.</li>
</ul>

<p>There are certainly benefits to this approach, such as a sane offline
story, since all of the assets of the application can be packaged
together into a bundle that is downloaded at install time, circumventing
painful technology like <a href="/installable-webapps/appcache.png">AppCache</a>. There is a clear install
step, during which you can grant an application permissions beyond the
scope of the open web platform. Also, it's very easy to add features to
packaged apps, since there are no annoying standards to worry about,
amirite?</p>

<h2>...are bad for the web</h2>

<p>Unfortunately, packaged web apps provide the worst of both worlds,
combining relatively poor web developer ergonomics with the longer
development and distribution cycle of native apps. Also, many of the
drawbacks of packaging are at odds with the philosophy of the open web
platform.</p>

<p>The first and most obvious problem is the lack of URLs for packaged
apps. URLs are critically important as unique identifiers for content
found on the web. They are great for sharing content, indexing, and
bookmarking. Secondly, there is no standard packaged app format across
platforms, which means that the packaging formats and APIs available are
completely different between Chrome, Firefox, and other packaged app
providers. This cross-platform aspect is the main economic reason to
develop for the web. Another drawback is that each of these packaged app
vendors has its own app store, sometimes complete with approval
processes similar to the much reviled App Store approval flow. </p>

<p>Lastly, once a browser vendor has a packaged app model, it's very
tempting for them to just implement new low level features there and not
on the open web. This effectively lifts the pressure for browser vendors
to go through the pain of standardization. The standard response can now
be "just go build a packaged app". A summary of these issues with packaged
apps:</p>

<ol>
<li>No URLs</li>
<li>Not cross platform</li>
<li>Dependent on centralized directories</li>
<li>Vendors have an excuse to punt on adding new features to the web
platform.</li>
</ol>

<p>Packaged apps are at odds with the web. To the unintiated, it feels as
if their inventors slapped web technology on top of the Apple app store
model. I know that there are some legitimate, security-motivated reasons
for their decisions, but believe that these are surmountable.</p>

<h2>Installable web apps</h2>

<p>If you have an iOS device at your disposal, take a look at
<a href="http://forecast.io/">forecast.io</a>. Forecast.io is an example of an <a href="http://blog.forecast.io/its-not-a-web-app-its-an-app-you-install-from-the-web/">app you install
from the web</a>. This approach is interesting because it combines
the best of both worlds. On one hand, you retain the benefits of the
web: URLs, cross-linking, lack of centralized control. On the other, you
get the benefit of elevated permissions.</p>

<p>A benefit of this approach is that there is a clear install step during
which you can request additional permissions, which is a natural place
for breaking out of the web's sandbox in a user-friendly manner. The
result of installation is a homescreen shortcut, which is both a launch
convenience, and a way of managing permissions. Removing that shortcut
can also mean revoking special permissions for that application.</p>

<p>Another benefit is that there is no centralized appstore - you can
discover apps in the same way that you discover the web today - through
search engines, links in your email inbox, feed readers and through any
other URL-based sharing scheme. There is no reason to conflate
installation with the presence of a centralized directory. Google search
is already revealing apps in search results. If you search for "Angry
Birds", you will find both the iOS and Android versions on the first
page.</p>

<p><a name="solution"></a></p>

<h1>Proposal: apps you can install from the web</h1>

<p>So far I've described the <a href="/installable-webapps//installable-webapps/#problem">problem</a>: a major barrier to the
vision of the extensible web: there is no good way of getting outside of
the sandbox. I have been complaining a lot without providing any
constructive answers.</p>

<p>In order to keep things constructive, the second half of the post
proposes a solution to get us out of the sandbox. There's a whole world
out there! Here is my birds-eye-view of the install-from-the-web world:</p>

<p><img src="/installable-webapps/flow.png" alt="flow" /></p>

<p>This diagram is intended to be general enough to work across
operating systems and device types, but the mocks themselves will be
sketched out with a phone form factor in mind. We'll be installing
<code>app.io</code>, a mobile app that lets you leave audio notes.</p>

<p><img src="/installable-webapps/screen1.png" class="border" /></p>

<p><em>Screen 1: App.io example.</em></p>

<h2>An API for installing webapps</h2>

<p>This can be done with an iOS-style approach (and corresponding Chrome
for Android <a href="https://code.google.com/p/chromium/issues/detail?id=153066">feature request</a>), which presents a generic UI
for adding apps to the homescreen (see Screen 2).</p>

<p><img src="/installable-webapps/screen2.png" class="border" /></p>

<p><em>Screen 2: Add via browser button.</em></p>

<p>There are trade-offs between having a button or an opt-in developer
<strong>API for installing web apps</strong>. With a button, any URL can be added to
the home screen, which may not make sense. But with an API, the
developer has to provide an explicit call to action for you to install
their app. The button UX will always be consistent, since it's part of
the browser. An API-based install path may be ugly or spammy. However,
an API can also provide a consistent experience across browsers without
the need for guessing where each browser places the button. Many
forecast.io-style apps on iOS have callouts on the page pointing to the
button in the browser chrome which would be broken if another browser
had a different method of adding to homescreen.</p>

<p>My opinion is that button- and API-based approaches both have a place.
For webapps that make more sense installed, the API can be a nice touch.
Other pages might be useful as webapps without their developer realizing
it, so the button-based approach is useful there.</p>

<p>How would the installation API look like? A JavaScript-based API only
callable on user action, similar to how audio playback in mobile
browsers prevents the annoying situation where visiting a page
automatically prompts you to install it. Installing a webapp should
come with a default set of permissions above and beyond what the web
platform provides.</p>

<pre><code>var button = document.querySelector('button#install');
button.addEventListener('click', window.app.requestInstall);
</code></pre>

<p>You should also be able to request additional permissions at
install-time. For example, to request installation and audio capture,
the following code should work:</p>

<pre><code>window.app.requestInstall({permissions: ['audioCapture']});
</code></pre>

<p>This action should also result in a standard browser-specific dialog to
accept installation, showing which permissions have been requested
(Screen 3).</p>

<p><img src="/installable-webapps/screen3.png" class="border" /></p>

<p><em>Screen 3: Confirm installation.</em></p>

<p>Once accepted, a launcher shortcut should be created (Screen 4). Two
pieces of metadata are necessary for this launcher:</p>

<ol>
<li><p>Icon, which should first look for a large enough version of the
<a href="http://en.wikipedia.org/wiki/Favicon#HTML5_recommendation_for_icons_in_multiple_sizes">multiresolution favicon</a> as determined by the UA. If none
exists, it should look for the <a href="http://goo.gl/6Qdi3">apple-touch-icon</a> in the
<code>&lt;head&gt;</code>. If none is specified, a screenshot of the page can be used
as in iOS.</p></li>
<li><p>Title, which can be extracted from the <code>&lt;title&gt;</code> element in the head.
If none is specified, the user can be prompted to input their own
title.</p></li>
</ol>

<p><img src="/installable-webapps/screen4.png" class="border" /></p>

<p><em>Screen 4: New launcher added to the home screen.</em></p>

<h2>Launching in standalone mode</h2>

<p>iOS already has an <strong>API to know if a webapp was launched in standalone
mode</strong> (ie. from the launcher) or if it was opened from a browser. This
functionality is available via <code>window.navigator.standalone</code>. It also
opens the app in full-screen mode.</p>

<p>Other vendors should standardize and implement similar functionality.
For example, something like <code>window.app.standalone</code>, if only for naming
consistency could be implemented, and a polyfill provided for the Apple
spec. It would also make sense to launch homescreen apps in full screen,
providing the same UX as the full-screen API (Screen 5):</p>

<p><img src="/installable-webapps/screen5.png" class="border" /></p>

<p><em>Screen 5: App launched in standalone mode.</em></p>

<h2>Requesting additional permissions</h2>

<p>Apps might need additional permissions that go beyond the default
baseline of permissions granted to the app at install time. Access to
your camera would fall into this bucket. An <strong>API call to request extra
permissions</strong> might look like the following:</p>

<pre><code>window.app.requestExtraPermissions(['videoCapture']);
</code></pre>

<p>Running this command would also require user-initiation and prompt a
modal optional permissions dialog (Screen 6) similar to the one seen at
installation. After granting it, the associated API call (in this case,
<code>getUserMedia</code>) can be invoked without incurring any infobars.</p>

<p><img src="/installable-webapps/screen6.png" class="border" /></p>

<p><em>Screen 6: Additional permissions request.</em></p>

<h2>Removing installed web apps and extra permissions</h2>

<p>If the installed webapp has a native launcher, removing the launcher can
do this implicitly. There should also be a browser- or system- UI
similar to existing app management interfaces that lets you remove
installed apps, or revoke granted permissions.</p>

<h2>That's it folks</h2>

<p>So there you have it: my strawman fixing the security model of the web,
which, as I outlined at the <a href="/installable-webapps//installable-webapps/#problem">beginning of this post</a>, is
critically important to address for the continued success of the web.
To recap, the solution consists of an API surface in the <code>window.app</code>
namespace, and a number of new screens that are part of the installation
process.</p>

<p>If tackled, this could solve one of the most important issues on the web
today. Otherwise, we may find ourselves in a place where the web
platform is irrelevant to application developers, who will just build
for packaged platforms.</p>

<p>I'm not silly enough to think that this proposal is the ultimately
correct and best solution for elevated priveledges on the open web.
There is a huge amount of work required to refine the flow, think of all
of the edge cases, implement it across browsers, etc. The above is just
a draft to re-ignite the web permissions discussion that died several
years ago. Please blog something in response or in the worst case, tweet
or email your opinion. Looking forward to hearing from you.</p>

<h1>Update: important links</h1>

<p><strong>July 18, 2013</strong>: Several people have pointed out that I've missed some
important links.  My public apologies!</p>

<p><a href="https://developers.google.com/chrome/apps/docs/developers_guide">Chrome hosted apps</a> are a somewhat similar concept,
but suffered from security issues that still need to be resolved to make
this proposal a reality. There was even an effort to make hosted web
apps installable from the web, called <a href="http://blog.persistent.info/2011/07/theres-web-app-for-that-site.html">CRX-less web
apps</a> (preserved on Mihai's blog), which today is little
more than a <a href="http://code.google.com/intl/en-US/chrome/apps/docs/no_crx.html">broken link</a>.</p>

<p>To my knowledge, the most active project along the lines of this
proposal is the <a href="https://developer.mozilla.org/en-US/docs/Web/Apps">Open Web Apps</a> work from Firefox OS. </p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Gestural music direction</title>
    <author><name>Boris Smus</name></author>
    <link href="/gestural-music-direction"/>
    
    <updated>2013-05-03T09:00:00-00:00</updated>
    
    <id>http://smus.com/gestural-music-direction</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Imagine this: you start conducting as if you were in front of a great
orchestra, and music fades in out of thin air, matching your tempo and time
signature. Your nuanced gestures can indicate changes in intensity, and of
course affect the speed of the piece. You'd first need to learn some
basic conducting patterns, like these:</p>

<p><img src="/gestural-music-direction/conduct-time-signatures.png" alt="Conducting patterns for various time signatures" /></p>

<!--more-->

<p>You would also have to wait for me to finish this project, which uses a LEAP
motion device (or your trackpad), the Web Audio API, and some signal processing
to achieve a scaled back version of the idea described above.</p>

<h2>Prototype</h2>

<p>The current prototype lets you control the tempo of a song called "Phantom"
from the excellent <a href="http://www.parovstelar.com/">Parov Stelar</a>. You can do this by making simple
conducting patterns, similar to the 2/4 pattern above. In practice, you can use
any pattern in which your hand oscillates between two points in space to play
with this prototype. You can even use your mouse instead of a LEAP motion
device. Just click in to enable pointer lock. This will ensure that your mouse
will always be focused inside your browser.</p>

<p>I built a visualizer which is an 8-bit inspired frequency graph which
also shows directional changes as pulsating red dots, and clusters which
flash to the beat.</p>

<p><a href="http://borismus.github.io/gestural-music-direction/"><img src="/gestural-music-direction/screenshot.png" alt="Screenshot of leap conductor" /></a></p>

<p>If you'd like to try it live, the <a href="http://borismus.github.io/gestural-music-direction/">demo lives here</a>.</p>

<h2>Handling input</h2>

<p>With a LEAP device plugged in, the prototype maps the palm's 3D center
to 2D. It works just as well with just a trackpad or mouse attached to
your computer, which directly outputs 2D coordinates. The input handling
algorithm then uses the resulting (x, y) pairs to do roughly the
following:</p>

<ul>
<li><p>First, track positions and first (velocity) and second (acceleration)
order history, including times. Store in a ring buffer, which is
implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/ring-buffer.js">ring-buffer.js</a>.</p></li>
<li><p>Extract sudden changes of direction based on heuristics related to
velocity and acceleration history.</p></li>
<li><p>Cluster directional changes using K-means or similar clustering
algorithm which is implemented in <a href="https://github.com/borismus/gestural-music-direction/blob/master/js/clusterizer.js">clusterizer.js</a>. I run this
K-means implementation with 3 values of k in [2, 3, 4] and pick the
one with the lowest error. I've also build a standalone
<a href="http://borismus.github.io/gestural-music-direction/cluster.html">clustering test page</a> with the following output:</p></li>
</ul>

<p><img src="/gestural-music-direction/cluster.png" alt="Clustering algorithm visualization" /></p>

<ul>
<li><p>To calculate tempo, pick a cluster and calculate mode of the deltas
between adjacent points.</p></li>
<li><p>The time signature is just the number of clusters over 4 (for the
simple 2/4, 3/4 and 4/4 patterns).</p></li>
</ul>

<h2>Changing tempo in real-time</h2>

<p>Once we have an idea of what pattern the user is creating with their hands, we
need to match up the song to the pattern, and continuously adapt the song's
playback rate to the user's motions.</p>

<p>The Web Audio API makes it dead simple to change the playback rate of an audio
buffer for a source node'ss entire duration. However, things get a bit
trickier if this rate changes continuously over time. Chris Wilson
describes a scheduling technique which addresses this exact problem in
his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/">"Tale of Two Clocks" HTML5Rocks article</a>. You can also see
a simple version of it inaction in his <a href="http://www.html5rocks.com/en/tutorials/audio/scheduling/goodmetronome.html">metronome demo</a>.</p>

<p>I used this idea to do a bit of granular synthesis on an audio buffer. I
schedule a bit of the buffer into the future, at the current tempo. As the
tempo changes, new bits of the buffer are scheduled at a different
playbackRate. I keep track of how far into the buffer we've gone and use that
as the grainOffset. Here's some code that illustrates this (but see the
<a href="https://github.com/borismus/gestural-music-direction/blob/master/js/music-player.js">variable rate music player</a> for the full code):</p>

<pre><code>MusicPlayer.prototype.loop_ = function() {
  // Schedule the next bar if it's not yet scheduled.
  while (this.nextNoteTime &lt; audioContext.currentTime + this.scheduleAheadTime) {
    this.scheduleSegment_(this.grainOffset, this.nextNoteTime);
    this.nextNote_();
  }
}

MusicPlayer.prototype.scheduleSegment_ = function(grainOffset, time) {
  // Get the part of the buffer that we're going to play.
  var source = audioContext.createBufferSource();
  source.buffer = this.buffer;
  source.connect(audioContext.destination);

  var rate = this.getPlaybackRate_();
  source.playbackRate.value = rate;

  var secondsPerBeat = 60.0 / this.tempo;
  source.noteGrainOn(time, grainOffset, secondsPerBeat * rate);
}

MusicPlayer.prototype.nextNote_ = function() {
  // Advance current note and time by a 16th note...
  var secondsPerBeat = 60.0 / this.tempo;
  // Notice this picks up the CURRENT tempo value to calculate beat length.
  this.nextNoteTime += secondsPerBeat;
  // Get the next grain.
  var rate = this.getPlaybackRate_();
  this.grainOffset += secondsPerBeat * rate;
}
</code></pre>

<p>In practice, I'm unfortunately hitting some rounding errors, so the
grains aren't stitched together as seamlessly as I wanted. You can
sometimes hear artifacts if you slow the tempo way down.</p>

<h2>A work in progress</h2>

<p>My initial idea was to use <a href="http://developer.echonest.com/">The Echo Nest</a> to pick the right song
(based on time signature and tempo), and then stream that song from some
streaming music service. Unfortunately it's quite hard to get at PCM versions
of tracks from Rdio and Spotify. That said, it can be <a href="https://github.com/oampo/AmbientCloud">done with
Soundcloud</a>. Long story short, the prototype currently only
supports one song.</p>

<p>A time signature recognizer is mainly useful for classical music, since so much
of popular music is in common time (with <a href="http://twentytwowords.com/2011/05/18/6-pop-songs-in-unusual-time-signatures/">rare exceptions of popular music with
complex time signatures</a>). But applying simple transformations
like changing the tempo just feels wrong for complex music without a very
obvious rhythmic structure.</p>

<p>Lastly, LEAP's palm tracking is still quite noisy (even after drastic
improvements to palm tracking as of <a href="https://developer.leapmotion.com/blog/sdk-0-7-7-released-new-palm-tracking-and-gesture-settings">SDK 0.7.7</a>). Also, the
bay windows in my living room lets in tons of infrared light which often puts
the device into a low fidelity tracking mode.</p>

<p>As always, let me know what you think, and of course, feel free to fork
and evolve on <a href="https://github.com/borismus/gestural-music-direction">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web Audio book</title>
    <author><name>Boris Smus</name></author>
    <link href="/webaudio-book"/>
    
    <updated>2013-03-18T09:00:00-00:00</updated>
    
    <id>http://smus.com/webaudio-book</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I wrote a short book about the Web Audio API. The book is meant as an
introduction to the web audio API, as well as some audio basics for web
developers with little audio experience. It is <a href="http://chimera.labs.oreilly.com/books/1234000001552/">available for free on
Chimera</a>, a web-based book viewer, which presents a nicely laid
out page and lets you leave per-paragraph comments. The online version
also includes inline samples from <a href="http://webaudioapi.com">webaudioapi.com</a>. If you don't
like reading on the web, you can also <a href="http://shop.oreilly.com/product/0636920025948.do">buy a physical copy or an
ebook</a> from O'Reilly.</p>

<!--more-->

<p>So I got this cool bat on the cover. I am told that technically
speaking, it is a brown long-eared bat (<a href="http://en.wikipedia.org/wiki/Brown_long-eared_bat">Plecotus auritus</a>). Though
it's no orca (an ideal O'Reilly cover, don't you think?), I'm very happy
that it's a <a href="http://en.wikipedia.org/wiki/Animal_echolocation">sound related animal</a>.</p>

<p><img src="/webaudio-book/cover.jpg" alt="Web Audio Book Cover" /></p>

<p>The book was written on my laptop in a Google Doc. I hand-drew some
illustrations in a notebook and brought them into the doc. Once I was
ready for feedback, I sent the doc around to my technical reviewers, who
left feedback in comments. After incorporating their feedback, I got
some editorial feedback, still in the doc. Once the draft was more or
less ready to go, I created an <a href="https://www.odesk.com/">oDesk</a> task to convert the Google
Doc into docbook.xml format. The contractor did a great job and charged
me about $100. This was my first time paying anyone to do work for me.</p>

<p>Thanks to all of the reviewers, editors, illustrators and organizers for
helping. Also, thanks to <a href="http://www.kevincennis.com/">Kevin Ennis</a> who kindly donated the
<a href="http://webaudioapi.com">webaudioapi.com</a> domain which I'm currently using to host the
samples.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Really simple social syndication</title>
    <author><name>Boris Smus</name></author>
    <link href="/really-simple-social-syndication"/>
    
    <updated>2013-03-14T09:00:00-00:00</updated>
    
    <id>http://smus.com/really-simple-social-syndication</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I've been thinking about this for a while, but the recent <a href="http://googlereader.blogspot.ca/2013/03/powering-down-google-reader.html">sunset
announcement of Google Reader</a> made me revisit this topic.
Google Reader isn't the only thing that's dead. RSS (aka. Really Simple
Syndication) has long been proclaimed dead as well. In fact most people
never even knew what RSS was. That said, it was a very useful tool for
me and many others that like to stay up-to-date in their areas of
interest. Increasingly, I've been getting my dose of news through social
networks. However, social networks contain a lot of noise that I care
little for. I want to rebuild the RSS spirit using modern social
networks. This post describes one possible approach, which I refer to as
<strong>Really Simple Social Syndication (RSSS)</strong>.</p>

<!--more-->

<h2>Really simple syndication: the good old days</h2>

<p>Here's what the content flow used to be with blogs and RSS:</p>

<p><img src="/really-simple-social-syndication/rss-flow.png" alt="RSS-based content syndication" /></p>

<p>It was simple. Really simple, actually!</p>

<h2>Social syndication: today</h2>

<p>I don't care much for social networks. I mostly see them as a two-way
utility for ultimately connecting content creators to content consumers.</p>

<p>One way, social networks like Facebook, Twitter, G+, etc are just
vehicles for finding out what to read based on your interests. I spend
too much time checking them individually for my news, and this is
unfortunate. </p>

<p>The other way, social networks make it easier to have your content read
by a bunch of the right people. In my case, the vast majority of
non-organic search traffic comes from social sources (mostly twitter). I
waste a bit of time tweeting, and G+ing new posts on my blog as they
come out, but sometimes they are picked up by others and I don't
actually need to do that.</p>

<p>As long as the content itself stays outside of the walled gardens of the
social networks, I think we can come to a syndication solution that
might rival the old RSS-based one, but enjoy the benefits of having some
extra signals from all of the social network junk. Here's the model I'm
thinking of:</p>

<p><img src="/really-simple-social-syndication/social-flow.png" alt="Social-based content syndication" /></p>

<p>Now for a little bit more about Pub and Sub.</p>

<h2>Sub: Requirements for consumption</h2>

<ul>
<li><p><strong>Reuse existing feeds</strong>. I don't want to re-create my sources. Use
existing people you follow on Twitter, from G+ circles, Facebook
friends.</p></li>
<li><p><strong>De-duplicated content</strong>. Some people post the same link on multiple
networks. Some popular posts are reshared by everyone. I only want to
see a post once.</p></li>
<li><p><strong>Content centric</strong>. Show me the content in some standard, readable
way. Hide the social stuff unless I explicitly ask. Most of the time I
don't care where it came from, don't care how many people liked it, or
what they wrote in the comments. Sometimes I'm curious and want a way
to trace it back.</p></li>
<li><p><strong>Social signals as a metric</strong>. If many of my sources share something,
I probably should at least take a skim.</p></li>
<li><p><strong>Web based service</strong> so that I can use it anywhere.</p></li>
<li><p>Set a <strong>volume-based daily quota</strong>. If I'm busy today I'd like to only
see the top N articles, sorted by some transparent metric of my
choosing.</p></li>
</ul>

<p>Flipboard, Pulse, Feedly all sort of fit into this class of readers.
Ideally this would be an API that just lets me connect a few social
accounts, and get back a filtered feed of content. This could be the API
for the product - a Really Simple Social Syndication (RSSS) feed.</p>

<p>Anyone could then build a UI on top of it for their favorite platform
(Google Reader-like).</p>

<h2>Pub: Requirements for content production</h2>

<ul>
<li><p>Automatically post content to a bunch of social services. </p></li>
<li><p>Intelligent shortening of links and content (eg. for twitter, to fit
in 140 chars).</p></li>
</ul>

<p>Wordpress plugins, Tumblr and others provide ways to automatically tweet
and otherwise post new updates to your content. There needs to be some
other way that works in general for any type of content. Such a service
could be similar to feedburner, in that it would take an existing RSS
feed and socialize it.</p>

<h2>Social networks, let's be friends!</h2>

<p>I'm not religious about Google Reader or RSS. It was a good solution for
content syndication at the time, but I'm ready to accept that perhaps
it's time to move on. Hopefully with tools like the above, we can have
something that comes close to the utility of RSS feeds.</p>

<p>Social networks could do some evil stuff which would preclude RSSS from
happening. Economically, they are incentivized own content, create
walled gardens, insert advertisements, and prevent access to their
feeds. I'm hoping that some human element will prevent that from
happening.</p>

<p>Here is a short list of what we need from the social networks:</p>

<ul>
<li><p>The content itself must be free from walled gardens (eg. paywalls,
login walls, etc)</p></li>
<li><p>Social network feeds are available to read in full, as is, without
magical suggestions, collaborative filtering, etc.</p></li>
<li><p>Social networks provide some programmatic way to post content.</p></li>
</ul>

<p>Once we have a good flow for the production and consumption of content,
using social networks as a delivery mechanism, I will be very happy to
minimize the amount of time spent on social networks directly, and focus
on consuming and producing interesting things. Also, happy &pi; day!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Interactive touch laptop experiments</title>
    <author><name>Boris Smus</name></author>
    <link href="/touch-laptop-experiments"/>
    
    <updated>2013-02-21T09:00:00-00:00</updated>
    
    <id>http://smus.com/touch-laptop-experiments</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Largely because of the plummeting price and thickness of touch screens,
these devices are increasingly ubiquitous. One of the latest trends is
touch screen laptops, spearheaded by <a href="http://www.microsoft.com/Surface/en-US">Surface</a> devices and the
recently announced <a href="http://chrome.blogspot.com/2013/02/the-chromebook-pixel-for-whats-next.html">Chromebook Pixel</a>. In this post I'll dive into
some experiements around this new form factor. The main goal is to try
to convince myself that this form factor makes sense for reasons other
than economic ones.</p>

<p>In exploring the interaction design angle of these new devices, I came
across a couple of what I think are a couple of interesting ideas that
I'd like to share with you: <strong>responsive input</strong> and <strong>simultaneous
interactions</strong> using both mouse/trackpad and touchscreen. I wrote some
demos that illustrate these ideas. <em>A touchscreen laptop is required for
these demos to work properly</em>.</p>

<ul>
<li><a href="http://borismus.github.com/touch-laptop-experiments/responsive">Auto scaling in response to input type</a>.</li>
<li><a href="http://borismus.github.com/touch-laptop-experiments/map">Mouse-to-map and touch-to-mark</a>.</li>
<li><a href="http://borismus.github.com/touch-laptop-experiments/transform">Multimodal transform demo</a>.</li>
</ul>

<!--more-->

<p>Because you probably don't have a touch screen laptop, I recorded a
rough video showing some of these interactions:</p>

<iframe width="640" height="360" src="http://www.youtube.com/embed/rcE2z9tudGw" frameborder="0" allowfullscreen></iframe>

<p>Hopefully this gives you a better sense of what I mean by responsive
input and simultaneous touch and mouse interactions.</p>

<h2>Responsive input</h2>

<p>The touch laptop class of device has a two main interaction modes:</p>

<ol>
<li>As a regular laptop with trackpad (or external mouse) and keyboard.</li>
<li>As a touch tablet with a keyboard.</li>
</ol>

<p>These two interaction modes differ fundamentally in many ways. The
following are some examples of these differences:</p>

<ul>
<li>Touch has no hover state.</li>
<li>Touch is less precise than mouse and requires bigger targets.</li>
<li>Touch requires that you are closer to the screen.</li>
</ul>

<p><img src="/touch-laptop-experiments/touch-laptop.png" alt="Chrome Pixel" /></p>

<p>Ideally, you want to provide optimal experiences for both cases. For the
mouse case, this means taking advantage of hover states and a finer
pointer. For the touch case, this means ensuring that touch targets are
big enough to be tapped, not relying on hover at all.</p>

<p>So I explored a user interface concept that adapts touch laptop
interfaces to the user's current input mode. The tricky bit is detecting
the user's current input mode. Several adaptation options are possible:</p>

<ol>
<li>Immediately transform to the mouse-style UI as soon as the input mode
changes (simplest, but can cause transitions to fire too rapidly
between the two modes, which may be jarring).</li>
<li>Transform only after some period of not using the other input mode
(eg. go to touch mode only if the user is actively using touch, and
not touching the mouse at all).</li>
<li>Transform based on some external criteria, like whether or not the
screen is docked to a mouse, or based on input from sensors other
than mouse/touchscreen.</li>
</ol>

<p>The first approach is problematic in that your first touch transforms
the page. If this transformation causes your target to move away from
it's initial position, you will miss it entirely. This can be mitigated
by having intelligent resizing which does not affect anything directly
under the touch point, but may result in a lopsidedly zoomed interface.</p>

<p>The second approach is problematic since the mode switching will happen
automatically after some period of inactivity, which may be jarring. The
last approach is either obvious (eg. mouse removed), or an area of
research (eg. predicting when the user will touch based on camera).</p>

<p>I wrote a <a href="http://borismus.github.com/touch-laptop-experiments/responsive">demo of auto scaling in response to input type</a>.
If you use the mouse, click targets will decrease in size. If you use
your finger, touch targets increase in size. (Of course, this will only
work on a touchscreen laptop).</p>

<h2>Simultaneous touchscreen + mouse/trackpad interactions</h2>

<p>In the above section, I described an automatic way to switch between
touch and mouse mode However, there is a middle ground between the two:
multimodal interactions that involve both touchscreen and
mouse/trackpad. Simultaneous bimodal interaction is already common. For
example, using mouse and keyboard simultaneously makes a very efficient
interface for FPS games, with the movement via the WASD keys, and
mouse-look.</p>

<p>One experiment involves using the mouse or trackpad as a navigation
device and using the touch screen as a way to input positional data.
This is illustrated through Google maps. You pan and zoom the map using
mouse events, and place markers on the map using the touch screen. Try
out this demonstration of <a href="http://borismus.github.com/touch-laptop-experiments/map">mouse-to-map and touch-to-mark</a> (again,
this requires a touchscreen laptop).</p>

<p>Another experiment involves manipulating geometric objects on the
screen. The idea here was to use the touch screen to select objects, and
use the trackpad/mouse as way of manipulating the selected object. In
this demo, you can manipulate the object in a number of ways:</p>

<ol>
<li>Move it by simply dragging it around on the screen with touch.</li>
<li>Rotate by selecting the object on the touchscreen, and then
performing a mousemove (either by moving a mouse or dragging one
finger on a trackpad). The rotation happens around the point where
you touched the object, which acts as a fulcrum. </li>
<li>Scale it in the same fashion as rotation (selecting object and
transformation origin with the touchscreen), except with a two-finger
drag on the trackpad, or using the mousewheel if a mouse is attached.</li>
</ol>

<p>With no selection, the canvas itself can be zoomed and panned with the
mouse/trackpad directly. Try out this <a href="http://borismus.github.com/touch-laptop-experiments/transform">multimodal transform
demo</a> (requires touchscreen laptop).</p>

<h2>Missing pieces</h2>

<p>Like any brave new world, the one of multimodal input has its own set of
challenges.</p>

<p>It's currently impossible to distinguish a touch laptop from any other
touch screen. Notably, this means that you should never assume that
touch support implies no mouse support. In practice, make sure that you
always bind to mouse events. If you also have touch event handlers, just
use <code>event.preventDefault()</code> there to ensure that you aren't handling
one event in multiple handlers. If you're interested in this, follow the
discussion at <a href="http://crbug.com/174553">http://crbug.com/174553</a>.</p>

<p>As a generalization of the above, there is currently no way to determine
which kinds of input are available in the browser. A fully fledged Input
Availability API might seem like overkill, but there are already some
cases beyond touch laptops that are relevant. For example, detecting the
presence of a physical keyboard would be very useful. Further, detecting
hardware features like an attached camera and microphone could fall into
the same bucket rather than relying on exception handling from APIs like
<code>getUserMedia</code>. Lastly, having such an API would allow websites to react
dynamically to changes in input (eg. a tablet gets docked to a physical
keyboard, or a mouse is attached).</p>

<p>The final missing piece is that dealing with two different event models
(mouse and touch) is definitely clunky. I have already <a href="http://smus.com/mouse-touch-pointer/">written
extensively about pointer events</a> and a <a href="https://github.com/borismus/pointer.js">pointer event
polyfill</a>. In this particular case, pointer events would be
great, because although they provide a consolidated model for input,
it's very easy and natural to distinguish between the two modalities.</p>

<p>These experiments are all available <a href="https://github.com/borismus/touch-laptop-experiments">on github</a>. </p>

<h2>Your turn!</h2>

<p>Do you have thoughts or demos around new types of interactions using
touch laptops? Please share them below.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>From VPS to static hosting</title>
    <author><name>Boris Smus</name></author>
    <link href="/moved-to-s3"/>
    
    <updated>2013-01-16T09:00:00-00:00</updated>
    
    <id>http://smus.com/moved-to-s3</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>All good things must come to an end. VPS hosting paid for by my former
university is no exception! Ever since the University of
Madeira-provided credit card paying for the account expired, I began
wondering whether it's worth paying for a VPS that I hardly use.
Combined with two consecutive 10-minute stretches of downtime last week,
I had my answer.</p>

<p>I run this blog, my mother's site and a handful of mini-sites, all of
which are inherentily static content. Today, I moved them all away from
my VPS completely. I migrated the relatively complex sites to the
<a href="https://github.com/borismus/lightning">lightning</a> engine, and updated the engine with a couple of nice
features: fixed content links in list pages and feeds, and support for
publishing to S3.</p>

<!--more-->

<h2>System administration</h2>

<p>In my early Linux days, I ran an AMD Athlon server off my parents'
internet connection. I took pride in configuration, maintenance,
administration, endlessly recompiling updates and dealing with broken
dependencies. I enjoyed the challenge and got very good at it. By
sinking enough time into any problem, I was confident that I would
ultimately solve it. Sometimes I contributed an ebuild or two to
portage. I learned a lot, and eventually my web server outgrew my
parents' internet connection.</p>

<p>So I turned to managed hosting. Several years later, sick of the crappy
management UI, and yearning to flex some sysadmin muscle, I jumped on
VPS opportunity for performance reasons. While clearly overkill for
static sites, it was appealing from a "what if?" perspective: what if
suddenly I wanted to run a complex webapp? No problem, VPS was ready!</p>

<h2>Except system administration sucks</h2>

<p>My VPS slice was running Ubuntu 8. Since Ubuntu 12 was released, I was
greeted with a "48 packages are out of date" message upon logging into
the machine. </p>

<p>Long ago, this message would have sent me down a rabbit hole of emerging
all of the outdated packages, resolving dependencies and rewriting
config files. It was gratifying to be on the bleeding edge, to have a
clean system with all of the daemons dancing to your tune in perfect
harmony.</p>

<p>These days, I could care less about being up-to-date. In fact, I
actively dislike upgrading. An upgrade is a risk, likely to lead to
something breaking, likely without me noticing at first. So rather than
the "ooh, new shiny" feeling I used to have when Apache needed an
update, I actively dread needing to update anything. I don't want to
need to tweak configurations, especially because I've forgotten a lot of
the domain-specific config languages. </p>

<h2>S3 for static hosting, PaaS for everything else</h2>

<p>Happily, all of my sites are currently static. Blogs and mini-sites all
lend themselves very well to being hosted on S3.</p>

<p>For the hypothetical case that I require a dynamic web server on
the internet, I'll turn to a Platform-as-a-service solution like
<a href="http://nodejitsu.com/">Nodejitsu</a> or <a href="https://developers.google.com/appengine/">AppEngine</a> to avoid doing rote configuration
tasks.</p>

<p>Being a sysadmin is not a part time job.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Internet mashup of things</title>
    <author><name>Boris Smus</name></author>
    <link href="/mashup-of-things"/>
    
    <updated>2013-01-11T09:00:00-00:00</updated>
    
    <id>http://smus.com/mashup-of-things</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Many new devices come with unexpected connectivity - often a WiFi
connection that enables them to connect to a hotspot and the larger
internet. <a href="http://www.nest.com/">Nest</a>, a smart thermostat, was one of the first
commercial products to do this. Many more indie projects are following
suit, with an explosion of kickstarters like this <a href="http://www.kickstarter.com/projects/limemouse/lifx-the-light-bulb-reinvented">teleoperated
light</a>, <a href="http://www.withings.com/">connected scale</a> or this <a href="http://supermechanical.com/twine/">general purpose
connected sensor</a>. The idea of an Internet of Things, in which
every appliance and object is somehow connected, has long been popular
in academic circles, and this time around it feels like we're actually
close.</p>

<p>If we think of these physical devices/appliances as web services with
APIs, we can mash them up just like we did in the early days of the web,
creating applications that are more useful than the sum of their parts.
In this post I argue for using the web as the medium to tie everything
together, describe a simple architecture for building networked physical
devices and build a web lamp controlled by an arduino.</p>

<!--more-->

<h2>Web meets ubiquitous computing</h2>

<p>The projects I mentioned above don't use proprietary home automation
protocols like X10.  Instead, they establish connections via the
internet. Nest registers itself with a central server (operated by Nest
inc) over WiFi. You can then control your thermostat by logging into
their website at nest.com.</p>

<p>There's no need for heavyweight ISO666 standardization efforts. This is
a very good thing, because it's really hard for anyone (including our
industry) to agree on something new. Instead, we reuse a proven protocol
that everyone's already agreed on: HTTP.</p>

<h2>An Internet of too many things</h2>

<p>Nest also provides Android and iOS apps to control your thermostat. But
what happens when you have your thermostat, light bulbs, and home
theater, scanner and <a href="http://www.google.com/cloudprint/learn/">printer</a> all connected up to the internet in
this fashion? Too many apps! Imagine you have all of these different
devices, perhaps multiple of each in some cases:</p>

<p><img src="/mashup-of-things/many-smart-things.png" alt="Many smart things" /></p>

<p>It pains me to imagine an app for each of these different services -
there are simply too many things to juggle. I would not want to install
apps for each of these devices for reasons similar to why I would never
install the United app: there is significant overhead to managing apps
on your smart phone. They take up space on your home screens, you need
to keep them up to date. <a href="http://jenson.org/">Scott Jenson</a> writes eloquently about
this problem on his blog.</p>

<p>If each hardware manufacturer releases their device with a central
registry and public API, however, third parties might be able to build
on top of the services that these devices provide, and certainly some
interesting and pleasant user experiences will arise in this nascent
area.</p>

<h2>A minimum viable physical web architecture</h2>

<p>There are many discovery protocols (eg. UPnP, zeroconf, ...), and many
messaging protocols (eg. TCP, HTTP, ...), and I won't argue for one over
another. The technical tradeoffs of each of these is outside of the
scope for this discussion. My approach was to devise the simplest
proof-of-concept that would work anywhere without fancy protocols, only
using widely available features available on the web, to enable a
platform for prototyping, the goal being to establish useful and
interesting interactions first, and implement the underlying technology
later.</p>

<p>In general, we want to be able to interact with devices that are nearby
and also with devices that are far away. With this constraint, even the
simplest approach requires a server component. Here's a sketch:</p>

<p><img src="/mashup-of-things/lamp-arch.png" alt="A simple WiFi lamp" /></p>

<p>In this setup, the controller listens to the server for what it should
do. At the same time, the remote sends commands to the server based on
user interaction. The server keeps track of the state of all of the
devices it manages.</p>

<h2>Example: cloud lamp</h2>

<p>The above approach is to simply have a RESTful server. The remote can
send commands to via POST requests, and the controller can poll for its
new state periodically with GET requests. A simple API for a lamp might
look like this:</p>

<ul>
<li><code>GET /</code>: Returns 1 if the lamp is on. Otherwise returns 0.</li>
<li><code>POST /on</code>: Turns the lamp on.</li>
<li><code>POST /off</code>: Turns the lamp off.</li>
</ul>

<p>This API and the three components (server, client and device) above are
enough to get a prototype off the ground. A simple implementation of all
three pieces can be found in the <a href="http://github.com/borismus/hello-lamp">Hello Lamp on github</a>.</p>

<p>The server-side is written on AppEngine, and simply tracks state. The
handlers all use <a href="http://en.wikipedia.org/wiki/Cross-origin_resource_sharing">CORS</a>, which lifts the cross-domain security policy
of the web, making it possible to POST to the lamp from any web page. </p>

<p>The client is a really simple web page with two ways of controlling the
lamp: the toggle switch and <a href="https://dvcs.w3.org/hg/speech-api/raw-file/tip/speechapi.html">speech recognition</a> (available
behind a flag in Chrome), which enables simple "lamp, turn off" and "on"
commands.</p>

<p><img src="/mashup-of-things/client.png" alt="Simple client" /></p>

<p>Finally, the device itself is an Arduino Uno with a <a href="http://arduino.cc/blog/2012/08/16/the-arduino-wifi-shield-is-now-available/">WiFi
shield</a>. One of the pins is connected to a relay which controls
a power socket with the plugged in lamp. The program itself is nearly
identical to the existing sample apps available for the WiFi shield.</p>

<p><img src="/mashup-of-things/arduino.png" alt="Arduino WiFi shield" /></p>

<h2>Practical considerations</h2>

<p>The solution described here is definitely not one to use in production
for many reasons, including robustness, performance and security.</p>

<p>In practice, we'd probably want a server to control multiple devices, so
each request should specify an ID representing the device. Also,
continuous polling is a very inefficient approach, since every HTTP
request has a lot of overhead. It's better to maintain open connection
server-controller and server-remote connections to enable pushing of
data directly. This connection should be bidirectional to let the device
feed back to the remote as well as enabling control.</p>

<p>A promising startup working in the direction of making vision more real
is <a href="http://electricimp.com">electricimp</a>. These guys provide
connectivity through a SD card form factor they call an "imp". Though
they have a proprietary communication protocol between their imps and
their cloud (mainly for performance), they ultimately provide a RESTful
web API to control each imp. Definitely looking forward to playing with
their offerings.</p>

<h2>Mash it up</h2>

<p>Now we have a device registering with a public web server. The server
provides an API for developers to access that device. Developers can
then build applications around that API which interact with the device.
This is no different from a regular web API, and at this point, all we
have built is a glorified remote control that uses the web instead of IR
for transmitting messages.</p>

<p>However! One of the great things about web APIs is that they can easily
be mixed with one another to create something greater than the sum of
the parts. This is known on the internet as a "mashup". The first
mashups emerged about decades years ago, but are still an important part
of the web landscape. I recently used a really nice one called
<a href="http://livelovely.com/">lovely</a>, which is a house hunting mashup combining Google Maps and
craigslist. Mashup making services like <a href="http://pipes.yahoo.com/pipes/">Yahoo! Pipes</a> and
<a href="https://ifttt.com/">IFTTT</a> have existed for a while. Notably, IFTTT includes support
for some of the "smart" physical devices I mentioned in the beginning of
this post.</p>

<p>To me, the truly interesting question around this physical web is the
user's experience. Each of the various promises of
device/appliance/thing control or automation is not terribly compelling.
Why would I unlock my phone, find and run the lamp app, and find the
right fixture? The alternative of walking up to the light switch and
flipping it seems much more lucrative. That said, I'm optimistic that
there are compelling user interfaces to be discovered, but they will
only emerge on top of this physical web framework.</p>

<h2>A plea for physical APIs</h2>

<p>As we move into a world connected devices, we need to have a solid,
widely available platform for communication between these devices. This
infrastructure can and should ultimately be the web.</p>

<p>This opinion of mine is not zealotry, but completely practical: I do not
want every device in my home to be stuck in an networked ecosystem
provided by some single company. Whether it's an Android app, an iOS
app, a web app, a universal dashboard on a smart phone, a voice
controlled system in the living room, or (more likely) something
completely different, any of these systems should be able to easily
interface with any device in the world.</p>

<p>The recent stink around <a href="http://www.technologyreview.com/view/508666/twitter-instagram-and-the-internet-of-disconnected-things/">twitter no longer embedding instagram
photos</a> is just one example of important chunks of the
internet become segregated from one another. This wall-building trend is
alarming and dangerous to the integrity of the web platform, which is
based on interoperability: cross-linking, cross-embedding, and API
mashups. While this new web of things is still in its infancy, it's
especially important that we nurture the same principles that made the
early web so great.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Reusable JavaScript for MVC frameworks</title>
    <author><name>Boris Smus</name></author>
    <link href="/reusable-js-mvc-frameworks"/>
    
    <updated>2012-11-06T09:00:00-00:00</updated>
    
    <id>http://smus.com/reusable-js-mvc-frameworks</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>These days, I write a lot more code, and my projects have increased in
complexity. Often, a single application brings many kinds of data
sources and bleeding edge web features together. On the other hand, most
of what I build are prototypes, which need to be churned out quickly,
work reliably in demos, and look/feel good.</p>

<p>MVC frameworks help write UI code much more quickly, but there are
drawbacks too: there are too many to choose from, they don't
interoperate with one another, and if you want to release parts of your
code to the open source community, only those developers that use the
same framework will benefit. The solution is to create a clear
separation between core application logic and MVC UI code. This way you
can reuse a lot of code and reduce switching costs.</p>

<!--more-->

<p>Before jumping in, here's a diagram of the approach:</p>

<p><img src="/reusable-js-mvc-frameworks/mvc-js-layers.svg" alt="diagram" /></p>

<p>The rest of the post is about interop problems with JS MVC frameworks,
and a closer look reasons for taking the above approach.</p>

<h2>MVC frameworks are great</h2>

<p>One of my key requirements is to be able to build user interfaces
<strong>quickly</strong>. This means that writing in pure JavaScript is out of the
question, because it simply doesn't provide a high enough layer of
abstraction for highly interactive user interfaces, and reduces to
spaghetti code in just a couple of days. Instead of raw JS, use an model
view controller (MVC) framework.</p>

<p>My current weapon of choice is <a href="http://emberjs.com/">Ember.js</a>, mostly because it
makes many decisions that I agree with, providing easy two-way binding,
a sane templating syntax, observers and computed properties. All-in-all,
it is pretty effective for whipping up consistent UIs quickly.</p>

<p>But there are problems with buying into an MVC framework.</p>

<h2>JS libraries are like insects</h2>

<p>JavaScript frameworks are like insects. There are thousands of them,
they move very quickly, and generally have very short life spans.</p>

<p>If you've ever taken a hiatus from client-side web development, you were
probably overwhelmed with the amount of new stuff available when you
returned. The flipside is that many of the frameworks you were familiar
with could have easily disappeared. To be fair, some winners have
emerged in the past, most notably in the utility frameworks, where
jQuery has risen to the top, and Prototype has fallen to obscurity. In
MVC frameworks, however, there are many interesting contenders and, as I
wrote earlier, still <a href="http://smus.com/backbone-and-ember/">no clear winner</a>.</p>

<p>So at this stage, going all-in in on a framework may be a bad idea,
because what if the community moves on to something else? What if the
framework developers get bored, stop caring or cease maintenance for
some other reasons? I have this feeling all the time, despite framework
authors promises to the contrary. And this is the case even though I
build mostly prototypes with relatively short life spans.</p>

<h2>Switching cost between MVC frameworks is high</h2>

<p>Once you bite the bullet and decide to invest in a framework, you often
have no easy way to move your code out of it. If you pick Backbone, but
decide mid-cycle that it's not for you, you are in for a world of hurt:</p>

<p>Not only do your Models and Views not share the same base classes, they
don't even use the same <strong>class system</strong>. Backbone and Ember provide
their own class systems that are not compatible. This is a ridiculous
problem to have, and one unique to JavaScript, which provides a
prototypal inheritance system which is so inconvenient, there are about
<a href="http://goo.gl/hqAHD">a million libraries</a> that add OO-style classes to the
language.</p>

<ol>
<li>Those who use or invent a custom class system in JavaScript that
looks more like traditional OO.</li>
<li>Those who don't believe in classes, or think that JavaScript provides
enough through prototypal inheritance.</li>
</ol>

<p>For the reasons outlined above, I'm very much in favor of the former
option: having a language-level class abstraction. This seems to be
<a href="http://h3manth.com/content/classes-javascript-es6">coming soon in ECMAScript 6</a>, and will basically provide
syntactic sugar on top of prototypal inheritance. Having a consistent
class and module system is one of the main reasons why languages like
<a href="http://www.dartlang.org/">Dart</a>, <a href="http://www.typescriptlang.org/">TypeScript</a> and <a href="http://coffeescript.org/">Coffeescript</a> are increasingly
appealing to me.</p>

<h2>Coding to a framework restricts your audience</h2>

<p>Because of a lack of interoperability between frameworks and their class
systems, if you write non-UI code using a framework, only users of that
framework will use your code. It's very unlikely that someone building
an Ember application will want to use your library that uses Backbone
objects. </p>

<p>Often your collaborators may have varied tastes and prefer one framework
over another, but including multiple MVC frameworks in the same
application gets messy quickly. If you have core functionality that you
want to release, release it in pure JavaScript, not as a jQuery plugin,
or Ember module. Of course use prototypal inheritance and proper
abstraction (or at least, as proper as JS can provide).</p>

<h2>Solution: defensive architecture</h2>

<p>To avoid framework and class-system lock-in, I have taken a slightly
different approach to developing with JavaScript MVC frameworks. It
affords the convenience of building a UI with MVC, but keeps the core
of the application flexible.</p>

<p>The basic idea is to separate the core functionality of the application
from the user interface into two separate layers. With this separation,
you can implement the two layers differently:</p>

<ol>
<li><p>Build the base layer using pure JavaScript prototypal inheritance.
This is the part you write with the intention of keeping for later.
This base layer will need an API that you will want to spend a bit of
time honing.  To make the separation crystal clear, you can think of the
UI as a client that uses this API as if it were on the server. This way
you can avoid creating leaky abstractions.</p></li>
<li><p>Use an MVC framework to implement the UI, and call into the base
layer directly. This lets you move quickly and focus entirely on writing
the user interface. This architecture lets you build your UI on a solid
foundation and avoid getting stuck.</p></li>
</ol>

<h2>Benefits of this approach</h2>

<p>You get many benefits by taking this approach:</p>

<ol>
<li><p>If you want to scrap your existing UI and write a new one very
quickly, you can easily do this and still reuse large chunks of your
logic.</p></li>
<li><p>Clear layer separation leads to more maintainable code.</p></li>
<li><p>Easy to ship the underlying core functionality as a library or
standalone module.</p></li>
<li><p>Easy to write unit tests for the core functionality of the
application. Unit tests aren't well suited to user interface code
anyway.</p></li>
</ol>

<h2>This is why we can't have nice things</h2>

<p>The lack of a widely used class system is ridiculous. The sheer number
of different JS class systems is a clear signal that this is a big
omission in the language. Similarly for MVC frameworks. A renewed
interest in JavaScript MVC shows that the web platform needs something
built-in to address this problem.</p>

<p>All other widely used programming languages provide a consistent class
system, and popular platforms provide a framework for separating
application logic and user interface. Until these things come to the
web, I'll continue to have second thoughts about embracing any
particular MVC framework or custom class system.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>How the web should work</title>
    <author><name>Boris Smus</name></author>
    <link href="/how-the-web-should-work"/>
    
    <updated>2012-09-12T09:00:00-00:00</updated>
    
    <id>http://smus.com/how-the-web-should-work</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>When developing in a particular environment, say Android or Cocoa, we
are subconsciously aware that the APIs are essentially fixed and beyond
our control. There is no effective mechanism to go and tell Apple that
some method is poorly named, or tell the Android team how much you wish
the Audio APIs were nicer to use.</p>

<p>The web, however, is built by many different companies and individuals,
giving consumers of the platform (web developers) a unique chance to
also become contributors to its evolution. Rather than griping about how
broken something on the web is, remember that you can play a part in
fixing it!</p>

<!--more-->

<h2>Current flow: "top down"</h2>

<p>Today, features get added to the web platform as a result of competition
between browsers and some standardization work. The web developer has
little say in the process until perhaps the very end. Worse, many spec
authors and browser engineers are not web developers, so they have no
intuition or experience as to what the web platform actually needs.
Here's how it works today:</p>

<ol>
<li>Browser vendor proposes way for solving a problem and implements it
in their browser. (browser vendor)</li>
<li>In some cases it's concurrently introduced as a W3C specification.
(spec author)</li>
<li>Other browser vendors implement the feature. (browser vendor)</li>
<li>Much later, someone implements a JS wrapper on top of feature to make
the feature practical for web developers to use. (web developer)</li>
</ol>

<p>Some of the negative features of this flow are:</p>

<ul>
<li>Takes a long time before any developer feedback is received.</li>
<li>New features have partial support for a very long time.</li>
<li>Innovation is very browser vendor/spec writer centric.</li>
<li>Very slow cycle overall.</li>
<li>Little room for incremental improvements.</li>
</ul>

<h2>Ideal flow "forward polyfills"</h2>

<p>Rather than sticking to the top down flow, web developers can be
included in the loop. While forming organizations like
<a href="http://www.w3.org/community/coremob/">CoreMob</a> might be a good first step, we can have a more direct
impact as well. The following alternative flow can work pretty well in
conjunction with the status quo, described above.</p>

<p>The basic idea is to hit the ground running with a prototype developed
in JavaScript, get early feedback and then propose to W3C when you are
ready.</p>

<ol>
<li>Propose a sane API that developers can use &amp; implement it on top of
existing API(s). (web developer)</li>
<li>Solicit adoption and iterate on feedback. (web developer)</li>
<li>Push through standardization. (spec author)</li>
<li>Implement natively in browsers. (browser vendor)</li>
</ol>

<p>This flow has many benefits over the first one.</p>

<h3>Prototypes have a tighter feedback loop</h3>

<p>By releasing a JavaScript library quickly, you immediately get feedback
from developers. As you tweak your API, you can incorporate real world
feedback. This can be a very short cycle, especially with the help of
tools like github, where consumers of your library can give you
suggestions and fix your bugs!</p>

<h3>Useful out of the gate</h3>

<p>By the time you are ready to propose your library to be standardized as
a core web specification, you already have a reference implementation
with unit tests. These can then be used by browser vendors as they
implement the specification. This is much more useful to have than
the abstract pseudocode that the W3C specs currently provide, and should
lead to a more consistent set of browser implementations.</p>

<p>Even more importantly, you know that what you are proposing is something
useful because that real web developers have already used it! This is
already better than many APIs currently offered by the web platform,
some of which has seen little developer uptake.</p>

<h3>A polyfill at launch</h3>

<p>By the time the first browsers start implementing the spec, you already
have a polyfill for developers to use &mdash; the library you wrote in
step 1. This polyfill should feature detect for the presence of the API,
and if not present, load the functionality via the JavaScript library.</p>

<h2>Limitation: some things need more than JavaScript</h2>

<p>Not all features can be implemented in JavaScript. In fact, some of the
most exciting ones require browser-level innovation because JavaScript
is heavily sandboxed (eg. contacts API, access to new sensors). Still,
many new features <strong>can</strong> be implemented, such as new layout models (eg.
flexbox, new storage APIs, responsive image solutions).</p>

<p>Even if you can't provide a JavaScript implementation, there are ways to
prototype these features to see if they are useful or not, giving
some of the benefits describe above. PhoneGap and other WebView wrappers
can be easily instrumented with plugins that bring native functionality
to the web. More ambitiously, if you can stomach the learning curve,
contribute to open source browsers like WebKit/Chromium and Firefox!</p>

<h2>Limitation: JS library to spec is uncharted territory</h2>

<p>One of the missing pieces in the "forward polyfill" approach I advocate
for in this post is that it can still be very difficult to get your
voice heard without being a browser vendor or spec author.</p>

<p>Anecdotally I've sent a few messages to the <a href="http://goo.gl/hrBvS">www-style@
list</a>, without many tangible results. Other web developers
such as, roughly following the tactic I described earlier, have
experienced similar frustrations in the recent discussions <a href="http://www.webmonkey.com/2012/05/browsers-at-odds-with-web-developers-over-adaptive-images/">regarding
high DPI images</a>.</p>

<h2>Conclusion</h2>

<p>This post is a summary of my thinking around most of my recent web
development work: projects like <a href="https://github.com/borismus/pointer.js">pointer.js</a>, <a href="https://github.com/borismus/physical-units">physical units</a>
and the <a href="https://github.com/borismus/srcset-polyfill/">srcset-polyfill</a> all have the same mission in mind: to
cease to exist by having their functionality replaced by the web
platform itself. The most notable example of such a project is PhoneGap,
which explicitly states self destruction as its now famous <a href="http://phonegap.com/2012/05/09/phonegap-beliefs-goals-and-philosophy/">second
goal</a>.</p>

<p>It may seem strange to develop projects with such a nihilistic purpose.
But keep in mind, that in this case it's not just the code that counts,
but the ideas behind it. We would be better off with many of these
things in the web platform: an implementation of pointer events,
physical units, and a good set of device APIs.</p>

<p>I'm not arguing that the current spec-and-browser-first flow should be
replaced. I'm merely suggesting that there is an alternative out
there that involves web developers; one worthy of exploration by
those of us that aren't in the business of writing browsers but
care enough about the web platform to try to make a change.</p>

<p><strong><em>UPDATE</em></strong>: At Paul Irish's great suggestion, please post your
comments in <a href="https://plus.google.com/115694705577863745195/posts/fMyCkBYvHRi">this Google+ thread</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Generalized input on the cross-device web</title>
    <author><name>Boris Smus</name></author>
    <link href="/mouse-touch-pointer"/>
    
    <updated>2012-06-14T09:00:00-00:00</updated>
    
    <id>http://smus.com/mouse-touch-pointer</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><strong>Update (August 7, 2013): Pointer.js is deprecated. Please use the
<a href="https://github.com/Polymer/PointerEvents">PointerEvents polyfill</a> instead.</strong></p>

<p>Mouse will soon cease to be the dominant input method for computing,
though it will likely remain in some form for the forseeable future.
Touch is the heir to the input throne, and the web needs to be ready.
Unfortunately, the current state of input on the web is... you guessed
it: a complete mess! There are two separate issues:</p>

<ol>
<li>No unified story between mouse, touch, and other spatial input.</li>
<li>Poor support for complex gestures, especially needed for touch.</li>
</ol>

<p>I'll look at each in a bit of detail, and then show you
<a href="https://github.com/borismus/pointer.js">pointer.js</a>. </p>

<!--more-->

<h2>Lack of unified touch and mouse system</h2>

<p>Most web developers should care about providing a good experience on
both mouse and touch interfaces. This is increasingly true with
crossover mouse-touch devices like the Transformer prime, and upcoming
Windows 8 laptops.</p>

<p>Here's what you end up with if you want to support touch and mouse
events on the web today:</p>

<pre><code>$(window).mousedown(function(e) { down(e.pageY); });
$(window).mousemove(function(e) { move(e.pageY); });
$(window).mouseup(function() { up(); });

// Setup touch event handlers.
$(window).bind('touchstart', function(e) {
  e.preventDefault();
  down(e.originalEvent.touches[0].pageY);
});
$(window).bind('touchmove', function(e) {
  e.preventDefault();
  move(e.originalEvent.touches[0].pageY);
});
$(window).bind('touchend', function(e) {
  e.preventDefault();
  up();
});
</code></pre>

<p>The above is a bunch of boilerplate code that does absolutely nothing!
You end up having to manually wrangle two completely different models
into one.</p>

<p>Microsoft is taking a very smart approach with IE10 to address this
issue by <a href="http://blogs.msdn.com/b/ie/archive/2011/09/20/touch-input-for-ie10-and-metro-style-apps.aspx">introducing pointer events</a>. The idea is to
consolidate all input that deals with one or more points on the screen
into a single unified model.</p>

<p>Unfortunately, it's not being proposed as a standardized spec.
Also, because it's not universally available, it will be yet another
thing developers need to support (if they want to support Windows
8/Metro apps). So now our sample above gets <a href="http://blogs.msdn.com/b/ie/archive/2011/10/19/handling-multi-touch-and-mouse-input-in-all-browsers.aspx">even more
boilerplate</a>, with at least three more calls like the following:</p>

<pre><code>$(window).bind('MSPointerDown', function(e) {
  // Extract x, y, and call shared handler.
});
$(window).bind('MSPointerMove', function(e) {
  // Extract x, y, and call shared handler.
});
$(window).bind('MSPointerUp', function(e) {
  // Extract x, y, and call shared handler.
});
</code></pre>

<p>Although their intentions are good, this approach potentially makes the
situation (hopefully temporarily) worse.</p>

<h2>Touch gestures need to be easy</h2>

<p>Touch UIs often involve gestures that aren't easy for developers to
implement, such as pinch-zooming and rotation. However, on the web, due
to the simplicity of <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">the touch events</a>, even implementing
something as simple as a button <a href="http://code.google.com/mobile/articles/fast_buttons.html">is non-trivial</a>.
Implementing more complex gesture recognizers on top of the primitive
<code>touch*</code> events is even less trivial.</p>

<p>Frameworks like <a href="http://dev.sencha.com/deploy/touch/examples/production/kitchensink/index.html/mouse-touch-pointer/#demo/touchevents">Sencha Touch</a> and <a href="http://eightmedia.github.com/hammer.js/">Hammer.js</a> come to
the rescue to address the lack of gestures, however these both have
problems. Sencha comes as a complete package, and it's impossible to use
their gesture recognizer without using their whole framework (or
spending considerable effort trying to pull it out). Hammer.js, on the
other hand, doesn't actually implement gesture recognition for
pinchzoom, but instead relies on the touch spec providing non-standard
<code>rotation</code> and <code>scale</code> values <a href="http://developer.apple.com/library/safari//mouse-touch-pointer/#documentation/UserExperience/Reference/TouchEventClassReference/TouchEvent/TouchEvent.html/mouse-touch-pointer/#//apple_ref/doc/uid/TP40009358">pioneered by Apple</a>.</p>

<p>Microsoft has a gesture layer on top of their consolidated pointer
model. This makes sense as an approach to take. True, certain gestures
only make sense for touch, and it's easy to distinguish the input type
using the <code>event.pointerType</code> API. That said, with a unified model,
there can be new gestures that span multiple input modalities, like
<a href="/mouse-touch-pointer/#">this research</a> suggests.</p>

<h2>Pointer.js - A solution to both problems</h2>

<p>The solution to this problem is to write another library, tag on a <code>.js</code>
to the end of the name, get everyone to use it, prove that it's very
useful, and have browsers and spec implement it natively. Once this
is spec'ed, approved, and widely implemented, it should just be a matter
of removing the script tag!</p>

<p><img src="/mouse-touch-pointer/pointer.js-architecture.png" alt="Pointer.js architecture." /></p>

<p>Pointer.js consolidates pointer-like input models across browsers and
devices. It provides the following:</p>

<ul>
<li>Events: <code>pointerdown, pointermove, pointerup</code></li>
<li>Event payload class: <code>originalEvent, pointerType, getPointerList()</code></li>
<li>Pointer class: <code>x, y, type</code></li>
</ul>

<p>To use it, simply include <code>pointer.js</code> in your web page. This
automatically rigs <code>addEventListener</code> with support for <code>pointer*</code> and
<code>gesture*</code> events.</p>

<p>Try some simple pointer.js demos:</p>

<ul>
<li><a href="http://borismus.github.com/pointer.js/demos/draw.html">Multi-touch drawing</a></li>
<li><a href="http://borismus.github.com/pointer.js/demos/basic-pointers.html">Pointer event logger</a></li>
<li><a href="http://borismus.github.com/pointer.js/demos/basic-gestures.html">Gesture event logger</a> (supports scale, longpress and doubletap)</li>
</ul>

<p>For more info about the library, check it out at
<a href="https://github.com/borismus/pointer.js">https://github.com/borismus/pointer.js</a>. Contributions in the form of
pull requests are most welcome: more demos using pointer events,
unimplemented gesture recognizers for common gestures, like swipe and
rotation, and tweaks to the system itself.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Platform fertility: open for innovation?</title>
    <author><name>Boris Smus</name></author>
    <link href="/platform-fertility"/>
    
    <updated>2012-06-06T09:00:00-00:00</updated>
    
    <id>http://smus.com/platform-fertility</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Although I love Apple's design aesthetic and ability to consistently
churn out amazing hardware, I'm never quite comfortable fully embracing
it. The reasons have to do with platform fertility, or how well suited a
platform is to incremental platform innovations and new platform
creation. Onward!</p>

<!--more-->

<h2>Personal computing</h2>

<p>The evolution of personal computing is generally accepted to look something
like the following:</p>

<pre><code>Mainframe -&gt; Minicomputer -&gt; Personal computer
</code></pre>

<p>The above relationship can be interpreted in at least two ways:</p>

<ol>
<li>The former was disrupted by the latter.</li>
<li>The former was used to design the latter.</li>
</ol>

<p>Taking the first interpretation, we are in the middle (closer to the end, I
reckon) of another disruptive innovation:</p>

<pre><code>Personal computer -&gt; Tablet
</code></pre>

<p>Indeed, compared to laptops, tablets seem to be a better way for the
general public to use computers. The touch interface is more intuitive,
and the physical form factor is better suited to casual use. For the
purposes of this post, let's assume for argument's sake that tablets
overtake personal computing in the future. The interesting thing is what
happens next:</p>

<pre><code>Tablet -&gt; (next disruptive innovation)
</code></pre>

<p>Let's go back to the second interpretation: <code>b was designed using a</code>
(or, in some biblical sense, b begat a). In other words, designers and
engineers used a mainframe to invent minicomputers, a minicomputer to
invent PCs, and a PC to build tablets. We can call this "platform
fertility" just for fun. Up to now, said professionals could use
the latest general purpose computers to do their job, but this may be
changing.</p>

<p>The tablet disruption was enabled by its predecessor, the PC, being
flexible and extensible enough, and thus well suited as a prototyping
platform. Linux, OS X and even Windows are all very flexible platforms,
intended to work with a variety of software toolkits and external
devices. These desktop platforms never imposed restrictions like
software signing, and even if they did, you the power user could always
override.</p>

<h2>Peril of a closed platform</h2>

<p>I don't want to argue about the semantics of the word "open", but
regardless of your religious dispositions, we can all agree that Apple
is not, nor has any pretenses to be associated with, that word. I'm not
making judgements here, it's just how they roll. The API surface is
carefully designed to give developers the right amount of flexibility,
but not more. AppStore is explicitly a sandbox, and if you don't play by
the rules, you lose your playground privileges.</p>

<p>So imagine for a minute that iPad swept the tablet market (shouldn't be
hard given the <a href="http://www.appleinsider.com/articles/12/05/04/ipad_tablet_market_share_will_dip_to_50_by_2017_study_says.html">current market distribution</a>). From a pragmatic
user's perspective, this is fine, even good! iOS is a very
well-integrated platform, working across all shiny Apple products, and
users are generally pretty happy with the interface and overall
experience. From a curious developer's perspective, however, things
are a bit different.</p>

<p>As an iOS developer that wants to improve the platform experience,
however, you are pretty much stuck with how things are. You can't
replace the lock screen, can't write long-running applications that read
in accelerometer data in the background, can't customize your home
screen launcher, etc.</p>

<p>The problem is exacerbated when you set out to try to invent the next
thing. How do you interface with your new stereo camera rig? How many
hurdles do you have to overcome to make it possible to interface with
your new smart watch? How about a pair of smart contact lenses? How do
you get raw USB access? Bluetooth? Ad-hoc wireless? Granted, the further
you venture away from the platform core, the less help you would expect
from it. This is generally where you climb down a layer of abstraction -
for example, to <a href="http://developer.android.com/sdk/ndk/index.html">NDK in Android</a>. Without such an option, you're
left dead in the water.</p>

<h2>A Litmus test</h2>

<p>Unless iPads become more hackable or other, more developer-friendly
tablets emerge as serious competitors, laptop computers will become
specialized tools for software professionals while tablets supplant
laptops for the rest of the public.</p>

<p>Platforms inherently restrict the developer in some sense, placing them
in a box delineated by the APIs that the platform provides. An
interesting way to examine this box is with this notion of "platform
fertility", centering around the question:</p>

<p><strong>Can this platform beget future platforms?</strong></p>

<p>Whether your idea of the next platform is incremental (for example, a
better lock screen) or a fundamental disruptive innovation (for example,
smart glasses), the answer to the above question for the current state
of iOS is a resounding no. Sorry bro!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Let's get physical (units)</title>
    <author><name>Boris Smus</name></author>
    <link href="/physical-units"/>
    
    <updated>2012-05-02T09:00:00-00:00</updated>
    
    <id>http://smus.com/physical-units</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>There's an increasing variety of devices in use today. Even generally
rectangular touch enabled devices vary hugely in their physical sizes,
aspect ratios, pixel densities, etc.</p>

<p>One thing that remains constant across these devices are their users.
Technologies come and go every year, but people stay the same. Existing
form factors: <a href="http://en.wikipedia.org/wiki/Smart_device">pads, tabs and boards</a> still make sense, and
will continue to do so for the forseeable future. As a result,
ergonomic considerations like touch target sizing, readable text and
image size remain constant. Fingers will be fingers and eyes will be
eyes! Our bodies are firmly rooted in the physical world, and the
interfaces we create should reflect that.</p>

<!--more-->

<p>Take touch targets, for example. Apple's <a href="http://developer.apple.com/library/ios/#DOCUMENTATION/UserExperience/Conceptual/MobileHIG/Characteristics/Characteristics.html">human interface guidelines
recommend</a> minimum touch target size of 44x44pt for iOS. On
an iPhone, 44pt is about 0.25in. On an iPad, 44pt is about 0.3in, physically
larger because of different device pixel density. Even Apple has such
discrepancies despite a very simple device landscape. Apple devices only
come in two configurations: iPhone and iPad (and their retina
counterparts).</p>

<p>The situation gets far more complex when we look at mobile devices in
general, which is the world of mobile web developers. In an ideal world,
images and fonts should be scalable and all units should be physical.
I built a <a href="https://github.com/borismus/physical-units">prototype</a> that illustrates roughly how this
model would work. Onward for the gory details.</p>

<h2>Device variance</h2>

<p>Here's a brief sampling of pixel density and aspect ratio for a few
popular devices (data <a href="http://en.wikipedia.org/wiki/List_of_displays_by_pixel_density">from wikipedia</a>):</p>

<pre><code>Device          DPI   Aspect
============================
iPhone*         163      3:2
iPad*           132      4:3
Nexus S*        117      5:3
Galaxy Nexus*   158     16:9
Galaxy Note*    142      8:5
Galaxy Tab      149      8:5
T-Prime         149      8:5
Lumia 900       217     10:6
</code></pre>

<p>The ones with a <code>*</code> denote double density devices. In other words, the
physical pixel density is double the value indicated, but the browser
generally reports the resolution to be this halved value. For more
information about this practice, read <a href="http://www.quirksmode.org/blog/archives/2010/04/a_pixel_is_not.html">a pixel is not a pixel</a>.</p>

<p>Thus, in practice, a 44x44px button rendered with a reasonable viewport:
<code>&lt;meta name="viewport" content="width=device-width"&gt;</code> will vary in
dimensions between 0.20in on the Lumia 900 to nearly double, 0.38in on
the Nexus S.</p>

<p>Further complicating things is a somewhat obscure fact that pixels might
not be square at all. The term for it is <a href="http://en.wikipedia.org/wiki/Pixel_aspect_ratio">pixel aspect
ratio</a>. As far as I can tell, this is only a concern when
displays aren't using their native resolutions, which luckily is a
rarity in mobile devices.</p>

<h2>The problem with pixels</h2>

<p><a href="http://www.lukew.com/ff/entry.asp?1085">Many HIGs</a> recommend minimum touch target sizes in
pixels. What really matters is the size of human fingers, which as we
established earlier, don't grow and shrink as a function of the device
you happen to be using.</p>

<p>Microsoft, anticipating a more fragmented ecosystem of devices with
varied densities and screen sizes, is recommending a physical size
approach instead, which makes more sense for touch targets. They suggest
a 9x9mm recommended lower bound, and an absolute minimum of 7x7mm. This
jives well with established research, such as MIT's <a href="http://touchlab.mit.edu/publications/2003_009.pdf">Touch Lab study of
Human Fingertips to Investigate the Mechanics of Tactile Sense</a>,
which found that the average human finger pad is 10-14mm and the average
fingertip is 8-10mm in diameter.</p>

<h2>Benefits of physical units</h2>

<p>As a developer, I'd like to just say "this button is 40x9mm" and have
that actually map to real physical dimensions. If I could ensure usable
sizes for all of my touch targets, that solve this problem across all
devices, once and for all!</p>

<p>Physical units also make feature-based device detection far more
reliable. Currently, the best we can do is basically guess what sort of
device is visiting your site, using heuristics like device resolution
(in magic CSS pixels that automatically get scaled). Using this
approach, it's hard to distinguish between laptops and tablets, for
example. It's even more challenging to distinguish phones from 7"
tablets, or 7" tablets from 10" ones. Approaches like <a href="https://github.com/borismus/device.js">device.js</a> would
benefit greatly.</p>

<p>Finally, as you saw, using pixels results in a huge variance in font
size, resulting in bad text readability in general. With physical units,
you can set a good default baseline size for text to ensure readability.</p>

<p>Of course, in all of these cases, it's possible to manually rescale the
view in order to get the desired zoom level that's readable and
touchable, but the whole point is to have a system that behaves well by
default.</p>

<h2>Scaling well</h2>

<p>So, in this world of physical sizes for everything, virtually everything
needs to be scaled to the appropriate size. This includes text, images,
and general layout (eg.  sizes, coordinates, etc). This means having
assets, fonts and a layout engine that's capable of scaling well,
without loss of visual fidelity.</p>

<p>General layout is pretty easy to do - all you need is to convert real
units into pixels for actually rendering. Text gets a bit tricker, since
most fonts are defined only at certain key-sizes, and scaling to
fractional sizes might not work perfectly. Scaling images, of course, is
a whole separate topic. Rasters are very difficult to scale down without
compromising quality, and scaling up creates a blurry or pixelated
looking image. The obvious solution is to use scalable image formats. On
the web, this basically means SVG, which is <a href="http://caniuse.com/#search=svg">quite well
supported</a> across browsers, but does have its own
idiosyncrasies.</p>

<h3>Vectors and rasters</h3>

<p>Still, even with vector images, there are potential arguments to be made
about the superiority of pixel-perfect assets, since the designer has
absolute control to decide their assets at the pixel level.
Unfortunately, the pixel-perfect approach causes a lot of pain for
designers, who are routinely forced to create multiple versions of the
same asset. On iOS, you specify both the regular and retina asset. On
Android, you specify four: <code>ldpi</code>, <code>mdpi</code>, <code>hdpi</code> and <code>xhdpi</code>. These
don't actually get scaled, but the closest one gets served based on the
device DPI. This means that it's impossible to create assets in an exact
physical size. Here's an excerpt from the <a href="http://developer.android.com/guide/practices/screens_support.html">Android docs</a>:</p>

<p><img src="/physical-units/android-dpi.png" alt="Android assets and device DPI" /></p>

<p>On the web, you can't realistically hope for pixel perfection because of
the vast variety of devices and browsers. Designers need to embrace the
medium, and do as well as they can given the constraints of the web.
Scaling the same image (vector or raster) may not be adequate for other
reasons. In some cases, especially with icons, you want to specify
assets with different levels of detail depending on the physical size.
Check out a few <a href="http://mrgan.tumblr.com/post/708404794/ios-app-icon-sizes">great</a> <a href="http://www.pushing-pixels.org/2011/11/04/about-those-vector-icons.html">posts</a> on this subject.</p>

<blockquote>
  <p>It’s simply not possible to create excellent, detailed icons which can
  be arbitrarily scaled to very small dimensions while preserving clarity.</p>
</blockquote>

<p><img src="http://farm7.static.flickr.com/6224/6311957505_6f15b6f925.jpg" alt="Varying levels of details depending on size" /></p>

<p>That said, there's no reason why icons with appropriate levels of detail
for smaller and larger sized screens shouldn't use the same scalable,
physical approach. In the above example, using scaled assets would let
the designer create 4 instead of 5 separate assets, since the 128px and
64px versions are the same, just scaled.</p>

<p>It's not all flowers and sausages, though. Vectors are inherently less
efficient to deal with, since they require the intermediate step of
rasterization before they can be blitted to the screen buffer. This is
expensive, but not a show stopper. Optimizing this issue is a topic that
probably warrants a whole article on its own, but one major win could be
to rasterize once depending on your device DPI, and then cache the
rasterized asset so that you don't need to rasterize the vector every
time you render.</p>

<h2>Doing it on the web</h2>

<p>If you want to figure out your phone's exact physical dimensions in a
browser based application, you're out of luck. Even units that share a
name with physical units (like <code>inch</code>, <code>cm</code>, etc) don't actually render
that way. Here's a <a href="http://lewisnyman.github.com/Where-are-our-absolute-units--Demo/">test page</a> that renders some text with
CSS inches. If you measure the actual output on your laptop or phone,
you'll notice that it's not a real inch.</p>

<p>Now, you could imagine doing some monkey patching to enable physical
units using JavaScript, but that's also impossible to do cleanly since
you need to know either the true device DPI or the physical size of
the screen, neither of which is queriable in any way on the web.
JavaScript does provide <code>window.devicePixelRatio</code>, but that only reports
the scale factor for pixel density, which is used for retina and other
displays where CSS pixels differ from physical pixels.</p>

<h2>How does it feel?</h2>

<p>Even though it's impossible to get physical size or DPI from a browser,
I wanted to try it out, to see if this would work in practice. Here's a
<a href="https://github.com/borismus/physical-units">prototype library</a> that uses entirely physical units
for everything, and fully scalable assets. All objects shown on <a href="http://borismus.github.com/physical-units/controls.html">this
controls page</a> should render to the correct physical size on
the following devices: iPhone, Galaxy Nexus, iPad, MacBook Air 13". Go
ahead and pull out a ruler (I did) and measure - the sizes should be
exact across the supported devices.</p>

<p>Here's a screenshot of a <a href="http://borismus.github.com/physical-units/sample.html">basic phone UI</a> built with only physical
units and percentages on an iPhone 4S and Galaxy Nexus:</p>

<p><img src="/physical-units/android-ios-physical.png" alt="Phones with a UI built using physical units" /></p>

<h3>Implementation details</h3>

<p>As mentioned, there's no way to get DPI in the browser. My prototype
works because I've hard-coded the physical DPI for each supported device
based on user agent matching. It's a terrible hack and not a great
option for production, even with more complete coverage.</p>

<p>The prototype currently only works by specifying sizes in absolute
units. It currently only supports inline styles, and not CSS rules. For
example, one can now write the DOM like:</p>

<pre><code>&lt;button style="width: 9mm;"&gt;Nice touch target&lt;/button&gt;
</code></pre>

<p>I ran into a few small implementation details along the way that I
thought might be worth mentioning.</p>

<p>Firstly, by default, nginx doesn't serve SVG with the correct mime type.
So, visiting the asset URL just downloads the asset rather than
rendering it, and embedding the asset in an <code>img</code> tag doesn't work. The
fix is <a href="http://stackoverflow.com/questions/3695409/nginx-offers-of-downoload-svg-instead-of-showing-it">simple and documented</a>: add the correct mime type to an nginx
configuration file.</p>

<p>Secondly, I was a bit disappointed not to find many pre-downloadable SVG
icon sets, but did discover <a href="http://raphaeljs.com/icons/">this resource</a> which has SVG
icons as pathes. To use them it's just a matter of pasting the path
specification into an SVG <code>path</code> element, as follows:</p>

<pre><code>&lt;svg width="32" height="32" xmlns="http://www.w3.org/2000/svg"&gt;
  &lt;path d="your-pathspec-goes-here" /&gt;
&lt;/svg&gt;
</code></pre>

<p>Though unstated, the icons on the above site seem to all be 32x32 units
(Note: not pixels).</p>

<p>Lastly, since CSS has units that look like physical ones (cm, mm, in,
etc), I wanted to create a new unit, say truemm, truein, etc.
Unfortunately, Chrome (and perhaps other browsers, haven't tested)
doesn't support units that it doesn't recognize. As a result, I had to
piggyback and override existing units. This approach would break the
behavior of sites that use for some unknown-to-me reason, but
introducing new units prefixed by <code>true</code> is verbose and confusing for
beginners. If you know of any reasons why a developer today would be
using CSS mms/inches, please let me know.</p>

<h2>Angular units</h2>

<p>Rob <a href="http://lists.w3.org/Archives/Public/www-style/2012Feb/0948.html">writes</a>:</p>

<blockquote>
  <p>The only use-cases for truemm are when you need content matching the
  size of some real-world object, e.g. a ruler, or a life-size image, or
  a human fingertip. That's it.</p>
</blockquote>

<p>I disagree. It so happens that generally touch devices are used at a
relatively fixed distance from screen to user. This makes physical units
useful for not just touch targets, but also text and image readability.</p>

<p>But in the same breath, Rob makes a great point:</p>

<blockquote>
  <p>Ask yourself, "do I really want this content to be the same physical
  size on a phone and a wall projector?" If the answer is yes, use
  truemm, otherwise don't.</p>
</blockquote>

<p>Indeed, there are cases where physical units aren't ideal. The
conditions for this are roughly:</p>

<ol>
<li>Unknown distance from viewer to screen.</li>
<li>Unknown screen size.</li>
</ol>

<p>In this case, the reasonable thing to do is adopt an angular unit, which
would be able to scale depending on viewing distance from the screen
(and of course DPI too).  This might mean that you use degrees, <a href="http://en.wikipedia.org/wiki/Minute_of_arc">minutes
of arc</a>, or some arbitrary angular unit "au" (not to be confused
with <a href="http://en.wikipedia.org/wiki/Astronomical_unit">AU</a>).</p>

<p>The idea of angular units isn't new. In fact, surprisingly, the CSS spec
features it quite prominently in the definition of CSS pixels. A <a href="http://inamidst.com/stuff/notes/csspx">recent
article</a> on this topic reminded that pixels are actually
specified to be angular in nature.</p>

<blockquote>
  <p>The reference pixel is the visual angle of one pixel on a device with
  a pixel density of 96dpi and a distance from the reader of an arm's
  length. For a nominal arm's length of 28 inches, the visual angle is
  therefore about 0.0213 degrees. For reading at arm's length, 1px thus
  corresponds to about 0.26 mm (1/96 inch). -- <a href="http://www.w3.org/TR/CSS21/syndata.html#length-units">CSS spec</a></p>
</blockquote>

<p>There are a couple of problem with this.</p>

<ol>
<li><p>Poor naming: calling such a unit a pixel is incredibly confusing.</p></li>
<li><p>Lack of flexibility: this approach assumes a 96dpi display and a
fixed viewing distance. As previously discussed, the former is a bad
assumption, and in the case of large format viewing, so is the latter.</p></li>
</ol>

<p>Introducing true angular units that can be configured based on display
distance from the observer would be a boon for designers and engineers
working on software for TVs and other large-format devices. Some default
viewing distance could be provided, but developers should have a means
of specifying a custom viewing distance (potentially inferred from
device hardware, as described in <a href="http://www.chrisharrison.net/index.php/Research/LeanAndZoom">this research</a>).</p>

<h2>Next steps</h2>

<p>Browser vendors need to start supporting true physical units. One path
might be creating all new units (eg. <code>truemm</code>, <code>truein</code>) or redefining
the old ones to be use absolute values, which in many cases may be what
the developers actually intended.</p>

<p>These units should also be usable in media queries, so that it's
possible to do things like:</p>

<pre><code>(max-device-width: 4in)
</code></pre>

<p>This will enable a more robust way to do device detection via media
queries a la <a href="https://github.com/borismus/device.js">device.js</a>.</p>

<p>There appear to be some efforts from Mozilla to have a media query for
DPI, via <a href="https://developer.mozilla.org/en/CSS/resolution">resolution</a>. This is a good start, but there should
also be a way to get DPI directly via JavaScript, otherwise enterprising
developers will need to resort to binary searching through possible DPIs
via <code>window.matchMedia</code>, which is just ridiculous.</p>

<p>The introduction of a true angular unit with a configurable viewing
distance would be amazing for a web that works well on large-format
displays.</p>

<p>Ultimately, UI developers need to understand the serious problems with
pixels because this problem isn't going away any time soon.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>New design</title>
    <author><name>Boris Smus</name></author>
    <link href="/redesign-2012"/>
    
    <updated>2012-03-30T09:00:00-00:00</updated>
    
    <id>http://smus.com/redesign-2012</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I re-designed this site using the <a href="http://www.google.com/webfonts/specimen/PT+Sans">PT Sans font</a>, aiming for
appealing typography for optimal readability. Interestingly,</p>

<blockquote>
  <p>PT Sans is based on Russian sans serif types of the second part of the
  20th century, but at the same time has distinctive features of
  contemporary humanistic designs.</p>
</blockquote>

<p>Since visitors are increasingly coming from a variety of devices, I also
created three variants of the site for small, medium and large screens
via media queries:</p>

<!--more-->

<pre><code>@media screen and (max-width: $small) {/* small */}
@media screen and (min-width: $small) and (max-width: $large) {/* medium */}
@media screen and (min-width: $large) {/* large */}
</code></pre>

<p>Note that the above is valid <a href="http://sass-lang.com/">SCSS</a> (as of version 3.2)! To try it out,
you may need to install the prerelease via <code>gem install sass --pre</code>.</p>

<p>As usual, the layout of this site is largely inspired by designers far
more skilled than I, including: <a href="http://viljamis.com/">http://viljamis.com/</a> and
<a href="http://www.markboulton.co.uk/">http://www.markboulton.co.uk/</a>.</p>

<h2>New engine</h2>

<p>This blog can now also double as a micro blog, where I can easily <a href="/archive/links">post
links</a> in a social network-agnostic way. Similarly, it now hosts
<a href="/archive/talks">all of my talks</a>, which used to live on
<a href="http://smustalks.appspot.com">http://smustalks.appspot.com</a>. It's also a lot easier for me to create
new posts.</p>

<p>All of these structural changes are largely due to me building a
completely new static site generator which addresses a lot of my pains
using other static generators. I'll write a separate article about this
engine when I'm ready to release it to the public.</p>

<h2>No more comments</h2>

<p>Since the start of this blog, I've struggled with the concept of
comments on my site and in on blogs in general. Even with services like
disqus, which try to integrate your blog commentator personality into
one place, I feel that in practice, commenting on the web is very hard
to keep in one place, and the most interesting discussions end up
distributed in many pockets such as Hacker News, twitter, etc. Having
the in-blog comments as well only makes things messier.</p>

<p>Other problems include:</p>

<ul>
<li>Leaving a comment right after reading content encourages unconsidered
responses, since readers haven't had time to process the content.</li>
<li>Lately my blog has been generating a lot of spam comments that I need
to moderate. I don't have time for that.</li>
<li>People tend to ask for technical support in the comments, which adds
very little value for other readers.</li>
<li>Disqus slows down page load time, and presents a UI that's not easily
skinnable.</li>
</ul>

<p>For a more in-depth analysis about blog comments, see Matt Gemmell's
thorough post <a href="http://mattgemmell.com/2011/11/29/comments-off/">on this subject</a>.</p>

<p>I've disabled comments on all posts. That said, I still want to hear
your ideas and feedback, and engage in discussion around topics I'm
obviously interested in (enough to write about!)</p>

<h2>Evolving designs</h2>

<p>This is the fourth iteration of my blog's design. I apparently do a
re-design every year, since I started in 2008.</p>

<p><style>
article img { border: 1px solid gray; }
</style></p>

<p><img src="/redesign-2012/v1.png" alt="original un-design" /></p>

<p>This version was a small modification done to an existing wordpress
theme.</p>

<p><img src="/redesign-2012/v2.png" alt="first iteration" /></p>

<p>I really liked this version for a long time, since it was so clean and
minimal. Unfortunately the header took way too much space, and the
typography left something to be desired.</p>

<p><img src="/redesign-2012/v3.png" alt="second iteration" /></p>

<p>This was the first "responsive" version. It had too many gimmicky,
non-standard design elements.</p>

<p><img src="/redesign-2012/v4.png" alt="current iteration" /></p>

<p>I'm reasonably happy with the current version, especially the typography
in the main body.</p>

<p>And strangely excited about the engine that powers this blog too
(codenamed <em>Lightning</em>, which generates a lot of static, and is also
very fast! Ha, get it?). Once released, I will write another meta-blog
post about it. For now, though, I wrote a bit about Lightning
<a href="/site">here</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Backbone and ember</title>
    <author><name>Boris Smus</name></author>
    <link href="/backbone-and-ember"/>
    
    <updated>2012-02-15T09:00:00-00:00</updated>
    
    <id>http://smus.com/backbone-and-ember</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="http://emberjs.com/">Ember</a> and <a href="http://documentcloud.github.com/backbone/">Backbone</a> are both promising JavaScript frameworks but have
completely different philosophies. In this post, I'll compare the two, both
from a practical and philosophical perspective. I'll defer to real world
experience with Backbone and <a href="http://sproutcore.com">SproutCore</a> (Ember's predecessor), as well as
basic experiments with Ember (full disclosure: haven't built a large Ember app
yet). I'll also supplement claims with quotes from a fantastic <a href="http://goo.gl/t7gHG">conversation</a>
from Freenode <a href="/backbone-and-ember/irc://irc.freenode.net/#documentcloud">#documentcloud</a> on February 3rd, 2011. For quote
context, <code>wycats</code> is <a href="http://yehudakatz.com/">Yehuda Katz</a>, one of the lead developers on Ember, and
<code>jashkenas</code> is <a href="http://ashkenas.com/">Jeremy Ashkenas</a>, one of the lead developers on Backbone.
<!--more--></p>

<p>Before I go into too much detail, it's pretty clear that both frameworks have
the same goal: to help developers build apps.</p>

<blockquote>
  <p>1:09 PM <strong>wycats</strong> backbone apps are apps</p>
</blockquote>

<p>So we have a roughly apples-to-apples comparison. Let me dive in and talk about
some philosophical differences:</p>

<h2>Backbone at a glance</h2>

<p>Backbone is a minimalist framework that builds on top of ideas from
jQuery to give some structure to web applications. It introduces
concepts of views, models, a restful sync interface, routers, etc, in
surprisingly little code. Backbone is the darling of microframework
lovers, who highly value small framework size and don't want to commit
to a single "full stack" solution.</p>

<p>Backbone is incredibly flexible, and doesn't impose how your views and
models should actually interact. The main benefit is that it adds some
structure to the app and provides convenient ways of listening to DOM
events and turning them into application events.</p>

<blockquote>
  <p>1:15 PM <strong>wycats</strong> backbone is 600loc <br />
  1:16 PM <strong>wycats</strong> "here's how you should think about your app structure"</p>
</blockquote>

<p>Since Backbone is so small, it leaves a lot of decisions up to the
developer. This is both a blessing (flexibility, works for me, etc) and
a curse, since support for many things is missing.</p>

<blockquote>
  <p>12:32 PM <strong>wycats</strong> jashkenas: backbone is 600 lines of code <br />
  12:32 PM <strong>wycats</strong> jashkenas: the idea that there are things missing in it that
  are common should not be controversial</p>
</blockquote>

<h2>Ember at a glance</h2>

<p>Ember has a very different history. It's an evolution of SproutCore,
which is a complete web application solution. Ember takes the core parts
of SproutCore - two-way data binding, computed properties, tight
template integration, and strips the rest off into sub-modules. Things
that come in module format are a data serialization layer (via <a href="https://github.com/emberjs/data">data</a>),
routing (via <a href="https://github.com/emberjs-addons/sproutcore-routing">routes</a>). Ember's out of the box functionality is
actually smaller than Backbone's, but it provides a higher level
abstraction.</p>

<p>I used SproutCore back in 2008 while working as an engineer on
iWork.com. It was a SproutCore pre-1.0 release, and things were a bit
rough. Ember seems to have come clean in many ways, presenting a more
consistent template data binding solution via Handlebars, and being a
lot less monolithic SproutCore once was.</p>

<p>Ember aims to provide a full solution in an opinionated way. Thus,
to get the most of the framework, developers must do things in a certain
style. While you're not forced to use Handlebars, it's the only way to
get some of the compelling features the framework provides (eg. for
two-way data binding). Many of the auxillary modules such as ember-data
are designed to fit will into the existing structure of the framework.</p>

<blockquote>
  <p>1:22 PM <strong>wycats</strong> tbranyen: ember is an end-to-end framework built on
  top of more modular components</p>
</blockquote>

<p>There is a sentiment from many people, including Backbone's founder that
much of what Ember provides is over-engineered:</p>

<blockquote>
  <p>12:28 PM <strong>jashkenas</strong> It's Backbone's take that Ember's more complex
  data binding model, intermediate controllers, run loop etc. ... are
  all interesting approaches, but are <em>not</em> usually helpful in building
  a real site.</p>
</blockquote>

<h2>Different audiences</h2>

<blockquote>
  <p>12:30 PM <strong>jashkenas</strong> wycats: and I find quite the opposite --
  self-selecting sample pools, as you'd expect.</p>
</blockquote>

<p>So to summarize at a high level, there are two camps (and excuse me
while I generalize a lot).</p>

<ol>
<li><p>Lovers of micro-frameworks. JavaScript hackers seeking extra
structure in their slightly complex apps. These people are
comfortable mixing and matching frameworks, solving problems as they
come, and just want to get started quickly.</p></li>
<li><p>Software Engineers that are used to a deep abstraction layer and a
full service stack. These people are probably coming from native app
development and want to write very complex applications on the web.</p></li>
</ol>

<p>Basically, these two developers have different needs, come from
different programming cultures, and are maybe even writing different
applications.</p>

<blockquote>
  <p>12:31 PM <strong>wycats</strong> jashkenas: so then it's fair to say that for <em>some</em>
  people, Ember's approach is overkill</p>
</blockquote>

<p>Andrew says it best:</p>

<blockquote>
  <p>12:36 PM <strong>andrewdeandrade</strong> It's really all about values. Occasionally
  I get frustrated by things backbone.js doesn't do and occasionally I
  get frustrated by things rails does that are hard to undo. My personal
  preference is to have a framework not do something and implement it
  myself than have a framework do something and figure out how to do the
  opposite. That's me. I know people who feel differently</p>
</blockquote>

<h2>A more detailed comparison</h2>

<p>After engineering with SproutCore, writing an app with Backbone, and
writing small amounts of Ember (mostly samples to get a feel for data
bindings, etc), I've got some sense of the issues that you will run into
when developing a moderately complex app. I'll go over some, and
supplement them with quotes from framework authors.</p>

<blockquote>
  <p>1:28 PM <strong>wycats</strong> jashkenas: do you disagree that the pattern "listen
  for these properties, and when any of them changes, trigger observers"
  is very common? <br />
  1:26 PM <strong>jashkenas</strong> that's correct -- you listen for changes to the source data and render computed values. not hard. <br />
  1:26 PM <strong>wycats</strong> jashkenas: yes… a pattern that happens sufficiently often that it's good to abstract</p>
</blockquote>

<p>In general, Ember seeks to find common problems that developers face,
and solve them in an opinionated way. Backbone, on the other hand,
leaves it to developers solve their own problems in the way that works
for them.</p>

<h3>Templates</h3>

<p>While Ember in theory lets you pick which templating engine to use, you
lose a lot of benefits if you're not using Handlebars.</p>

<blockquote>
  <p>1:29 PM <strong>jashkenas</strong> forcing your users to use logic-less templates is <em>incredibly</em> constraining. <br />
  1:30 PM <strong>tomdale</strong> jashkenas: ember supports any templating language you'd like. you just don't get auto-updating <br />
  1:30 PM <strong>jashkenas</strong> right, exactly.</p>
</blockquote>

<p>In practice, my last project used Mustache because it was logic-less.
Such templating engines are my preference, and I'm happy to be
constrained to a pretty good templating system if that means access to
powerful features, cleaner application code and less boilerplate.</p>

<p>I don't think that Ember should be toting choice of template as a big
benefit. Like any application framework, Ember is a contract between
developer and framework author. You write in our style and we'll give
you powerful features. In an ideal world, this lets developers focus on
their app instead of dealing with middleware.</p>

<h3>Views</h3>

<p>view decomposition is one of the first tasks a front end developer
faces. They need to decide which parts of their app will be implemented
with what view. Should each list item be its own view? Should the list
be a single monolithic view?</p>

<p>In my experience writing Backbone apps, views are very primitive and
tend to cause issues. There's no support for any sort of view nesting,
which is totally critical for large applications with complex UIs. In
contrast, Ember provides an easy way of nesting views inside one
another.</p>

<p>The other thing you'll notice with Backbone is that there's a lot of
micro-management required when building Backbone views. They need to
be properly cleaned up by hand, otherwise you end up with <a href="http://stackoverflow.com/questions/7125402/backbone-bind-multi-event-to-one-button-after-i-new-view-multi-times">zombie
views</a> bound to events, or events that <a href="http://stackoverflow.com/questions/7348988/backbone-js-events-not-firing-after-re-render">don't fire at all</a>.</p>

<p>Some backbone projects, such as the <a href="https://github.com/tbranyen/backbone.layoutmanager">layout manager</a>, aim to remedy
some of these limitations by creating a Layout abstraction, that allows
nested views and handles a lot of the rendering. This is a very
interesting project, but I haven't tried it yet.</p>

<h3>MVC?</h3>

<p>Ember is a traditional MVC framework, where it's clear which parts are
the view, the controller and the model. Backbone on the other hand, is
explicitly not an MVC. It never even claims to be!</p>

<p>In fact, if you read the <a href="http://documentcloud.github.com/backbone/">main page</a>, you'll notice that
there's no mention of controllers anywhere. Don't get me wrong: Backbone
still gives your applications structure, but has no opinion on the
gluing layer between data and presentation. I think Backbone once had
controllers, but they were renamed to routers, designed primarily for
handling URLs and history/pushState.</p>

<p>In my experience, there's very much a need for a controller when writing
even moderately complex apps. You're presented with several options:</p>

<ol>
<li>Write controller code in views</li>
<li>Write controller code in models</li>
<li>Write controller code in a router</li>
<li>Write your own controller infrastructure</li>
</ol>

<p>If you care about separation of concerns, none of these options are
really acceptable.</p>

<h3>Data and servers</h3>

<p>Backbone packs a huge punch in a small package. It comes with
<code>Backbone.sync</code>, which lets you fully customize how you want to interact
with the server.</p>

<p>Following the CRUD pattern, Backbone lets you specify the response
format of an Read, but unfortunately doesn't allow you to fully
customize how you would like to serialize Create and Update and Delete
payloads for calling your server-side API. For my last project, I ended
up just completely redefining Backbone.sync, which is very powerful, but
I needed to write a lot of boilerplate to make it work reasonably.</p>

<p>The problem with Backbone's approach is that it doesn't separate two
parts of data stores: internal collection management and the interface
with the backend API.</p>

<p>Ember data, on the other hand, has the concept of API adapters, which
let you specify the interface the server. Additionally it has a
DataStore model. I haven't experimented with this in Ember yet, but
SproutCore's version of this worked quite well.</p>

<h3>Performance considerations</h3>

<p>When I worked on iWork.com, we definitely had some issues with
SproutCore performance for large amounts of data. The problem was that
we couldn't really optimize our code because we were locked into the way
SproutCore does things, so our way forward was to patch SproutCore
itself to address some of the performance issues, or break out of the
framework and implement the performance intensive part manually.</p>

<p>Jeremy voices this concern here:</p>

<blockquote>
  <p>1:33 PM <strong>jashkenas</strong> finally, and perhaps most importantly, embers tight
  coupling of handlebars-ui-with-very-specific-bindings-to-ember-models is
  trouble, performance-wise. You can't build the really intensive parts of
  your UI with that level of binding / dom tweaking.</p>
</blockquote>

<p>And I agree with him. It's very legitimate concern for large apps, and
Yehuda didn't answer adequately in my opinion.</p>

<blockquote>
  <p>1:34 PM <strong>wycats</strong> jashkenas: exposing the performance question to the
  user is trouble <br />
  1:34 PM <strong>wycats</strong> over the long haul Ember will be able to heuristically
  decide how bulky to update <br />
  1:34 PM <strong>jashkenas</strong> sufficiently smart compiler, eh? <br />
  1:34 PM <strong>wycats</strong> jashkenas: :P  </p>
</blockquote>

<p>I'm a bit concerned about this point and would love to hear a less
handwavy answer from Yehuda.</p>

<h2>Final thoughts (Backbone)</h2>

<p>My conclusion from writing an app with Backbone, and attending a Bocoup
training workshop, is that Backbone by itself is not sufficient for
building complex web apps. You will invariably go one of two directions:</p>

<ol>
<li>Engineer a lot of stuff on top of it, or</li>
<li>Use existing Backbone plugins of various maturity and hope they work
well together.</li>
</ol>

<p>Based on my experience with jQuery, and the mess of ensuing plugins, The
latter seems overly optimistic. So basically, be prepared to write a lot
of extra <a href="https://github.com/tbranyen/backbone-boilerplate">boilerplate</a> code. But if you're a JavaScript developer, you
can handle that!</p>

<p>In practice, it's very difficult to remain productive if you're writing
both an app and a framework at the same time. Unfortunately this was my
tendency when using Backbone. I hate reinventing the wheel. Especially
if it's kind of lopsided.</p>

<p>That said, Backbone is fantastic for mid-to-low complexity applications
that want to maintain structured code.</p>

<h2>Final thoughts (Ember)</h2>

<p>Ember, on the other hand, forces you into its way of doing things. This
is a framework with opinions that gives you less flexibility. However,
if you grit your teeth a little bit and buy in, you'll be exposed to a
well thought out set of libraries that work well together.</p>

<p>I really like the features Ember offers, and its philosophy of finding
common problems developers will face and solving them. One thing I
anticipate is that many of the non-core Ember modules are immature.
However, just having these modules that are designed to work together is
a boon for serious application developers.</p>

<p>I'm still concerned that Ember applications may be stuck if dealing with
particularly hairy custom view situations, or large amounts of data, but
I'll have a better sense of the limitations soon.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>PDF conversion and client-side scraping</title>
    <author><name>Boris Smus</name></author>
    <link href="/pdf-conversion-client-side-scraping"/>
    
    <updated>2012-02-08T09:00:00-00:00</updated>
    
    <id>http://smus.com/pdf-conversion-client-side-scraping</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>One of my ongoing projects is called smusique, a mobile web-based sheet
music viewer. The application was designed for tablets, enabling a
rich browsing experience through sheet music. Though some sheet
music is encumbered by licenses, most of the classic stuff is legally
available online via sheet music databases such as the <a href="http://imslp.org">Petrucci
Library</a> at <a href="http://imslp.org">imslp.org</a>.
<!--more--></p>

<p>While the focus of my project was to design and develop a delightful
tablet-based interface for sheet music, I needed to solve some technical
problems:</p>

<ol>
<li>Figure out how to deal with PDFs in the browser.</li>
<li>Create or find a database with a large enough corpus of searchable
sheet music.</li>
</ol>

<p>Read on to find out how I solved these problems using the AppEngine
conversion API and through a Chrome extension that does client-side
scraping.</p>

<h2>Dealing with PDFs</h2>

<p>Since digital sheets are largely in PDF format, I needed some solution
for showing PDFs inside a web user interface. Unfortunately
showing PDFs inline is pretty much impossible using today's web. Ideally,
the browser would have support for something like the following:</p>

<pre><code>&lt;img src="something.pdf" page="3" /&gt;
</code></pre>

<p>But this functionality simply doesn't exist in a widely supported
fasion. Given how important showing PDFs is for my application, I
needed a workaround. A few options came to mind:</p>

<ol>
<li>Attempt to show PDFs in an iframe, and programatically scroll to the
right position based on the page number.</li>
<li>Convert PDFs on the client side using something like <a href="http://andreasgal.com/2011/06/15/pdf-js/">PDFJS</a></li>
<li>Build my own PDF conversion server</li>
</ol>

<p>I experimented briefly with the first approach and unsurprisingly found
mixed levels of browser support for PDF rendering inside iframes. Some mobile
browsers allowed PDFs to be opened within iframes, but exhibited
unexpected scaling behavior, could not be programmatically scrolled, or
both.</p>

<p>Did not seriously consider PDFJS as a solution, judging that running PDF
conversion in a mobile web browser would be prohibitively slow.</p>

<p>Just as I was bracing to take the plunge and write my own
<a href="http://www.imagemagick.org/">ImageMagick</a>-based conversion API running on Django/Slicehost, I
discovered the very new <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">AppEngine conversion API</a>.</p>

<h2>The conversion API</h2>

<p>Turns out that AppEngine provides a new experimental <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">file format
conversion API</a>. It allows you to map from a variety of input file
formats to a variety of outputs, including PDF to PNGs (one per page),
which is exactly what I wanted. The full list of conversion paths is
available in <a href="http://code.google.com/appengine/docs/python/conversion/overview.html">the docs</a>. Some of the more exciting
features include image to text conversions via OCR, and generating images
from HTML.</p>

<p>My conversion was really straight forward to implement:</p>

<pre><code>def convert_pdf(self, pdf_data):
    """Converts PDF to PNG images. Returns an array of PNG data."""
    asset = conversion.Asset('application/pdf', pdf_data, 'sheet.pdf')
    conversion_request = conversion.Conversion(asset, 'image/png')
    result = conversion.convert(conversion_request)
    if result.assets:
        return [asset.data for asset in result.assets]
    else:
        raise Exception('Conversion failed: %d %s'
                % (result.error_code, result.error_text))
</code></pre>

<p>Note that the API methods recently changed (maybe in AppEngine 1.6?)
from <code>conversion.ConversionRequest</code> to <code>conversion.Conversion</code>. I should
have expected breaking changes since the conversion API is still
experimental, but it stumped me for a little while anyway.</p>

<p>The API works pretty well, but doesn't provide much meaningful feedback
if running in the dev server. So far I've only managed to overload the
conversion service a few times with very large (eg. ~100 page) PDFs.
That said, a lot of sheet music is incredibly long, especially in an
orchestral setting. So, if I was doing this for production, I would
probably need to build a custom solution.</p>

<p>One other caveat with this approach is that I have no idea how much
money I would be charged for using this service. Conversion is quite
intensive, and given the recently increased rates, I would be wary.</p>

<p>Once the images are converted, I upload them to an Amazon S3 instance
where I keep my data. To do this, I use <a href="http://aws.amazon.com/code/134">S3.py</a>, a really simple
library for interacting with Amazon S3:</p>

<pre><code>def upload_helper(self, path, data, contentType):
    conn = S3.AWSAuthConnection(AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
    options = {'x-amz-acl': 'public-read', 'Content-Type': contentType}
    response = conn.put(DEFAULT_BUCKET, path, S3.S3Object(data), options)
    return URL_FORMAT % {'bucket': DEFAULT_BUCKET, 'path': path}
</code></pre>

<p>The full code for the AppEngine server is open source on <a href="https://github.com/borismus/smusique-uploader">github</a>.</p>

<h2>Client-side scraping</h2>

<p>Unfortunately IMSLP doesn't provide a useful API out of the box. I
wasn't ambitious enough to create a full API for it, but needed some
interim solution. I figured that for a demo, it wouldn't be such a
terrible experience to have to seed the database with the repertoire you
were interested in if it was easy enough to do.</p>

<p>Ordinarily, one might write a little scraper utility in their favorite
scripting language. However, scraping HTML from python (or any other
scripting language for that matter) is really not my favorite activity.
Additionally, relying on the command line excludes the target audience
(musicians) for this application.</p>

<p>As a workaround, I came across a potentially interesting idea:
client-side scraping with a Chrome extension. Let me explain.</p>

<p>Suppose you want to scrape part of a corpus of data that's available
through a website, but want to let the user decide which parts they are
interested in. Simply use a Chrome extension that injects code into the
target page, fetches the interesting part of the DOM using selectors and
perhaps jQuery for convenience, and then uploads the data to some
server. I used <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a> for this purpose. In the manifest, the
entry looks as follows:</p>

<pre><code>"content_scripts": [{
  "matches": ["http://imslp.org/wiki/*"],
  "js": ["imslp.js"],
  "css": ["imslp.css"]
}]
</code></pre>

<p>Then, <code>imslp.js</code> does the scraping, which can happen automatically as a
user navigates through a page, or by adding extra elements to the page.
This simple IMSLP scraper creates a "send to smusique" button beside
each PDF on IMSLP:</p>

<p><img src="http://i.imgur.com/6YhZ2.png" alt="screenshot" /></p>

<p>Once clicked, <code>imslp.js</code> gets the URL to fetch, extracts all of the meta
data of the current piece via CSS selectors, and makes a cross-domain
request to the converter with all of this information.</p>

<p>This approach is much harder to prevent than traditional, server side
scraping. Although setting UserAgent restrictions is futile, many sites
use JavaScript to render their content, and serve up a content-free
page. In contrast, there's very little a content provider can do to
protect themselves from client side scraping, making this a very
powerful technique. If the content is in the DOM, it can be extracted.</p>

<p>The source of the extension is somewhat messy but available also on
<a href="https://github.com/borismus/smusique-extension">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>A mobile web application tech stack</title>
    <author><name>Boris Smus</name></author>
    <link href="/mobile-web-app-tech-stack"/>
    
    <updated>2012-01-18T09:00:00-00:00</updated>
    
    <id>http://smus.com/mobile-web-app-tech-stack</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>There are many technical decisions to make when writing web applications. I've
come back to writing modern web applications lately, and wanted to consolidate
some scattered thoughts that I’ve recorded over the course of my development
cycle. In particular, this post is about the set of frameworks that I found to
be instrumental in developing my most recent project. I'll go over some of the
most important framework types, each of which could be expanded into an article
in its own right. This is not meant to be an extensive comparison of existing
offerings, just a slice of technologies that I experimented with most recently.
<!--more--></p>

<p>Although my focus is on mobile first, I think that this set of technologies can
be applied to web apps in general. All of my decisions and data points were
made with a few requirements in mind:</p>

<ul>
<li>JavaScript only (CoffeeScript, Dart, are definitely worth a serious look, but
cause an explosion in choice which I wanted to avoid)</li>
<li>Must work well in modern browsers (iOS 5, Android 4)</li>
</ul>

<h2>Picking an MVC</h2>

<p>The model view controller pattern has been in use in native UI app development
for decades. The basic idea is to separate the data layer (storage,
communication, data) from the presentation layer (UI, animation, input). There
are other similar patterns such as MVVM (Model View ViewModel), but the main idea
is to have well-defined separation between the presentation and data layers for
cleaner code and ultimately long-term velocity:</p>

<p><img src="/mobile-web-app-tech-stack/separation.jpg" alt="separation" /></p>

<p>There are tons of offerings of JavaScript model view controller frameworks.
Some, such as <a href="http://documentcloud.github.com/backbone/">Backbone.js</a> and <a href="http://spinejs.com/">Spine.js</a> are written in pure
code, while others like <a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://angularjs.org/#/">Angular</a> rely on
DOM data attribute binding. Relying on HTML5 data DOM attributes feels wrong
for an MVC system, whose whole point is to separate view and data. This
excludes the Knockout and Angular frameworks. Spine.js is easier with
CoffeeScript, which we exclude based on my initial requirements.</p>

<p>Backbone.js has been around for longer than most (except perhaps JavaScriptMVC,
seems like a dead project), and also features a growing open source community.
For my app stack, I went with Backbone.js. For more information about picking
an MVC, check out <a href="http://addyosmani.github.com/todomvc/">TodoMVC</a>, which implements the same Todo
application using different MVC frameworks. Also see this
<a href="http://codebrief.com/2012/01/the-top-10-javascript-mvc-frameworks-reviewed/">MVC framework comparison</a>, which strongly favors the
<a href="http://emberjs.com/">Ember.js</a>, a relative newcomer to the scene. I haven’t yet had a chance
to play with it, but it’s on my list.</p>

<h2>Picking a templating engine</h2>

<p>To build a serious application on the web, you inevitably build up large DOM
trees. Rather than using JavaScript APIs to manipulate the DOM, it can be much
simpler and more efficient to write HTML using a string-based template instead.
Generally speaking JS templates have evolved to use this at-first strange
convention of embedding the template content inside script tags: <code>&lt;script
  id="my-template" type="text/my-template-language"&gt;...&lt;/script&gt;</code>. The basic
pattern of use for all template engines is to load the template as a string,
construct template parameters and then run the template and parameters through
the templating engine.</p>

<p>Backbone.js depends on <a href="http://documentcloud.github.com/underscore/">Underscore.js</a>, which ships with a somewhat
limited templating engine with verbose syntax. There are other options
available, including <a href="http://api.jquery.com/category/plugins/templates/">jQuery Templates</a>, <a href="http://handlebarsjs.com/">Handlebars.js</a>,
<a href="http://mustache.github.com/">Mustache.js</a> and many others. jQuery Templates have been deprecated
by the jQuery team, so I did not consider this option. Mustache is a
cross-language templating system, featuring simplicity and a deliberate
decision to support as little logic as possible. Indeed, the most complex
construct in Mustache is a way to iterate an array of objects. Handlebars.js
builds heavily on Mustache, adding some nice features such as template
precompilation and in-template expressions. For my purposes I didn’t need these
extra features, and chose Mustache.js as my templating platform.</p>

<p>In general, my impression is that the existing templating frameworks are quite
minimal and comparable in features, so the decision is largely a matter of
personal preference.</p>

<h2>Picking a CSS Framework</h2>

<p>CSS frameworks are essential tools that extend CSS’s feature set with
conveniences such as variables, a way to create hierarchical CSS selectors, and
some more advanced features. This essentially creates a new language: an
augmented version of CSS (let’s call it CSS++). For development ease, some
frameworks implement a JavaScript CSS++ interpreter in the browser, while other
frameworks let you monitor a CSS++ file and compile it whenever there are any
changes made. All CSS frameworks should provide command line tool to compile
CSS++ down to CSS for deployment.</p>

<p>As with templating languages, there are many choices all of which do very
similar things. My choice was motivated by personal syntax preference, and I
prefer <a href="http://sass-lang.com/">SCSS</a> because it avoids weird syntax like <code>@</code>. One drawback of SCSS is
that it doesn’t ship with a JavaScript interpreter (there is an <a href="https://github.com/bmavity/scss-js">unofficial one</a>
that I haven’t tried), but does come with a command line watcher. Other similar
CSS frameworks include <a href="http://lesscss.org/">LESS</a> and <a href="http://learnboost.github.com/stylus/">Stylus</a>.</p>

<h2>How to layout views</h2>

<p>HTML5 provides a variety of ways to layout content, and MVC frameworks provide
no opinion about which of these layout technologies to use, leaving the
sometimes difficult decision to you, the developer.</p>

<p>Generally speaking, relative positioning is appropriate for documents, but
falls apart for apps. Absolute positioning should be avoided, as should tables,
clearly. Many web developers have turned to the float property to align
elements, but this is suboptimal for building application views, since it’s not
optimized for app-like layouts, which results in many odd problems and <a href="http://stackoverflow.com/questions/8554043/what-actually-is-clearfix">infamous
clearfix hacks</a>.</p>

<p>After much experimentation with various web layout technologies over the years,
I think that a combination of fixed positioning and flexbox model is ideal for
mobile web applications. I use fixed positioning for UI elements that are fixed
on the screen (headers, sidebars, footers, etc). The flex box model is great
for laying out stacked views on the page (horizontally or vertically). It’s the
only CSS box model explicitly optimized for interface design, quite similar to
Android’s LinearLayout manager. For more information about the flexbox model,
read <a href="http://www.html5rocks.com/en/tutorials/flexbox/quick/">Paul's article</a> and note that this spec is being replaced by a
<a href="http://www.w3.org/TR/css3-flexbox/">new, non-backwards compatible version</a>.</p>

<h3>Adaptive Web Apps</h3>

<p>One final section on this matter: I’m a strong proponent of creating
device-specific user interfaces. This means re-writing parts of your view code
for different form factors. Luckily, the MVC pattern makes it relatively easy
to reuse a single model for multiple views (eg. tablet and phone).</p>

<p>Flipboard for iOS demonstrates this idea very well, giving tablet and phone
users a highly tailored experience for each device form factor.</p>

<p><img src="/mobile-web-app-tech-stack/flipboard-phone.jpg" alt="flipboard-phone" />
The phone UI is optimized for vertical swipes, allowing single hand use.</p>

<p><img src="/mobile-web-app-tech-stack/flipboard-tablet.jpg" alt="flipboard-tablet" />
Tablet UI works well for two hands holding the device on opposite sides.</p>

<h2>Input considerations</h2>

<p>On mobile, the main way users interact with your application is by touching the
screen with their fingers. This is quite different from mouse-based
interaction, since there are 9 additional points to track on the screen, which
means developers need to move away from mouse events when writing mobile apps.
In addition, mouse events on mobile have the problem of clicks being delayed by
300ms (there is a well-known <a href="http://code.google.com/mobile/articles/fast_buttons.html">touch-based workaround</a>). For more information
about using these events in mobile browsers, see <a href="http://www.html5rocks.com/en/mobile/touch.html">my touch events article</a>.</p>

<p>It’s not enough to just <code>s/mousedown/touchstart/</code> all of your event handlers.
There is a completely new set of gestures that users have come to expect on
touch devices, such as swipes to, for example, navigate through lists of
images. Though Apple has a little-known <a href="http://developer.apple.com/library/safari/#documentation/UserExperience/Reference/GestureEventClassReference/GestureEvent/GestureEvent.html#//apple_ref/doc/uid/TP40009353">gestures API</a>, there is no open spec for
doing gesture detection on the web. We really need a JavaScript library to do
gesture detection, for some of the <a href="http://www.lukew.com/touch/TouchGestureGuide.pdf">more common gestures</a>.</p>

<h2>How to make it work offline</h2>

<p>For an app to work offline, you need two things to be true:</p>

<ol>
<li>Assets are available (via AppCache, Filesystem API, etc)</li>
<li>Data is available (via LocalStorage, WebSQL, IndexedDB, etc)</li>
</ol>

<p>In practice, building offline apps on the web is a difficult problem. Generally
speaking offline functionality should be built into your app from the
beginning. It’s especially difficult to offline-ify an existing web application
without significant code rewriting. Additionally, there are often unknown
storage limits for various offline technologies, and undefined behavior for
what happens when those limits are exceeded. Finally, there are problems with
technologies in the offline technology stack, most notably AppCache, as I
outlined in a <a href="http://smus.com/game-asset-loader">previous post</a>.</p>

<p>A very interesting approach to write truly offline-capable apps is to go
“offline first”. In other words, write everything as if you have no internet
connection, and implement a syncing layer that synchronizes data when an
internet connection exists. In the Backbone.js MVC model, this can fit nicely
as a custom <code>Backbone.sync</code> adapter.</p>

<h2>Unit testing</h2>

<p>It’s hard to unit test your UI. However, since you’re using an MVC, the model
is completely isolated from the UI and as a result, easy to test. <a href="http://docs.jquery.com/QUnit">QUnit</a> is
quite a nice option, especially because it allows to unit test asynchronous
code using it’s <a href="http://docs.jquery.com/QUnit/start#decrement">start() and stop()</a> methods.</p>

<h2>Signing off</h2>

<p>To summarize, I used Backbone.js for MVC, Mustache.js for templating, SCSS for
a CSS framework, CSS Flexbox to render views, custom touch events and QUnit for
unit testing to write my mobile web application. For offline support, I’m still
experimenting with various technologies and will hopefully follow up with more
information a future post. While I strongly believe in the need for each class
of tool (eg. MVC) outlined here, I also believe that many of the specific
technologies I described here are interchangeable (eg. Handlebars and
Mustache).</p>

<p><strong>One more thing</strong>: yesterday (on January 17th, 2012), <a href="http://walmartlabs.github.com/thorax/">Thorax</a> was
announced. This is a Backbone-based set of libraries very similar in spirit to
what I describe in this post. I've yet to investigate it in any depth, but the
name is great :)</p>

<p>Use a similar set of frameworks? Have a personal favorite? Think I’m missing an
important type of framework? Let me know!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Hybrid operating systems</title>
    <author><name>Boris Smus</name></author>
    <link href="/hybrid-operating-systems"/>
    
    <updated>2011-11-28T09:00:00-00:00</updated>
    
    <id>http://smus.com/hybrid-operating-systems</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I've been thinking a bit about native and web applications, and how they can
(or can't) coexist in the future. This topic has been steeping in my head for
months now, so here is a brain dump of some of my thoughts.<!--more--></p>

<h2>On hybrid apps</h2>

<p>Hybrid apps embed a browser, using a web view for part of their UI.
This is a very flexible definition for a very flexible beast. At one
extreme are native apps that embed a Web View to render a small widget
(written in HTML/CSS/JS) in the UI. On the other hand, you could have a
web application that provides just a native frame around the content
which is entirely implemented as a web app. The hybrid app spectrum
looks something like this:</p>

<pre><code>native &lt;--------------- hybrid apps ---------------&gt; web
Mail.app                GMail for iOS              GMail
</code></pre>

<p>Pieces of the web stack have long been useful as building blocks for
applications, but lately there has been a bloom in features under the
HTML5 moniker. As a result, many developers are gravitating towards the
right end of the hybrid app spectrum.</p>

<p>There are several frameworks which target this niche by providing thin native
frames, such as <a href="http://phonegap.com/">PhoneGap</a> and <a href="http://fluidapp.com/">Fluid</a>. There are three benefits to these
frameworks, helping developers to:</p>

<ol>
<li>Make money: capitalize on AppStore and Market earnings.</li>
<li>Use more features: get access to features not available from the web.</li>
<li>Provide good platform-specific native integration.</li>
</ol>

<p>The first two are relatively well understood, so for the purposes of this post,
I'm more interested in the third. Android PhoneGap apps are launched from the
home screen, Fluid Mac apps from spotlight or your dock, etc. Then you can
switch between these apps as if they were regular OS X applications.</p>

<p>Even though the browser is a single app, it runs tons of applications
like GMail, twitter, etc. The browser allows people to do two
fundamentally different things: view content on the web (sites), and <em>do
things</em>, like listen to music, play games and create documents (apps).
For a deeper discussion on the distinctions between web apps and web
sites, check out James Pearce's article <a href="http://tripleodeon.com/2011/09/of-sites-and-apps/">Of Sites and Apps</a>.</p>

<p>A pure web application does not rely on platform-specific native code
and runs inside the browser, but still has to rely on a native frame
solution to reap the native integration benefit frameworks like PhoneGap
and Fluid provide.</p>

<h2>Hybrid operating systems</h2>

<p>Some browsers today have an explicit notion of apps. Without the proper OS
integration, this is confusing. Now your device (laptop, tablet, phone, tv) has
apps, one of which is a browser, and the browser has apps too. So to launch an
app, users have to open the browser, and then launch what they want. This
reinforces the distinction between web and native apps in people's heads and
makes for a very inelegant solution.</p>

<p>One approach is for the OS to make the browser special in the operating system,
allowing it also to manage installed web applications. Another approach is for
operating systems to implement their SDKs using HTML, CSS and JavaScript, but
with non-standard APIs specific to the platform, such as WebOS. The extreme of
this approach is Chrome OS, where browser and OS are indistinguishable.</p>

<p>Like apps, operating systems vary in how much they embrace web applications.
Here is an interesting spectrum to think about:</p>

<pre><code>web agnostic &lt;------------ hybrid OS ------------&gt; apps are webapps
iOS           Windows 8              Web OS               Chrome OS
</code></pre>

<p>Let me define what I mean by "hybrid OS". A hybrid OS is aware of the
presence of not just native, but also web applications, and provides
ways of managing web apps, possibly alongside native ones. Windows 8 is an
example of a hybrid OS, employing the new Metro UI, while also supporting
Windows 7-style UI. Palm's Web OS, has no native mode at all (except via
<a href="https://developer.palm.com/content/api/dev-guide/pdk/overview.html">PDK plugins</a>), and all apps are web apps (though they require the use of a
special set of JavaScript libraries).</p>

<p>Web apps today are actively used, in some cases, even <a href="http://www.readwriteweb.com/archives/financial_times_proves_html5_can_beat_native_mobil.php">surpassing</a> native
apps in popularity. Unfortunately, users of web apps are stuck in their browser
which is confusing and limiting, given the current landscape of native
applications. Hybrid operating systems make explicit the idea that in the end,
web apps are just apps.</p>

<h2>Future</h2>

<blockquote>
  <p>The second goal of PhoneGap is for the project to cease to exist. This is not
  a nihilistic sentiment... -- <a href="http://wiki.phonegap.com/w/page/46311152/apache-callback-proposal">Apache Callback Proposal</a></p>
</blockquote>

<p>My goal is for web apps to become compelling enough to force OS creators to
hybridize their platforms. In other words, I'd like to see rightward movement
in both the app and OS spectrums. As more hybrid operating systems emerge, we
get closer to PhoneGap's second goal.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Stack Exchange question notifier</title>
    <author><name>Boris Smus</name></author>
    <link href="/stack-exchange-question-notifier"/>
    
    <updated>2011-11-15T09:00:00-00:00</updated>
    
    <id>http://smus.com/stack-exchange-question-notifier</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I've been a member of the Chrome developer relations team for the last 9
months, where one aspect of my job is developer support. I actively
monitor the chromium-extensions@chromium.org list, answering questions
about developing Chrome extensions. Thankfully, there are many others that
help me in this endeavor, both from the Chrome team and from within the
developer community itself, notably a certain <a href="http://goo.gl/di3kR">PhistucK</a>, the current
record holder for the number of messages posted to the list.</p>

<p>Over the last years, another channel of questions has emerged:
<a href="http://stackoverflow.com/">Stack Overflow (SO)</a>. SO is a great Q&amp;A community for
software developers, providing many advantages over ordinary discussion
groups. Google has recognized this and sponsored SO tags,
such as <a href="http://stackoverflow.com/questions/tagged/google-chrome">google-chrome</a>, and <a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a>. As I
outlined in an mail to the group, we are now including SO as
an official Chrome extension developer support channel.</p>

<!--more-->

<p>As part of optimizing the support workflow, I wrote a Chrome extension
which monitors questions with certain tags on SO and other Stack
Exchange sites. This extension implements a browser action which gets
badged with the number of unreviewed questions.</p>

<p>The extension UI uses the same visual style as other Google apps, and
blends well into Chrome. I think it works reasonably well from a design
perspective, both in the options page and in a browser action:</p>

<p><img src="/stack-exchange-question-notifier/stack-screenshot.png" alt="screenshot" /></p>

<p>If you're curious to see how the extension is implemented, the source
<a href="https://github.com/borismus/Question-Monitor-for-Stack-Exchange">is available</a> free of charge! This extension was implemented in
<a href="http://code.google.com/closure/">Closure-style</a> JavaScript.</p>

<p>If you support developers on Stack Overflow, please <a href="https://chrome.google.com/webstore/detail/bnnkhapbhkejookmhgpgaikfdoegkmdp">install it</a>,
give me constructive feedback and consider monitoring the
<a href="http://stackoverflow.com/questions/tagged/google-chrome-extension">google-chrome-extension</a> tag :)</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2011 greatest hits</title>
    <author><name>Boris Smus</name></author>
    <link href="/uist-2011"/>
    
    <updated>2011-10-20T09:00:00-00:00</updated>
    
    <id>http://smus.com/uist-2011</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I went to UIST 2011 in Santa Barbara and presented our research on
<a href="http://crowdforge.com">CrowdForge</a>.</p>

<p>Here's a sample of some of the great work that was presented this year, in the
3 research areas that interest me most: crowdsourcing/human computation,
mobile physical computing, and music.<!--more--></p>

<h1>Crowdsourcing</h1>

<p>Notable work in both workflow-oriented approaches (Jabberwocky, CrowdForge,
PlateMate) and synchronous collaborative approaches (Crowds in seconds,
Collabode, Real-time).</p>

<h3>PlateMate</h3>

<ul>
<li>Presents a workflow-based crowdsourcing nutritional analysis from food
photographs.</li>
<li>Implemented in Django/Python, same as CrowdForge</li>
<li>Found out about CrowdForge 90% of the way into the research.</li>
</ul>

<p><img src="http://crowdresearch.org/blog/wp-content/uploads/2011/09/dinner.jpg" alt="platemate" /></p>

<h3>Real-time crowd control</h3>

<ul>
<li>Nice research from Jeff Bigham's group.</li>
<li>Compares different strategies of merging input from different users.</li>
<li>Reminded me of my <a href="/android-powered-mindstorms">twitter mindstorms project</a>.</li>
</ul>

<iframe width="560" height="315" src="http://www.youtube.com/embed/P_Tqn-3BF_I" frameborder="0" allowfullscreen></iframe>

<h3>Crowds in two seconds</h3>

<ul>
<li>Great research from MIT crowdsourcing folks (Michael Bernstein, Rob Miller)</li>
<li>Retainer: keep workers "on tap" - ready to work by paying $0.30/hour.</li>
<li>Rapid refinement: crowd algorithm to narrow search space to accelerate.</li>
<li>Cool application: take movies, crowdsource the best moment.</li>
</ul>

<iframe width="560" height="315" src="http://www.youtube.com/embed/9IICXFUP6MM" frameborder="0" allowfullscreen></iframe>

<h3>The Jabberwocky programming environment</h3>

<ul>
<li>Really ambitious MTurk like project.</li>
<li>Combine different worker types from different spheres (social groups, paid workers, machines)</li>
<li>Full runtime stack (Dog high level language, ManReduce low level language, Dormouse runtime)</li>
<li>Great potential to mix different spheres of workers, but a rather intimidating project... scope creep!</li>
</ul>

<p>More in the <a href="http://kamvar.org/assets/papers/jabberwocky.pdf">Jabberwocky</a> paper.</p>

<h3>Real-time collaborative coding in a web IDE</h3>

<ul>
<li>More awesome work from MIT (Max Goldman, Greg Little, Rob Miller)</li>
<li>Web-based Java EtherPad editor for collaboration</li>
<li>Main problem: others leave code in semi-working state. When to sync?</li>
<li>Idea: automatic error-aware integration. Auto-sync code when it compiles (or - tests pass)</li>
</ul>

<p>More on the <a href="http://groups.csail.mit.edu/uid/collabode/">Collabode</a> site.</p>

<h1>Mobile physical computing</h1>

<p>Interesting work in mobile virtual and projected UI spaces.</p>

<h3>PocketTouch</h3>

<ul>
<li>Great work from Microsoft Research to let you use your phone through fabric (while in the pocket)</li>
<li>Since orientation in-pocket is unclear, there's an orientation setting gesture (not sold on this)</li>
<li>Unfortunately typical front pocket jeans work rather poorly.</li>
</ul>

<iframe width="560" height="315" src="http://www.youtube.com/embed/fHSDpE0kTag" frameborder="0" allowfullscreen></iframe>

<h3>Multi-user Interaction with Handheld Projectors</h3>

<ul>
<li>Most projector-based systems require a fixed place. This one is fully mobile.</li>
<li>System produces visible projection (for image) and invisible projection (for data)</li>
<li>Camera tracks location of nearby projections</li>
<li>Cool applications: virtual boxing, content transfer, 3D model viewing.</li>
</ul>

<p><img src="http://www.disneyresearch.com/_images/459-00B516CF.jpg" alt="concept" /></p>

<p>To get a better sense of the project, take a look at <a href="http://www.disneyresearch.com/research/projects/hci_sidebyside_drp.htm">this video</a>.</p>

<h3>Imaginary phone</h3>

<ul>
<li>Lets you control a mobile phone device without taking it out of your pocket.</li>
<li>Maps iPhone controls to the palm of your hand similar to <a href="http://en.wikipedia.org/wiki/Guidonian_hand">Guidonian hand</a></li>
<li>Touchscreen progression: fingers replaced styli. Next step: palm replaces phone (bit of a stretch)</li>
</ul>

<iframe width="560" height="315" src="http://www.youtube.com/embed/aCARtauIS50" frameborder="0" allowfullscreen></iframe>

<h3>OmniTouch</h3>

<ul>
<li>Shoulder mounted depth camera/projector.</li>
<li>Works for all sorts of surfaces. Heavy math for tracking and projecting.</li>
<li>Tracks "hover" (haven't seen this notion before) and "touch" states.</li>
</ul>

<iframe width="560" height="315" src="http://www.youtube.com/embed/Pz17lbjOFn8" frameborder="0" allowfullscreen></iframe>

<h1>Music</h1>

<p>I had the pleasure of hearing <a href="https://ccrma.stanford.edu/~ge/">Ge Wang</a> speak
about some of his older projects, including <a href="http://www.youtube.com/watch?v=RhCJq7EAJJA">Ocarina</a>,
<a href="http://www.youtube.com/watch?v=KetWJi0zou0">Leaf Trombone</a>, the Stanford Mobile orchestra, and others. Notably I
hadn't really seen the <a href="http://chuck.cs.princeton.edu/doc/language/">ChucK language in action</a>, and would be
interested in seeing if the <a href="http://www.html5rocks.com/en/tutorials/webaudio/intro/">Web Audio API</a> could support this sort of thing.
Browser-based ChucK, anyone?</p>

<p>The last piece of research that I really enjoyed was called "onNote: Playing
printed music scores as a musical instrument". The idea here was to use OMR
techniques for markerless tracking of sheet music using positioning and using a
finger for pointing. The other neat application here was to support
compositional remixing by literally cutting up sheet music and splicing it back
together.</p>

<iframe width="420" height="315" src="http://www.youtube.com/embed/fGOk16Cnq7c" frameborder="0" allowfullscreen></iframe>

<p>This was my first UIST, and though I'm unlikely to have new research to submit
for UIST 2012, I'm seriously considering going anyway, just to stay on top of
the great work that this vibrant community generates.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Screen video capture for Chrome OS</title>
    <author><name>Boris Smus</name></author>
    <link href="/screen-capture-for-chrome-os"/>
    
    <updated>2011-10-04T09:00:00-00:00</updated>
    
    <id>http://smus.com/screen-capture-for-chrome-os</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>This post is about video capture in Chrome that doesn't rely on any
external dependencies like Flash (no fun), NPAPI (not supported on
Chrome OS) and Native Client (not <em>yet</em> supported on Chrome OS).</p>

<p>I take screenshots all the time for bug reporting, image editing, etc.
On OS X, this functionality is conveniently built in, and available
through <code>Command</code> - <code>Shift</code> - <code>4</code>. As a web denizen, I find it very
useful to auto-upload these captures to a remote server, so I wrote this
<a href="https://github.com/borismus/screencapture-www">minimal image uploader</a> which replaces the default behavior
on OS X to capture the screenshot, and also uploads it to a picture
hosting service.</p>

<p>Taking video capture of various UIs is also immensely useful for showing
demonstrations, complex interactions, and subtle bugs. I recently
re-discovered that QuickTime on OS X comes with this functionality built
in. Prior to that I used (paid) ScreenFlow, which also has very nice
dimension cropping and time dilation features.</p>

<p>What if we're on a web-only device, such as a Chromebook running Chrome
OS? There is a still screenshotting API, but capturing video is less
trivial. I've released an extension that captures and play backs video
captures inside Chrome, and also lets you share stills to Picasa (using
the OAuth 2 <a href="http://smus.com/oauth2-chrome-extensions">extension library</a>). It's available on the
<a href="https://chrome.google.com/webstore/detail/omahgjnmfgeeeoekegajhndkncocoofd">webstore</a>, and the source is on <a href="https://github.com/borismus/chrome-screencast">github</a>. Read on to
learn how it works, and see how you can help.</p>

<h1>Screenshots in Chrome</h1>

<p>Chrome provides the <a href="http://code.google.com/chrome/extensions/tabs.html#method-captureVisibleTab"><code>captureVisibleTab</code></a> extension API for taking
a screenshot of a tab. It requires host permissions on the page, but as
usual the <all_urls> permission will enable the API across all pages
(with some exceptions). A few successful extensions, such as
<a href="http://awesomescreenshot.com/">Awesome Screenshot</a>, use this API and allow
cropping, annotation and sharing of screen grabs.</p>

<p>What if you want to capture video of a tab? Chrome provides no
pre-existing API for this purpose, however, we can piggyback on the
still screenshot API, executing it repeatedly from the background page
for every frame we want to capture:</p>

<pre><code>var images = [];
var FPS = 30;
var QUALITY = 50;
timer = setInterval(function() {
  chrome.tabs.captureVisibleTab(null, {quality: QUALITY},
    function(img) {
      images.push(img);
    });
}, 1000 / FPS);
</code></pre>

<p>As we capture, we store the base64-encoded strings representing video
frames in an array. Once we're done capturing, we can simulate video
playback by rapidly swapping the images in and out:</p>

<pre><code>var background = chrome.extension.getBackgroundPage();
timer = setInterval(function() {
  if (currentIndex &gt;= images.length - 1) {
    pause();
    return;
  }
  setIndex(currentIndex + 1);
  updateSliderPosition();
}, 1000 / background.FPS);
</code></pre>

<p>This approach turns out to be surprisingly efficient, with the extension
being able to capture at 30 FPS on a MacBook Air, and 10 FPS on a
Chromebook without too much noticeable slowdown.</p>

<p>Note that we rely on a fixed FPS for ease of implementation, however one
could imagine using <code>requestAnimationFrame</code> and tracking the variable
frame rate so that the playback speed is reasonable. However, there are
definitely precision issues with JavaScript's timers, so this is a much
more challenging approach.</p>

<p>So we can capture and playback videos inside the browser, but getting it
out of the browser is another matter entirely. As a temporary measure,
my colleague <a href="http://greenido.wordpress.com">Ido Green</a> built a screen stitching service which
encodes multiple images into a movie using ffmpeg. Ideally, of course,
we would encode in the browser. Perhaps a JavaScript video encoder could
be implemented, though the performance may be too poor for practical
use. Alternatively, a ffmpeg Native Client-based approach might be
suitable, especially given that ffmpeg has <a href="http://code.google.com/p/naclports/source/browse/trunk/src/libraries/ffmpeg-0.5/">already been ported</a>.</p>

<h1>Free ideas</h1>

<p>There are a few logical next steps for this sample. As already
mentioned, encoding video in the browser is a top priority, but there
are a slew of other interesting directions, some of which can be seen as
features, and others as separate products.</p>

<ul>
<li><p>The <code>captureVisibleTab</code> API doesn't track the mouse cursor. This could
be done by injecting an overlay onto the current page and tracking
mousemove and click events. This data could then either be drawn onto
a canvas context, or encoded separately as <code>mouseData</code>, and then drawn
with JavaScript at playback time.</p></li>
<li><p>Cropping the video dimensions, modifying the video time schedule
(speedup, slowdown, truncation) and annotation are all desired
video-editing class features that could be implemented by treating
images as canvases.</p></li>
<li><p>A compelling use case for this technology is creating screen sharing
sessions for demos and presentations. Thus, it would be very useful to
stream the video to a server, and broadcast it to multiple clients in
real time. <a href="http://updates.html5rocks.com/2011/08/What-s-different-in-the-new-WebSocket-protocol">Binary websockets</a> are now available in Chrome,
and this could be a great application.</p></li>
<li><p>Audio annotations on screen captures make perfect sense, and are
widely supported by desktop applications. APIs for sound capture have
been a <a href="http://www.w3.org/2009/dap/">long time coming</a>, but finally we may have an answer
via the <a href="http://www.webrtc.org/">WebRTC</a> ecosystem, and the <code>getUserMedia</code> call.</p></li>
</ul>

<p>By the way, I've switched to exclusively using Markdown for all of my
published writing, and wrote an <a href="https://github.com/borismus/markdown-preview">markdown preview</a> for Chrome
to make my life a bit easier.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Loading large assets in modern HTML5 games</title>
    <author><name>Boris Smus</name></author>
    <link href="/game-asset-loader"/>
    
    <updated>2011-09-22T09:00:00-00:00</updated>
    
    <id>http://smus.com/game-asset-loader</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>HTML5 games are here today, and rapidly increasing in complexity. Impressive
<a href="http://madebyevan.com/webgl-water/">demos</a> are <a href="http://www.chromeexperiments.com/">everywhere</a>, and prominent titles like <a href="https://chrome.google.com/webstore/detail/ciamkmigckbgfajcieiflmkedohjjohh">Gun
Bros</a> and <a href="http://chrome.angrybirds.com">Angry Birds</a> prove that it's possible to
create competitive gaming experiences in the browser. Games like these are
possible thanks largely to the modern web stack which includes WebGL, the Web
Audio API, Web Sockets and others.</p>

<p>Often forgotten, however, is the less sexy story of loading game assets. As the
web platform progresses and allows for increasingly complex games, game assets
(ex. textures, movies, music and images) grow in size and number, and asset
management becomes a sticking point for game developers.</p>

<p>Let me share with you some truths:</p>

<ol>
<li>Modern games require gigabytes of assets (textures, movies, etc)</li>
<li>Gamers don't like waiting for their game to load</li>
<li>Browser gamers want to be able to play regardless of internet connectivity</li>
</ol>

<p>"But wait," you say, "I know! Just use the <a href="http://diveintohtml5.org/offline.html">Application Cache</a>
and yer done!". Not so fast, dear reader... As described below, there are
problems with this approach, and I propose some solutions.</p>

<h1>Problems with Application Cache</h1>

<p>So you've started implementing your awesome asset loading solution using
AppCache. The good news is that there are some useful tools to help you debug
if you have taken this difficult route:</p>

<ol>
<li>You can get basic information about the site's app cache through the
Developer Tools' <a href="http://code.google.com/chrome/devtools/docs/resources.html">resource panel</a>.</li>
<li>You can view (and remove!) caches stored in Chrome by navigating to
<code>chrome://appcache-internals/</code>.</li>
</ol>

<p>But let me be blunt: <strong>AppCache is annoying to deal with</strong>. If you've made a
small error in your cache manifest file, you'll quickly hit a brick
wall. I ran into an issue where I forgot to include a <code>NETWORK:</code>
fallback clause, and wasted hours trying to figure out why all of my
XHRs were responding with status 0.</p>

<p>Part of what makes AppCache difficult to debug is its very <strong>limited
JavaScript API</strong>. Aside from letting you inspect the status of the entire cache
with <code>window.applicationCache</code> and the <code>updateready</code> event, AppCache doesn't
give us much to work with. There's no way to tell if a particular resource
we're dealing with is cached or not and no programmatic way of clearing the
cache.</p>

<p>AppCache takes a fully transactional approach to asset loading.  Either
the cache is fully loaded, or fully unloaded. Compounding this issue,
it's impossible to resume the download of an AppCache. Thus, if you have
a large amount of assets, your user will have to <strong>wait a long time for
everything to be loaded</strong>, and if they reload, they will need to restart
their cache download.</p>

<p>Lastly, you can only include one cache manifest per page, making it
<strong>impossible to group assets</strong> into multiple bundles. There are hacks that
use multiple iframes with different cache manifests to work around this
limitation (used in <a href="http://chrome.angrybirds.com">Angry Birds</a>), but these are ugly!</p>

<p>Ultimately, what we need is a well-thought-out Application Cache
enhancement or replacement. Given how quickly web standards bodies move,
I've started thinking a bit about a transitional solution.</p>

<h1>Designing a game asset loader</h1>

<p>An ideal asset loading solution requires some of these features:</p>

<ol>
<li>Granular asset loading. Load all, in groups, or individually.</li>
<li>No asset size limits.</li>
<li>Offline capability.</li>
<li>Programatic control over assets.</li>
</ol>

<p>It makes sense to group assets in bundles and let the loader take care
of the details. We can even create a custom manifest format, for
example, in JSON format:</p>

<pre><code>{
  "assetRoot": "./media/",   // The root of the assets.
  "bundles": [{
    "name": "core",          // A bundle definition.
    "contents": [            // The contents within.
      "theme.mp3",
      "loading.jpg"
    ]
  }, {
    "name": "level1",        // Multiple bundles defined.
    "contents": [            // Note: order implicit since bundles
      "L1/background.jpg",   // objects are stored in an array.
      "L1/blip.wav"
    ]
  }, {
    "name": "level2",
    "contents": [
      "L2/intro.mov"
    ]
  }],
  "autoDownload": false      // If true, download all in order.
}
</code></pre>

<p>With this manifest format in mind, sample API usage might look like
this:</p>

<pre><code>// Load the asset library.
var gal = new GameAssetLoader('/path/to/gal.manifest');

// Read the manifest and other good stuff.
gal.init(function() {
  // When ready, download the bundle named 'core'.
  gal.download('core');
});

// When the core assets are loaded.
gal.onLoaded('core', function(result) {
  if (result.success) {
    // Show a loading indicator.
    document.querySelector('img').src = gal.get('loading.jpg');
  }
});

// Check the progress of the download.
gal.onProgress('core', function(status) {
  console.log('status:', status.current/status.total, '%');
});
</code></pre>

<p>Note that although I've been using the name Game Asset Loader, this
approach can be used for loading any large non-game assets, such as for
example, a video or photo gallery.</p>

<h1>Implementation details</h1>

<p>Luckily, the modern web stack enables us to create a custom solution to
address all of these requirements. By leveraging technologies such as
the HTML5 Filesystem API or Indexed DB, we have programmatic access to
a storage mechanism that we can use to build an asset loader described
here.</p>

<p>I used the <a href="http://www.html5rocks.com/en/tutorials/file/filesystem/">Filesystem API</a> to implement a version of the asset
loader. The code requests a large amount of persistent storage using the 
<a href="https://groups.google.com/a/chromium.org/group/chromium-html5/msg/5261d24266ba4366?dmode=source">Quota API</a>, which is undocumented, but works anyway:</p>

<pre><code>// Get quota.
storageInfo.requestQuota(window.PERSISTENT, quota,
  onQuotaGranted, onError);

// Callback when the quota API has granted quota
function onQuotaGranted = function(grantedBytes) {
  // Save grantedBytes in the adapter
  that.grantedBytes = grantedBytes;
  // Once quota is grantedBytes, initialize a filesystem
  requestFileSystem(window.PERSISTENT, grantedBytes, onInitFS, onError);
};

// Callback when the filesystem API has created a filesystem.
function onInitFS = function(fs) {
  // Create a directory for the root of the assets.
  fs.root.getDirectory(ROOT_DIR, {create: true}, function(dirEntry) {
    that.root = dirEntry;
  }, onError);
};
</code></pre>

<p>The approach fetches assets with <code>XMLHttpRequest</code>, and stores them in the
filesystem. All files in the filesystem are accessible via the <code>filesystem://</code>
schema, and can be used as any other resource. This filesystem URL is returned
by the library in the <code>get(path)</code> call.</p>

<p>Note that the writable HTML5 filesystem API is currently available in Chrome
only, but that it's quite possible to use IndexedDB (supported in Firefox and
IE10) as the data store.</p>

<h1>Usage scenarios</h1>

<p>The following section briefly describes what the game asset loader (GAL) does
in several scenarios.</p>

<p>Player goes to game.com which uses the game asset loader. The game calls
<code>gal.download('core')</code> to download core assets and
<code>gal.download('level1')</code> to load the first level into the player’s
filesystem. While the core bundle loads, the game displays a loading
indicator. Once core is loaded, the game displays the main menu. As soon
as the first level is loaded, the "Play now" button is enabled. As the
player plays, the GAL downloads more of the levels in the background.</p>

<p>Next time, the player tries playing offline. He goes to game.com, whose
code is cached via AppCache, and loads GAL again. This time GAL knows it’s
offline, looks up its manifest stored on the filesystem and doesn’t try to
download new assets. The old assets still work though.</p>

<p>Player is still offline, making good progress, and beats level 5, but
there are no assets downloaded for level 6. Luckily, before starting
each level, the game calls <code>gal.download('levelBundle')</code> to make sure
that the contents of that bundle are downloaded. The callback returns an
error and the game displays an error telling the player that he needs to
be online to download the next level.</p>

<p>So the player goes online and tries again. GAL re-downloads a manifest.
Next, GAL tries re-downloading every asset that the JS requests. Luckily most
of these assets are still in the browser cache, and won't be re-downloaded. The
loader then saves all of the assets in the filesystem, clobbering old files
indiscriminately. (This is bad, and needs to be fixed. Read on!)</p>

<h1>Future work</h1>

<p>In particular, re-downloading every asset while online is not desirable
behavior, and we can't always rely on the browser cache for this. For
smaller files, we can probably rely on ETag and Last-Modified headers
and hopefully the browser won't re-download the files. However, the
<strong>asset loader will still overwrite the asset in the filesystem, even if
it's unmodified</strong>. This needs to be fixed. Large files are not likely to be
cached by the browser, so we will need more intelligent <strong>caching built into
the asset loader itself</strong>.</p>

<p>There are other edge cases that need to be considered, such as what happens
when an <strong>asset is removed from a manifest</strong>. Ideally if this occurs, it
<strong>should also be removed from the filesystem</strong>, but this is not currently
implemented.</p>

<p>I'm happy to release the <a href="https://github.com/borismus/game-asset-loader">source</a> under the permissive Apache 2
license and provide <a href="https://github.com/borismus/game-asset-loader/blob/master/tests/tests.js">unit tests</a> and a <a href="https://github.com/borismus/game-asset-loader/tree/master/tests/game">sample</a> project
for your perusal. It's well documented and should be reasonably easy to
understand. I've also made provisions to separate the core library
interface from the Filesystem-based implementation, making it even
easier to implement an Indexed DB adapter.</p>

<p>Before I go, let me reiterate that this library isn't quite production ready,
but a step in the right direction for facilitating real games on the web.
Please comment below if you have feedback on the idea, or are using the
library to write a game of your own!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Developing multiplayer HTML5 games with node.js</title>
    <author><name>Boris Smus</name></author>
    <link href="/multiplayer-html5-games-with-node"/>
    
    <updated>2011-08-30T09:00:00-00:00</updated>
    
    <id>http://smus.com/multiplayer-html5-games-with-node</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>One day I had some friends over at my house introducing me some cool iPad
games. One of the games was Osmos, developed by an Canadian indie studio called
<a href="http://www.hemispheregames.com">Hemisphere Games</a>. You control a little blob that floats in 2D space, and
the only thing your blob can do is shoot pieces of itself in a given
direction, which propels it in the opposite direction. The rules of the game
are simple, the main rule being that when two blobs collide, the larger one
will consume the smaller one. The rest of the rules pretty much follow directly
from conservation of mass and energy. <a href="http://www.youtube.com/watch?v=pso6UBicLWU">See for yourself</a> - it's way
better than it sounds!</p>

<p>Osmos really caught my attention because of its simple but engaging gameplay,
meditative pace and distinct <strong>lack of multiplayer support</strong>, which struck me
as a potentially very interesting problem to tackle. And so, Osmus (mu for
multiplayer) was born as a browser-based multiplayer Osmos clone.</p>

<h2>How it works</h2>

<p>When a browser navigates to the osmus landing page, the server sends the new
client the current state of its universe, which is composed of blobs with
randomized velocities. At this point, the client can passively watch the game
progress, but of course, can also join the game as a player controlled blob.
Once a player joins, he can click or tap (on mobile devices) the canvas to
shoot off a new blob.</p>

<p>As the game progresses, the server decides when someone (possibly one of the
autonomous blobs) is victorious, at which point, players are notified and the
game is restarted.</p>

<p>The rest of this post is about some development-related details. I've had to take the game offline because it was too expensive for me to keep running on my VPS.
However I did record a video:</p>

<iframe width="640" height="360" src="http://www.youtube.com/embed/NiPZK3g_i1M" frameborder="0" allowfullscreen></iframe>

<h2>Game architecture</h2>

<p>I wrote osmus to be split into distinct, loosely coupled components both to
make the codebase more approachable for other contributors, and to make it easy
to experiment with interchangeable technologies.</p>

<p><img src="/multiplayer-html5-games-with-node/osmus-architecture.png" alt="architecture" /></p>

<p>Osmus uses a shared game engine that runs in both the browser and on
the server. The engine is a simple state machine whose primary function
is to compute the next game state as a function of time using the rules of
physics defined within.</p>

<pre><code>Game.prototype.computeState = function(delta) {
  var newState = {};
  // Compute a bunch of stuff based on this.state
  return newState;
}
</code></pre>

<p>This is a pretty narrow definition of a <em>Game Engine</em>. In the game developer
world, what's typically meant by a game engine may include anything from a
renderer, sound player, networking layer, <a href="http://en.wikipedia.org/wiki/Game_engine">etc</a>. In this case, I've
made very clear divisions between these components, and the osmus game core
only includes just the physical state machine, so that both client and server
can compute the next states and be reasonably synchronized in time.</p>

<p>The client is composed of three main components: a renderer, input
manager and sound manager. I built a very simple canvas-based renderer
that draws blobs as red circles, and player blobs are green ones. My
colleague <a href="http://twitter.com/kurrik">Arne Roomann-Kurrik</a> wrote an alternative
<a href="https://github.com/mrdoob/three.js/">three.js</a> based renderer with some epic shaders and shadows.</p>

<p>The sound manager handles playback of both sound effects and background music
(taken from <a href="http://feryl.bandcamp.com/album/8-bit-magic-a-module-chiptune-collection">8-bit Magic</a>). The current implementation uses audio tags,
with two <code>&lt;audio&gt;</code> elements, one for the background music channel, and one for
the sound effects. There are known limitations of this approach, but given the
modularity of my implementation, the sound implementation can be swapped out
for one that uses Chrome's <a href="https://dvcs.w3.org/hg/audio/raw-file/tip/webaudio/specification.html">Web Audio API</a>, for example.</p>

<p>Finally, the input manager handles mouse events, but can be replaced with one
that uses touch instead, for a mobile version. In the mobile context, it will
likely make sense to use CSS3 transformations instead of canvas, since CSS3 is
hardware accelerated on iOS, while HTML5 canvas still isn't, and WebGL is not
implemented.</p>

<p>Speaking of mobile, I was happily surprised that osmus works pretty well on
iPad, especially on an iPad 2 running the latest iOS version. This is really
great, and one of the tangible benefits of writing games for the open web.</p>

<h2>Networking is hard</h2>

<p>From a networking perspective, a game is a rather ambitious project that
requires seamless real-time synchronization between clients. Because of this,
bidirectional client-server communication is essential. In the modern web
stack, this is provided by <a href="http://dev.w3.org/html5/websockets/">Web Sockets</a> which supply a thin layer
above TCP and hide a lot of gory details from the implementer. To further hide
network stack details, I use the <a href="http://socket.io/">socket.io</a> library, which provides a dead
simple event-driven abstraction for the whole thing. Unfortunately there's
currently no support for binary data, which would greatly compress message
size, perhaps by an order of two magnitudes in the case of osmos.</p>

<p>From a bit of research which included this <a href="http://www.youtube.com/watch?v=zj1qTrpuXJ8">nice talk from Rob Hawkes</a>, it
became clear that to have any sort of shared experience, the simplest model is
to have the true game state on the server, and have clients periodically sync
with it. The main trade off here is synchronization quality vs. network
traffic required.</p>

<p>On one extreme, a game can be written by having the game logic entirely on the
server and sending updates (or perhaps even <a href="http://www.onlive.com/">simply screenshots</a>) to
the client at 60 FPS, but this is generally not feasible due to the sheer
amount of bandwidth required for this model. On the opposite extreme, you can
imagine a network architecture in which clients connect, get initial state, and
are then largely autonomous.</p>

<p>In practice, there is a happy medium in which many multiplayer games fall,
which means replicating non-trivial code in both the client and the server.
Luckily, now that we're in the ubiquitous JavaScript era, there is no longer a
need to duplicate functionality, but can instead share code by writing the game
engine in JavaScript, and then running it in both a browser on the client, and
in <a href="http://nodejs.org/">node.js</a> on the server.</p>

<p>There's a lot more to be written about writing the multiplayer bits of osmus,
which will hopefully turn into a more detailed article at some point in the
future.</p>

<h2>Shared JS modules</h2>

<p>As mentioned earlier, osmus uses a physics engine that's shared between clients
and the server. One might imagine that sharing JavaScript code between the two
would be a breeze, but it's not that simple.</p>

<p>Module loaders are a mess. There's the <a href="http://www.commonjs.org/">CommonJS spec</a>,
<a href="http://requirejs.org/">RequireJS library</a> and node.js require system, none of which play
nicely together. If you want to share code between client and server (one of
the big wins of JS on the server) without a module loader, you can use this
somewhat hacky pattern:</p>

<pre><code>(function(exports) {

var MyClass = function() { /* ... */ };
var myObject = {};

exports.MyClass = MyClass;
exports.myObject = MyObject;

})(typeof global === "undefined" ? window : exports);
</code></pre>

<p>This hack relies on the fact that node.js defines a <code>global</code> object while the
browser does not. With the hack, node.js <code>require()</code> will be happy, and you can
also include the file in a <code>&lt;script&gt;</code> tag without polluting your namespace,
assuming of course, that no other JS pollutes your namespace with a
<code>window.global</code> object!</p>

<p>Unfortunately this approach only works well for one shared module. As soon as
you have multiple modules depending on each other (via <code>require</code>s in node-land,
and globals in browser-land), the difference between node's namespacing and
browser's inclusion becomes painfully apparent and requires more hacky
workarounds.</p>

<p>Another approach is to use <a href="http://substack.net/posts/24ab8c/browserify-browser-side-require-for-your-node-js">browserify</a> to bundle all JS and emulate
requires in the browser. This approach relies on node.js to serve the
generated JS, which is not ideal, since static files should be served by a
<a href="http://nginx.net/">webserver</a> optimized for the purpose. However node.js + browserify can
be configured to compile JS that can be served statically without relying on
node to serve it. This approach introduces some overhead:</p>

<ol>
<li>Extra build step for deploying.</li>
<li>Performance overhead of whatever mechanism browserify uses to support
<code>require()</code> calls.</li>
</ol>

<p>Overall this approach sounds better to me, and I hope to try it out in a future
version of osmus.</p>

<h2>Your turn</h2>

<p>Today I'm releasing <a href="http://o.smus.com/">osmus</a> as a completely open source HTML5 game. Feel
free to <a href="https://github.com/borismus/osmus">fork it</a> to your heart's content. Oh, and for other game related
goodness, check out this article on <a href="http://www.html5rocks.com/en/tutorials/canvas/performance/">HTML5 canvas performance</a> recently
posted on html5rocks.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Global chrome media keys with Key Socket</title>
    <author><name>Boris Smus</name></author>
    <link href="/chrome-media-keys-revisited"/>
    
    <updated>2011-07-28T09:00:00-00:00</updated>
    
    <id>http://smus.com/chrome-media-keys-revisited</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I just skipped to the next Google Music track without leaving vim. Wanna play?
Here's the <a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">app</a> and <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">Chrome extension</a>. To learn how it works, read on!</p>

<h2>Script injection limitations</h2>

<p><a href="/chrome-media-keys">Last time around</a> I implemented keyboard bindings by injecting a content
script into every tab in Chrome, capturing key events and sending them to a
background page. This approach has some serious performance drawbacks:</p>

<ul>
<li>Content scripts injected into each page.</li>
<li>Background pages don't perform very well.</li>
</ul>

<p>And functional limitations:</p>

<ul>
<li>Won't work on special URLs like <code>chrome://</code> and <code>file://</code>.</li>
<li>Won't work when the omnibox is focused.</li>
<li>Requires chrome to be in the foreground.</li>
</ul>

<h2>Global key bindings and websockets</h2>

<p>What we really want is global key bindings. I don't care where my keyboard
focus happens to be right now, I just want to switch to the next freaking song!
This sort of thing requires OS-level event capture, which is functionality most
browsers don't come with. To get around this, I run a standalone app to capture
global keys and run a websocket server to send these events to the browser.
Note that this approach generalizes well to other use cases where functionality
is not available in a browser, but can be more readily implemented natively.</p>

<p>The obvious drawback to this approach is that it requires the user to run a
separate process to capture events.</p>

<h2>Media key bindings in Cocoa and Python</h2>

<p>Rogue Amoeba, maker of some popular OS X audio utilities, has a
<a href="http://rogueamoeba.com/utm/2007/09/29/apple-keyboard-media-key-event-handling/">nice post</a> on their blog on capturing media keys from an OS X
application. The basic idea is to subclass NSApplication and override the
sendEvent: selector:</p>

<pre><code>- (void)sendEvent: (NSEvent*)event {
  if( [event type] == NSSystemDefined &amp;&amp; [event subtype] == 8 ) {
      // Event processing
  }
  [super sendEvent: event];
}
</code></pre>

<p>Which in PyObjC results in the following equivalent code:</p>

<pre><code>def sendEvent_(self, event):
    if event.type() is NSSystemDefined and event.subtype() is 8:
        # Event processing

    NSApplication.sendEvent_(self, event)
</code></pre>

<p>It's pretty neat to be able to implement Cocoa apps without having to write a
single line of objective C. Writing a statusbar app with no dock item was
surprisingly simple (though I have doubts that this works well for Lions and
Tigers and Bears). All that's required is to set <code>LSUIElement</code> to <code>true</code> in the
Info.plist.</p>

<p>To package the whole PyObjC application, I wrote a setup.py script and used
<a href="http://svn.pythonmac.org/py2app/py2app/trunk/doc/index.html">py2app</a>, which generates a Mac OS X .app bundle which, from a user's
perspective is indistinguishable from an OS X app written in Objective C.</p>

<h2>A WebSocket server in python</h2>

<p>In addition to spawning off a Cocoa application and capturing events, of course
I create a WebSocket server. WebSockets use a pretty simple protocol which can
easily be implemented using python sockets. I based my implementation heavily
on <a href="https://gist.github.com/512987">this one</a>.</p>

<p>Since a Cocoa application runs its own event loop which captures the
main thread, the websocket listener needs to run in a separate thread:</p>

<pre><code>class KeySocketServer(Thread):
    def __init__(self):
        self.server = websocket.WebSocketServer('localhost', 1337, KeySocket)
        Thread.__init__(self)

    def run(self):
        self.server.listen()
</code></pre>

<p>The WebSocket standards are still evolving and implementers are, as ever,
scrambling to catch up. The good news is that this means
<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06#section-4.6">binary support</a> is coming, which is a boon for games and other
intensive network consumers. The bad news is that the latest Chrome canary (at
the time of writing), requires the response to contain the
<code>Sec-WebSocket-Accept</code> header, conforming to the
<a href="http://tools.ietf.org/html/draft-ietf-hybi-thewebsocketprotocol-06">draft-ietf-hybi-thewebsocketprotocol-06</a>, which is incompatible with
the python WebSocket code I'm currently using.</p>

<p>On my wishlist is a robus python WebSockets implementation that supports
multiple versions of the spec while it's still in flux.</p>

<h2>Injected scripts</h2>

<p>On the Chrome extension side, a script is injected into the web player
application, which creates a WebSocket client that connects to the python
server on port 1337. When media keys are pressed, the python server sends
messages to the JS clients and the injected JS simulates DOM events in the web
player application, controlling music playback.</p>

<h2>Try it out</h2>

<p>If you listen to Google Music, thesixtyone or Grooveshark in Chrome on OS X and
want global key bindings, please install the <a href="https://chrome.google.com/webstore/detail/fphfgdknbpakeedbaenojjdcdoajihik">extension</a> and
<a href="https://github.com/downloads/borismus/keysocket/KeySocket.zip">application</a>. If you're feeling generous, contribute your time and love!
I'd gladly take fixes for</p>

<ul>
<li>Key Socket servers for Linux and Windows</li>
<li>Content scripts to control other web audio players</li>
<li>Web Socket implementations that work with the new spec</li>
</ul>

<p>And of course, here's the <a href="https://github.com/borismus/keysocket">source</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Extending chrome developer tools</title>
    <author><name>Boris Smus</name></author>
    <link href="/extending-chrome-developer-tools"/>
    
    <updated>2011-07-11T09:00:00-00:00</updated>
    
    <id>http://smus.com/extending-chrome-developer-tools</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Chrome’s developer tools (also known as the WebKit inspector) are super useful
for web developers. If you aren't ramped up already, take a look at this
<a href="/devtools-cheatsheet">cheat sheet overview</a>. Also check out some of many online resources,
such as the <a href="http://code.google.com/chrome/devtools/docs/overview.html">official documentation</a>, which is quite readable, and this
<a href="http://www.youtube.com/watch?v=nOEw9iiopwI">12 tricks screencast</a> from Paul Irish.</p>

<p>But let’s not get sidetracked here. I wasn't going to write about using the
developer tools, it’s about <strong>developing</strong> developer tools. There are two ways
to go:</p>

<ol>
<li><p>You can extend the tools by writing chrome extensions that use the new
<code>chrome.experimental.devtools</code> APIs. These recently created APIs are still
subject to change, and marked experimental, so it’s a bit more difficult to
distribute and install them compared to other chrome extension APIs (more on
this later).</p></li>
<li><p>You can quite easily hack the devtools code yourself, to customize it to
your needs. Then, if you do a really good job, you can contribute your changes
back to the chromium project and feel better for being such an awesome person.</p></li>
</ol>

<p>Let’s dive in, starting from the beginning.</p>

<h2>Developer tools extensions</h2>

<p>Chrome provides <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">three separate APIs</a> for extending the developer tools:</p>

<ol>
<li><p>Audits - add a new audit to the Audits tab in the developer tools:
<img src="/extending-chrome-developer-tools/audit.png" alt="auditshot" /></p></li>
<li><p>Panels - add a whole new panel to the developer tools: <img src="/extending-chrome-developer-tools/panel.png" alt="panelshot" /></p></li>
<li><p>Resources - access a <a href="http://groups.google.com/group/http-archive-specification/web/har-1-2-spec">HAR file</a> containing known resources</p></li>
</ol>

<p>There are several <a href="http://code.google.com/chrome/extensions/trunk/samples.html#devtools">extension samples</a> available as starting points,
which cover all of the devtools extension API. The API is geared towards
Audit-oriented extensions (HTML/CSS/Javascript validators, performance
analyzers) and Panel-oriented extensions (framework specific tooling, etc).
You can also write extensions that integrate more loosely with the developer
tools (perhaps only using the resources API).</p>

<h3>My JSHint audit extension</h3>

<p>I wanted to get my hands dirty, and wrote a <a href="http://jshint.com">JSHint</a>-based Javascript
validator extension. This extension uses the devtools audit API to create an
audit that checks all scripts (linked and inline) on the current page. Errors
are shown per-script in the audit results view. I wanted to go further, and
allow the user to click on the error and jump to the resource, but this wasn’t
possible with the developer tools extension API. To get around this limitation,
I patched the developer tools with a pretty quick fix. More on modifying the
devtools themselves later.</p>

<p>Check out the JSHint audit extension <a href="https://github.com/borismus/jshint-extension">on github</a>.</p>

<h3>Writing an extension</h3>

<p>The first thing you should know about developer tools extensions, is that (at
the time of writing), they are experimental. This means that you will need to
launch chrome with a special flag to use them. I use a shell script called
<code>chrome-devtools</code> in my <code>~/bin</code> (which is in my <code>$PATH</code> of course):</p>

<pre><code>/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary \
  --user-data-dir=/Users/smus/.chrome-devtools \
  --enable-experimental-extension-apis \
  --debug-devtools-frontend=/Users/smus/devtools_frontend
</code></pre>

<p>Let me explain these switches:</p>

<ul>
<li><p><code>--user-data-dir</code>: specifies a custom profile for your chrome (so you have a
fresh profile to deal with and don’t accidentally clobber something important
in your main chrome profile).</p></li>
<li><p><code>--enable-experimental-extension-apis</code>: turns on experimental extension APIs.</p></li>
<li><p>More on the last switch later.</p></li>
</ul>

<p>By the way, all of chrome’s switches are explained on <a href="http://peter.sh/experiments/chromium-command-line-switches/">this page</a>,
provided by Peter Beverloo.</p>

<p>Developer tools extensions are based on a devtools page, which gets loaded when
the devtools open. You can specify this page in the manifest like other pages
(such as background and options pages):</p>

<pre><code>{
  // manifest start
  "devtools_page": "devtools.html",
  // manifest end
}
</code></pre>

<p>The API favors extensions that are built around an Audit or a Panel, and you
are free to use the resources API, as well as the rest of the chrome extension
features.</p>

<p>One important note is that the devtools page has very limited access to chrome
extension APIs, so you need to use <a href="http://code.google.com/chrome/extensions/messaging.html">messaging</a> and a background page to
access the full chrome API.</p>

<p>devtools.html:</p>

<pre><code>// Send request to the background page
chrome.extension.sendRequest({}, function(response) {
  // Handle response
});
</code></pre>

<p>background.html:</p>

<pre><code>chrome.extension.onRequest.addListener(function(request, sender, callback) {
  // Call some chrome extension APIs. For example,
  chrome.tabs.getSelected(null, function(tab) {
    // Etc... and then callback to devtools.html
    callback({data: tab.id});
  });
});
</code></pre>

<h3>Debugging devtools extensions</h3>

<p>When writing Chrome extensions, you have the power of the Chrome developer
tools at your disposal. You can debug content scripts by inspecting the page
into which the Javascript has been injected. Background pages can be viewed by
running <code>chrome://extensions</code> in developer mode, and clicking on
background.html for your extension.</p>

<p>However, when you write a developer tools extension, you rely on this
devtools_page as well as the rest of the extension ecosystem. Debugging this
page can get a bit meta – just inspect the devtools with the devtools!</p>

<p><img src="/extending-chrome-developer-tools/meta.png" alt="meta" /></p>

<p>Now you can inspect your devtools.html page, and debug away!</p>

<h2>Changing the product itself</h2>

<p>In addition to being highly extensible via Chrome extensions, the devtools are
also pretty easy to modify and tinker with. As you noticed earlier, the tools
are a web application written in Javascript, CSS and HTML, and thus inspectable
by the devtools themselves. This web application is hidden inside Chrome, but
luckily Chrome makes it easy to run a custom version of it. Basically, it takes
three steps:</p>

<ol>
<li>Download a <a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html">devtools frontend zip</a> (pre-packed version of
devtools). You will need to drill down into a directory for
<a href="http://commondatastorage.googleapis.com/chromium-browser-continuous/index.html?path=Mac/91508/">your platform</a>.</li>
<li>Extract the zip to some directory, <code>DIR</code>.</li>
<li>Invoke Chrome with the <code>--debug-devtools-frontend=DIR</code> switch, specifying
the same directory as in the previous step.</li>
</ol>

<p>Since the <code>devtools_frontend.zip</code> has some dependencies on Chrome, your zip file
and chrome version should be pretty close, or you may run into problems. You can
run the exact same version by also downloading the <code>chrome-platform.zip</code>.</p>

<p>Note: the Chromium project has a more detailed <a href="http://code.google.com/chrome/devtools/docs/contributing.html">instruction set</a> for
getting started.</p>

<h3>Making it your own</h3>

<p>Now that you have your custom devtools frontend, you can make tweaks to the
JS/CSS/HTML source, and see those changes in your development version of
chrome after you manually restart the browser.</p>

<p>For example, a bunch of people wanted to get rid of the yellow highlight that
appears overlaid in the browser window when hovering over DOM elements. At the
time of writing, the method responsible for this is in <code>inspector.js</code> called
<code>WebInspector.highlightDOMNode</code>. By applying this small patch, you can disable
the default behavior.</p>

<pre><code>1150,1151c1150
&lt;     // Do not highlight the DOM node.
&lt;     //this.highlightDOMNodeForTwoSeconds(nodeId);
---
&gt;     this.highlightDOMNodeForTwoSeconds(nodeId);
</code></pre>

<p>You can also easily make cosmetic tweaks to the devtools by changing the CSS.
A lot of the base styles are implemented in <code>inspector.css</code>. For instance, for
presentation purposes, the devtools are a bit small, so I find it useful to
increase the size of the devtools, which is a one line CSS change.</p>

<p>There's already a ton of useful features packed into the devtools, but I bet
you can come up with some more awesomeness to add as an extension or chromium
patch. Let me know if you do!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>OAuth 2.0 from chrome extensions</title>
    <author><name>Boris Smus</name></author>
    <link href="/oauth2-chrome-extensions"/>
    
    <updated>2011-07-07T09:00:00-00:00</updated>
    
    <id>http://smus.com/oauth2-chrome-extensions</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Applications that access online services often need to access a user's private
data. Chrome Extensions are no different. OAuth has emerged as the standard way
of letting users share their private resources across sites without having to
hand out their usernames and passwords. There is already a very nice 
<a href="http://code.google.com/chrome/extensions/tut_oauth.html">OAuth library for Chrome Extensions</a> that aims to simplify some of 
the pains that developers face when authorizing against OAuth endpoints.</p>

<p>Since this library was written, the OAuth standard enjoyed a version bump
(<a href="http://oauth.net/2/">OAuth 2.0</a>) which greatly simplifies the flow by no longer requiring
cryptography in the client. Also, some adventurous companies (notably
Google, Facebook and others) have actually implemented OAuth 2.0
endpoints. At the time of writing, OAuth 2.0 is still a draft spec, but is
nearing completion, and Chrome Extensions need some love.</p>

<p>You may be wondering why you even need an OAuth 2.0 library in the first place.
As Aaron Parecki pointed out in his <a href="http://www.slideshare.net/aaronpk/the-current-state-of-oauth-2">Current State of OAuth 2</a>
presentation at <a href="http://opensourcebridge.org/">Open Source Bridge</a> in Portland, OAuth 2 is very
much a moving target. The spec is not yet finalized, and there are 16 versions
of it (although the most popular seems to be v10). Also, today's OAuth 2
implementations diverge from the spec in varying degrees, adding to the
developer pain. The reason this library needs to be chrome extension-specific
is that unfortunately Chrome extensions can't directly use the OAuth 2.0
server-side or client-side flows because they live at <code>chrome-extension://</code>
URLs.</p>

<p>When writing the OAuth 2.0 library for Chrome extensions, I had some goals in
mind:</p>

<ol>
<li>Support a variety of OAuth 2.0 providers that implement the spec</li>
<li>Allow one app/extension to use multiple different OAuth 2.0 endpoints</li>
<li>Avoid background pages for performance reasons</li>
</ol>

<h2>The OAuth 2.0 Library</h2>

<p>There's a bit of setup involved if you'd like to create a Chrome extension that
connects to an OAuth 2 endpoint. This brief tutorial will guide you through
connecting to Google's APIs.</p>

<p>Register your application with an OAuth 2.0 endpoint that you'd like to
use. If it's a Google API you're calling, go to the <a href="https://code.google.com/apis/console/">Google APIs</a> page,
create your application and note your client ID and client secret. For more
info on this, check out the <a href="http://code.google.com/apis/accounts/docs/OAuth2.html">Google OAuth 2.0</a> docs. When you setup your
application, you will be asked to provide redirect URI(s). Please provide the
URI that corresponds to the service you're using.</p>

<p>Here's a table that will come in handy:</p>

<p><style>
  #impls { margin-left: -100px; }
  #impls td, #impls th { border: 1px solid #999 }
  #impls td { padding: 5px }
</style></p>

<table id="impls">
  <tr>
    <th>Adapter</th>
    <th>Redirect URI</th>
    <th>Access Token URI</th>
  </tr>
  <tr>
    <td>google</td>
    <td>http://www.google.com/robots.txt</td>
    <td>https://accounts.google.com/o/oauth2/token</td>
  </tr>
  <tr>
    <td>facebook</td>
    <td>http://www.facebook.com/robots.txt</td>
    <td>https://graph.facebook.com/oauth/access_token</td>
  </tr>
  <tr>
    <td>github</td>
    <td>https://github.com/robots.txt</td>
    <td>https://github.com/login/oauth/access_token</td>
  </tr>
</table>

<h4>Step 1: Copy library</h4>

<p>You will need to copy the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib">oauth2 library</a> into your chrome extension
root into a directory called <code>oauth2</code>.</p>

<h4>Step 2: Inject content script</h4>

<p>Then you need to modify your manifest.json file to include a content script
at the redirect URL used by the Google adapter. The "matches" redirect URI can
be looked up in the table above:</p>

<pre><code>"content_scripts": [
  {
    "matches": ["http://www.google.com/robots.txt*"],
    "js": ["oauth2/oauth2_inject.js"],
    "run_at": "document_start"
  }
],
</code></pre>

<h4>Step 3: Allow access token URL</h4>

<p>Also, you will need to add a permission to Google's access token granting URL,
since the library will do an XHR against it. The access token URI can be looked
up in the table above as well.</p>

<pre><code>"permissions": [
  "https://accounts.google.com/o/oauth2/token"
]
</code></pre>

<h4>Step 4: Include the OAuth 2.0 library</h4>

<p>Next, in your extension's code, you should include the OAuth 2.0 library:</p>

<pre><code>&lt;script src="/oauth2/oauth2.js"&gt;&lt;/script&gt;
</code></pre>

<h4>Step 5: Configure the OAuth 2.0 endpoint</h4>

<p>And configure your OAuth 2 connection by providing clientId, clientSecret and
apiScopes from the registration page. The authorize() method may create a new
popup window for the user to grant your extension access to the OAuth2
endpoint.</p>

<pre><code>var googleAuth = new OAuth2('google', {
  client_id: '17755888930840',
  client_secret: 'b4a5741bd3d6de6ac591c7b0e279c9f',
  api_scope: 'https://www.googleapis.com/auth/tasks'
});

googleAuth.authorize(function() {
  // Ready for action
});
</code></pre>

<h4>Step 6: Use the access token</h4>

<p>Now that your user has an access token via <code>auth.getAccessToken()</code>, you can
request protected data by adding the accessToken as a request header</p>

<pre><code>xhr.setRequestHeader('Authorization', 'OAuth ' + myAuth.getAccessToken())
</code></pre>

<p>or by passing it as part of the URL (depending on the server implementation):</p>

<pre><code>myUrl + '?oauth_token=' + myAuth.getAccessToken();
</code></pre>

<p><strong>Note</strong>: if you have multiple OAuth 2.0 endpoints that you would like to
authorize with, you can do that too! Just inject content scripts and add
permissions for all of the providers you would like to authorize with.</p>

<p>I've provided <a href="https://github.com/borismus/oauth2-extensions/tree/master/samples">some sample extensions</a> that use this library to help
you get started.</p>

<h2>Varying OAuth implementations</h2>

<p>Writing this library for one OAuth 2.0 endpoint was pretty straightforward.
The issues came when branching out to support multiple OAuth 2.0 server
implementations which comply to various degrees with differing versions of the
spec.</p>

<p>Facebook was the worst offender here. They claim to be an OAuth 2.0
implementation in line with v10, but are actually quite far from it. Here are
some of the issues:</p>

<ul>
<li><p>Token request method is GET instead of POST.</p></li>
<li><p>Token response is some strange form encoded format instead of JSON.</p></li>
<li><p><a href="http://developers.facebook.com/docs/authentication/permissions/">List of scopes</a> (aka "extended permissions") was really hard to
find.</p></li>
<li><p>Apparently to get a user's favorite music, you need the <code>user_likes</code>
permission. Facebook, please <a href="http://forum.developers.facebook.net/viewtopic.php?pid=283691">fix your docs</a>.</p></li>
<li><p>No refresh tokens but they have an offline_access permission which makes your
access token expire later. This is ridiculous!</p></li>
</ul>

<p>Twitter doesn't even have an OAuth 2.0 API. <a href="http://dev.twitter.com/anywhere">@Anywhere</a> does not
count. Some good <a href="http://www.quora.com/Why-isnt-Twitter-implementing-OAuth-2-0-just-like-Facebooks">questions</a> on <a href="http://www.quora.com/When-is-Twitter-going-to-implement-OAuth-2-0">quora</a> about this.</p>

<p>Still there are a lot of services that <em>DO</em> implement OAuth 2.0, such as
Foursquare, Gowalla, Windows Live, Salesforce, Soundcloud and many others.</p>

<h2>Extending the Library</h2>

<p>To mitigate differences between OAuth 2.0 implementations, I implemented the
<a href="http://en.wikipedia.org/wiki/Adapter_pattern">Adapter pattern</a>. Doing this encapsulates protocol differences
in a separate adapter module for each server implementation.</p>

<p>The library comes with adapters for Google, Facebook and Github. These adapters
are located in the <a href="https://github.com/borismus/oauth2-extensions/tree/master/lib/adapters">adapters directory</a> here. If you would like to
contribute your own adapter, please take a look at the sample adapter and then
<a href="https://github.com/borismus/oauth2-extensions">fork the project</a>, submit a pull request, and I'll try to add
it to the project.</p>

<p>Also, please let me know if you experience problems using this library and
we'll sort them out! The best way to do this is via <a href="http://github.com/borismus">github</a> or
<a href="http://twitter.com/borismus">twitter</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Brazil trip</title>
    <author><name>Boris Smus</name></author>
    <link href="/brazil-trip"/>
    
    <updated>2011-06-21T09:00:00-00:00</updated>
    
    <id>http://smus.com/brazil-trip</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Last month I took a work trip to São Paulo, Brazil. I gave four Chrome/HTML5
presentations and talked to many engineers and designers over the course of the
week, trying hard not to sound like a broken record. Luckily Brazil has a lot
of people so the audiences were different each time! Here's a link to my
<a href="http://smustalks.appspot.com/brazil-11/">slides</a>, that I tweaked slightly depending on the audience.</p>

<p>I'll be dumping all of my slide decks and/or talk videos to <a href="http://smustalks.appspot.com/">smustalks</a> on
AppEngine. I've made the switch from building slides in Keynote to using this
fantastic HTML5 <a href="http://code.google.com/p/html5slides/">slide deck</a> template and my favorite <a href="http://www.vim.org/">text editor</a>.</p>

<h2>Food</h2>

<ul>
<li>Delicious fruits of all varieties: exotic cashew, persimmon, star fruit, guyava, and way tastier bananas.</li>
<li>Ridiculously massive portions of sashimi at japanese restaurants.</li>
<li>The Rodizio we went to was super exquisite. Largest salad bar ever, including caviar (srsly).</li>
</ul>

<h2>Life</h2>

<ul>
<li>SP is a concrete jungle like nothing I've ever seen. The whole city is built up within a 60km radius.</li>
<li>Chaotic traffic patterns and insane motorcyclists abound. Everyone loves to drive between lanes and honk gratuitously.</li>
<li>Super bumpy roads date back to the dictatorship days. If you take the wrong exit, prepare to jump around.</li>
<li>Helicopters fly all over town and sometimes land precariously close to unsuspecting window cleaners dangling from highrise roofs.</li>
<li>Beware foul smelling rivers and slums on the way to Friday morning meetings!</li>
</ul>

<h2>Work</h2>

<ul>
<li>Super friendly people but my Portuguese skills failed to topple the language barrier.</li>
<li>Long process to get into any office building. Some ask for ID, others ask for passport. Takes half an hour just to get in...</li>
<li>Somehow, Google Brazil still feels like Google!</li>
</ul>

<p>After my work was done, I took some <a href="https://picasaweb.google.com/boris.smus/Brazil">travel photos</a> and heard
<a href="https://picasaweb.google.com/boris.smus/Brazil#5612942830070183426">Samba da Minha Terra</a> live, and by so doing, joined the ranks of Don and
Magdalena!</p>

<pre><code>I've never sailed the Amazon,
I've never reached Brazil;
But the Don and Magdalena,
They can go there when they will!

            - R. Kipling
</code></pre>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Dynamic chrome extension icons</title>
    <author><name>Boris Smus</name></author>
    <link href="/dynamic-icons-chrome-extensions"/>
    
    <updated>2011-06-06T09:00:00-00:00</updated>
    
    <id>http://smus.com/dynamic-icons-chrome-extensions</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Extension developers aren't given much freedom to modify Chrome's browser
chrome. Without resorting to changing the page itself, or using the new devtools
extension APIs, there are two main ways of doing this. <a href="http://code.google.com/chrome/extensions/pageAction.html">Page actions</a>, which
reside in the omnibox, and <a href="http://code.google.com/chrome/extensions/browserAction.html">browser actions</a>, which are positioned to the
right of the omnibox both of which are simple buttons with icons, click actions
and hover states. Chrome conveniently <a href="http://code.google.com/chrome/extensions/browserAction.html#method-setIcon">provides an API</a> to dynamically change
the icon of these buttons.</p>

<p>You can do this by creating image data by hand or using the canvas API's
<code>getImageData</code> function.:</p>

<pre><code>var canvas = document.getElementById('canvas');
var context = canvas.getContext('2d');
// ...draw to the canvas...
var imageData = context.getImageData(0, 0, 19, 19);
chrome.browserAction.setIcon({
  imageData: imageData
});
</code></pre>

<p>Note that this would be possible even without this chrome-specific API, by
instead using <a href="http://en.wikipedia.org/wiki/Data_URI_scheme">data URIs</a> to set the image. There's a GMail <a href="http://googlesystem.blogspot.com/2011/01/dynamic-gmail-favicon.html">labs plugin</a>
that does this to badge the favicon with the unread email count.</p>

<p>What makes this an interesting design domain is the limitation of the medium.
Limited real estate (browser actions at 19x19 px, page actions at 16x16 px) adds
significant constraints. Still, one can show numbers, colors, small icons and
graphs in this context, or even small amounts of text.</p>

<h2>Applications of Dynamic Icons</h2>

<p>There are extensions that use dynamic icons already, such as the
<a href="https://chrome.google.com/webstore/detail/kpekpmmfocifmbnnoahnclccmjkckpcl">PageRank extension</a>, which effectively shows the Google PageRank for the
current page right inside the browser action.</p>

<p>Here are some other possibilities (not at all an exhaustive list):</p>

<ul>
<li>Create badges for page actions (which don't implement <code>setBadge*</code> calls).</li>
<li>Icon of the weather forecast, click to toggle between days.</li>
<li>Bandwidth meter: how large was the download of this page?</li>
<li>A random profile pic of people that +1'ed the site.</li>
</ul>

<h2>The Smallest Music Visualizer</h2>

<p>I wrote a sample extension to demonstrate the dynamic icon potential of Chrome
extensions. This indispensible extension is a music visualizer that renders
inside of a browser action button. I use the <a href="http://chromium.googlecode.com/svn/trunk/samples/audio/specification/specification.html">Web Audio API</a> to playback a
song and analyse the audio stream, render the visualized audio spectrum with a
canvas element and then transfer the resulting image data to the browser action
icon.</p>

<p><img src="/dynamic-icons-chrome-extensions/music-vis.png" alt="screenshot" /></p>

<p>Try out the <a href="https://chrome.google.com/webstore/detail/befnabfghcghgpmkjoalbecphdgdmick?hl=en">chrome extension</a> here, but note that it requires the Web Audio
API flag to be enabled under about:flags (and a browser restart afterward).
Check out and fork the <a href="https://github.com/borismus/Music-Visualizer-Chrome-Extension">source on github</a>.</p>

<h2>Learnings</h2>

<p>This music visualizer extension loads an mp3 file when the extension
background page loads, which takes a certain amount of time. To provide a
better user experience, I was hoping to change the icon to reflect that the
file was being loaded, and ran into two issues.</p>

<p>The first issue was that when I tried to render a small string like "wait" in
the icon, I wanted to use a custom <code>@font-face</code> embedded font, which is now
well supported in CSS3. You can load CSS fonts</p>

<pre><code>@font-face {
  font-family: "Silkscreen"
  src: url(slkscr.ttf);
}
</code></pre>

<p>and then use them in a canvas:</p>

<pre><code>context.font = "8px Silkscreen";
context.fillText('load');
</code></pre>

<p>When using custom fonts from HTML, the browser waits for the font to load, and
then does a relayout. When using it from canvas, things get a bit tricky since
the browser of course doesn't do font relayout for you and there's
unfortunately no DOM event that fires when all embedded fonts finished loading.
More precisely, onload behaviors differ from browser to browser. Mozilla waits
for all fonts to load before firing the event, WebKit doesn't. You can work
around this problem by assigning the custom font to a div, and observing the
div's width, which will change when the font loads (<a href="https://github.com/paulirish/font-face-detect/blob/master/isFontLoaded.js">codified</a>).</p>

<p>Although I ultimately didn't use this font to show loading state, I recommend
checking out the <a href="http://kottke.org/plus/type/silkscreen/">silkscreen font</a> for 8-bit style designs that lend
themselves well to small resolution envirnoments. You can fit about 3x3
characters inside a 16x16 canvas:</p>

<p><img src="/dynamic-icons-chrome-extensions/silkscreen.png" alt="silkscreen" /></p>

<p>To show that the extension is still loading, I went with a progress bar instead
of a message. A second issue arose when I wanted to show the progress
bar animating while the mp3 loads. Unfortunately the Web Audio API doesn't
currently support asynchronous loading of files, so the UI thread gets blocked
during the <code>audioContext.createBuffer</code> call of this code snippet:</p>

<pre><code>var request = new XMLHttpRequest();
request.onload = function() {
  var audioBuffer = audioContext.createBuffer(request.response, false);
}
</code></pre>

<p>Async loading of audio buffers is now a <a href="https://bugs.webkit.org/show_bug.cgi?id=61947">tracked issue</a> for you upvote in
the webkit bug tracker. I thought of working around this with Web Workers, but
gave up early because of difficulties with passing objects between worker
threads, and no shared memory options that would let workers access the
context of the main UI thread.</p>

<p>Another interesting observation is that <code>requestAnimationFrame</code> does not work in
a background page. I initially tried to use it to animate the music visualizer,
but it didn't work. This is of course the API is designed to only callback when
the calling page is in the foreground, and since the background page is never
foregrounded, the callback never fires.</p>

<p>That's it for me, now it's your turn! So, dearest reader, go forth and write
some awesome Chrome extensions which tastefully use dynamic icons for page
actions, browser actions, and favicons to make our browsing experience even
better.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Crunching numbers for the NHL finals</title>
    <author><name>Boris Smus</name></author>
    <link href="/nhl-finals-numbers"/>
    
    <updated>2011-05-31T09:00:00-00:00</updated>
    
    <id>http://smus.com/nhl-finals-numbers</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Now that the Vancouver Canucks are in the NHL Finals, a special visit to
Vancouver is in order. Unfortunately, the recently posted schedule really
sucks. There’s just one weekend game, and the rest are spaced in such a way
that I can only hope to see Game 5 (if it happens) and either Game 4 or Game 6
(if it happens) if I manage to work from home half of the week. The question
is: which game to choose? I want to be in Vancouver when the Canucks take the
cup!</p>

<h2>Yay math!</h2>

<p>Being super nerdy, I decided to throw math at the problem. Brushing up on
probability, I calculated the odds of a series ending at each game (4-7). For a
series to end in game 4, one team has to win all four games. The odds of this,
given a 50% chance for each team to win, is (1/2)^4. Since there are two teams,
multiply by two, resulting in a 1/8 chance. You can do similar calcuations to
get the following odds:</p>

<pre><code>p(4)   p(5)   p(6)   p(7)
0.125  0.25   0.3125 0.3125
</code></pre>

<p>Based on this distribution, it seems that game 6 is more likely to see the end
of the series than game 4, but then I would risk missing a game 4 victory.
<a href="http://www.ted.com/talks/arthur_benjamin_s_formula_for_changing_math_education.html">Arthur Benjamin</a> would be proud of me (unless my numbers are wrong).</p>

<h2>A bit of history</h2>

<p>Historical data tells a different story. As it turns out, the NHL is really
old, dating back to 1927, but the league switched to best-of-7 scoring in 1939.
Wikipedia has a great chronology of NHL playoffs, complete with scores and
brackets.</p>

<p>I analyzed series scores from all playoff games in the last 25 years (since
1985) by fetching raw wikipedia articles via wget and running them through a
[python script]. Older records can also be found on wikipedia, but they are
buried inside other pages, in harder to parse formats.</p>

<pre><code>for year in {1985..2011} do
  wget -O ${year} "http://en.wikipedia.org/w/index.php?title=${year}_Stanley_Cup_playoffs&amp;action=raw"
done
</code></pre>

<p>This yielded the following distribution for playoff games, based on 383
matches:</p>

<pre><code>p(4)   p(5)   p(6)   p(7)
0.16   0.24   0.33   0.27
</code></pre>

<p>For finals games only, the distribution is more skewed, probably in part due to
a shortage of data (just NN finals games from the wikipedia page), but finals
game history is probably a better predictor for finals games than all playoffs
games, so this is worthwhile:</p>

<pre><code>p(4)   p(5)   p(6)   p(7)
0.28   0.24   0.27   0.21
</code></pre>

<p>Here are all three distributions plotted in a graph:</p>

<p><img src="/nhl-finals-numbers/nhl-series-odds.png" alt="graph" /></p>

<h2>Analyzing...</h2>

<p>The most striking thing about this graph is the huge difference between finals
history and stats, especially in the 4 game series scenario. Perhaps some teams
just buckle under the pressure, while their opponents remain steadfast. Or this
is just a statistically insignificant fluke that can be attributed to having an
insufficiently large sample of finals matches.</p>

<p>As expected, the historical distributions are skewed toward the series
finishing in Game 4 due to unequal strength between teams. In other words, if
one team is stronger than another, then the odds of that team winning each game
would be greater, making the series shorter on average.</p>

<h2>Decision!</h2>

<p>Armed with numbers, I can decide whether to pick Game 4 or Game 6. Here are the
odds of seeing the Canucks (yeah yeah, maybe the Bruins) win in games 4 or 5,
compared to 5 or 6, as calculated from each of the three methods:</p>

<pre><code>         Game 4 or 5            Game 5 or 6
Stats        38%                    58%
Playoffs     40%                    57%
Finals       52%                    51%
</code></pre>

<p>Interesting. Given how annoying it would be to miss a Game 4 victory and how
favorable the finals history-based odds look, I’ll go for 4 and 5. Now to try
to work from home, book flights and bask in Canuck glory!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Chrome developer tools cheatsheet</title>
    <author><name>Boris Smus</name></author>
    <link href="/devtools-cheatsheet"/>
    
    <updated>2011-05-13T09:00:00-00:00</updated>
    
    <id>http://smus.com/devtools-cheatsheet</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Chrome's Developer Tools have been getting much deserved love at this last
Google I/O. Paul Irish and I started with an <a href="http://www.io-bootcamp.com/">bootcamp lab</a>, a hands-on 
walk through tweaking a web application using the developer tools. We handed
out a cheatsheet to give developers an overview of available features.</p>

<p><a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf"><img src="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.jpg" alt="cheatsheet jpg" /></a></p>

<p>The cheatsheet is available for download in <a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.pdf">PDF</a> and
<a href="https://github.com/borismus/DevTools-Lab/raw/master/cheatsheet/chromedev-cheatsheet.png">PNG</a>. </p>

<p>Thanks to the awesome I/O organizers, we also gave away free HTML5 cake (as
promised) after the session ended:</p>

<p><img src="/devtools-cheatsheet/html5-cake.jpg" alt="html5 cake" /></p>

<p>The developer tools were also featured at the <a href="http://www.youtube.com/watch?v=MiYND_zvIc0&amp;t=8m30s">start of the Chrome keynote</a>
with a webkit-speech demo. Finally, Pavel Feldman (lead engineer for the dev
tools) and Paul gave an great <a href="http://www.youtube.com/watch?v=N8SS-rUEZPg">I/O talk</a>.</p>

<p>One of the most promising aspects of the Chrome Developer Tools is that they
are easy to extend with (experimental) <a href="http://code.google.com/chrome/extensions/trunk/experimental.devtools.html">chrome extension APIs</a>. I'm stoked to
see what people create to make these developer tools even better!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Multi-touch for your desktop browser</title>
    <author><name>Boris Smus</name></author>
    <link href="/multi-touch-browser-patch"/>
    
    <updated>2011-05-02T09:00:00-00:00</updated>
    
    <id>http://smus.com/multi-touch-browser-patch</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>In mobile development, it's often easier to start prototyping on the desktop
and then tackle the mobile-specific parts on the devices you intend to support.
Multi-touch is one of those features that's difficult to test on the desktop, since
most desktops didn't have multi-touch hardware, and thus desktop browsers don't
have touch event support. Things are different today (you hear every mother say). 
Most new Macs, for example, ship with multi-touch capable input of some sort.
Unfortunately the browsers haven't really caught up yet.</p>

<p>Enter Fajran Iman Rusadi, who released a <a href="https://github.com/fajran/npTuioClient">npTuioClient</a> NPAPI plugin with a
JavaScript wrapper. Unfortunately this library provides a non-standard API to
multi-touch, which is not ideal for developers that want to write their
multi-touch application on desktop and then run the same code on their mobile
devices without modifications.</p>

<h2>Browser Patches</h2>

<p>As HTML5 grows up, browser vendors struggle to stay current up with the growing
variety of specifications. The result is <a href="http://caniuse.com/">uneven feature support</a> across
browsers and a complex problem for web developers.</p>

<p>The web development community has rallied around <strong>shims</strong> and <strong>polyfills</strong>
for the solution. These are bizarre terms that I find confusing and so will
defer to <a href="http://remysharp.com/2010/10/08/what-is-a-polyfill/">Remy Sharp to define</a>. The basic idea of both is to fill in
functionality that's missing in the browser implementation.</p>

<p>Since we now have a well established <a href="https://dvcs.w3.org/hg/webevents/raw-file/tip/touchevents.html">touch events specification</a> working
group at the W3C, I wrote <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a>, a multi-touch polyfill thatlets
you, the developer, write the same code, test it on your desktop browser and
then, run it on your real device. Totally tubular!</p>

<p>MagicTouch.js still relies on the npTuioClient plugin, it just creates
spec-compatible touch events. Incidentally, here's how you can trigger custom
DOM events:</p>

<pre><code>var event = document.createEvent('CustomEvent');
// Initialize the event, make it bubble up and possible to cancel
event.initEvent('touchstart', true, true);
// Assign properties to the event
event.touches = touchArray;
...
// Get the element associated with the event
var element = document.elementFromPoint(...);
// Assign the element
event.target = element;
// Finally, dispatch the event
element.dispatchEvent(event);
</code></pre>

<p>Note that this approach to create custom DOM events is not cross-browser
compatible. I only tested in Chrome.</p>

<h2>Installation</h2>

<p>Here how to get multi-touch web events working in Chrome for Mac:</p>

<ol>
<li>Download and install the <a href="https://github.com/fajran/npTuioClient#readme">npTuioClient NPAPI plugin</a>
into <code>~/Library/Internet Plug-Ins/</code>.</li>
<li>Download the <a href="https://github.com/fajran/tongseng/downloads">TongSeng TUIO tracker</a> for Mac’s MagicPad and start the
server</li>
<li>Download <a href="https://github.com/borismus/MagicTouch">MagicTouch.js</a> and include both the script and the plugin in your
app.</li>
</ol>

<p>The code for this is as follows:</p>

<pre><code>&lt;head&gt;
  ...
  &lt;script src="/path/to/magictouch.js"&gt;&lt;/script&gt;
&lt;/head&gt;
&lt;body&gt;
  ...
  &lt;object id="tuio" type="application/x-tuio" style="width:0; height:0;"&gt;
    TUIO Plugin failed to load
  &lt;/object&gt;
&lt;/body&gt;
</code></pre>

<p>...and you're off to the races! Your multi-touch code will now work. Try out
this <a href="https://github.com/borismus/MagicTouch/blob/master/samples/tracker.html">finger tracking demo</a> on either your multi-touch mobile device or your
newly patched desktop browser.</p>

<h2>Future Steps</h2>

<p>As you saw, MagicTouch.js takes some effort to set up initially, requires
you to use an <code>&lt;object&gt;</code> in the HTML, and also needs you to run a separate
process for intercepting touch events. While we can't quite get away without
having to run another process, we can eliminate the NPAPI plugin by using the
<a href="http://dev.w3.org/html5/websockets/">WebSocket API</a> to communicate to that process.</p>

<p>If you're interested in multi-touch mobile web development, check out this
<a href="http://www.html5rocks.com/mobile/touch.html">article on html5rocks.com</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>jQuery conference 2011</title>
    <author><name>Boris Smus</name></author>
    <link href="/jquery-conference"/>
    
    <updated>2011-04-26T09:00:00-00:00</updated>
    
    <id>http://smus.com/jquery-conference</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A few weekends ago I went to the jQuery Conference held at the MS campus in
Mountain View. And I took notes!</p>

<p>Overall trends about the jQuery community:</p>

<ul>
<li><p>People are writing more complex apps on top of jQuery and there
is a widely understood need for MVC frameworks, such as <a href="http://documentcloud.github.com/backbone/">Backbone.js</a>, 
<a href="http://knockoutjs.com/">Knockout.js</a> and <a href="http://javascriptmvc.com/">JavaScript MVC</a>.</p></li>
<li><p>Feature detection is important!</p>

<ul>
<li>Polyfill - replicates standard feature with a compatible API</li>
<li>Shim - provides its own API for a future feature</li>
</ul></li>
<li><p>Serious need for templating systems. Boris Moore showed a very performant
demo of jQuery Templates. Many other templating systems exist as well,
like one built into <a href="http://documentcloud.github.com/underscore/">underscore.js</a> and <a href="http://mustache.github.com/">mustache.js</a>.</p></li>
<li><p>Many new mobile performance tools: <a href="http://www.blaze.io/">blaze.io</a> -- a tool that gives a
general overview of a site's performance, <a href="http://pcapperf.appspot.com/">pcapperf</a> -- a web performance
analyzer that uses tcpdump output from mobile device activity, and <a href="http://jdrop.org/">jDrop</a>
-- a service that lets you capture large amounts of data on your mobile
device and then analyze it on the desktop web browser.</p></li>
<li><p>People are rallying around <a href="http://jshint.com/">JSHint</a>, a fork of Crockford's <a href="http://www.jslint.com/">JSLint</a>
project, but with more configurable JavaScript sanitation rules.</p></li>
<li><p>Haters gotta hate. Everybody seems to get a kick out of hating Douglas
Crockford. Give the nice opinionated man a break and go write some
JavaScript.</p></li>
</ul>

<p>I went to a bunch of talks, and I took the most notes for during this talk:</p>

<h2>State of jQuery</h2>

<p>John Resig talked about a bunch of changes to the project structure, largely
irrelevant to jQuery library consumers. He also covered some of many jQuery 1.6
improvements:</p>

<ul>
<li>Rewrite of <code>attr()</code> and <code>val()</code>. For example, <code>attr('val', false)</code> removes
the attribute</li>
<li>Separate <code>prop()</code> from <code>attr()</code>. Indeed!</li>
<li><code>$('input:focus')</code> gets focused input box across platforms</li>
<li>Significant performance boosts:
<ul>
<li><code>attr()</code> performance ~85% faster, <code>val()</code> ~150% faster, <code>data()</code> ~115%
faster</li>
</ul></li>
<li>Integration with requestAnimationFrame for animations</li>
<li><code>$.map(Object, function)</code> now works (as it does for Arrays)</li>
</ul>

<p>Pro tip: jQuery automatically parses serialized JSON if it's included as the value 
of a HTML5 data attribute. Example: <code>&lt;header data-array="[0,1,2]"&gt;</code> then 
<code>$('header').data('array')[1] == 1</code></p>

<h2>State of jQuery Mobile</h2>

<p>Mobile matters. 5.3 billion mobile subscriptions (cf. global population of 6.8
billion), 10 billion web-enabled mobile devices.</p>

<p>John Resig also touched on jQuery Mobile, and then Scott Jehl and Todd Parker went 
into a lot more detail.</p>

<ul>
<li>Navigation model now uses the <a href="https://developer.mozilla.org/en/DOM/Manipulating_the_browser_history">history API</a> for hash-less URLs.</li>
<li>jQM minified and packed is ~18kb!</li>
<li>Nice gallery of goodness at <a href="http://www.jqmgallery.com/">jQuery Mobile Gallery</a>
<ul>
<li>Including <a href="http://www.barackobama.com/m/">Obama's mobile site</a>!</li>
</ul></li>
<li>Media queries
<ul>
<li>Useful as a browser support cutoff heuristic.</li>
<li>CSS classes added based on media queries, facilitating simpler styles</li>
<li>Uses <a href="https://github.com/scottjehl/Respond">Respond.js</a>, a polyfill for browsers that don't support media queries</li>
</ul></li>
<li>Philosophy: easily brandable cross-device experience</li>
<li>All builtin views are ARIA-enabled</li>
</ul>

<p>Pro tip: mouse events in some mobile browsers are on a <a href="http://cubiq.org/remove-onclick-delay-on-webkit-for-iphone">300ms delay</a> to allow
the browser to interpret user's gestures. jQuery Mobile includes a fix for
this!</p>

<h2>Prototyping Tools in jQuery</h2>

<p>Super useful and informative set of tools!</p>

<p>MockJAX is a library that simulates a server.</p>

<ul>
<li>Intercepts and simulates AJAX calls
<ul>
<li>Define a URL structure and a response structure</li>
</ul></li>
<li>Can define responses as a function.</li>
<li>Can simulate error responses.</li>
<li>Useful for unit testing as well!</li>
</ul>

<p>MockJSON: create fake JSON on demand</p>

<ul>
<li>A way to generate random-ish JSON</li>
<li>For example, <code>{'age|0-99'}</code> outputs <code>{'age': randint_between_0_and_99}</code></li>
</ul>

<p>Amplify: abstraction layer for all data</p>

<ul>
<li>Abstracts away shifting server-side APIs</li>
<li>amplify.request.define can define a data store. </li>
</ul>

<p>For example:</p>

<pre><code>amplify.request.define("list", "ajax", {
  url: "/todo/",
  dataType: "json",
  type: "GET"
});
</code></pre>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>From Wordpress to Hyde</title>
    <author><name>Boris Smus</name></author>
    <link href="/wordpress-to-hyde"/>
    
    <updated>2011-04-20T09:00:00-00:00</updated>
    
    <id>http://smus.com/wordpress-to-hyde</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I've used <a href="http://wordpress.com/">Wordpress</a> for a couple of years now as my web publishing platform of
choice. I customized it a little bit, and made a <a href="/new-design">custom theme</a> for it. But 
nothing is perfect, and neither is wordpress. As a result of relatively heavy
use, I've collected a list of things I don't much like about it:</p>

<ul>
<li>Can't upload source code since it restricts allowed file types.</li>
<li>Can't easily edit files with a regular text editor.</li>
<li>Painful theme customization.</li>
<li>PHP makes it unpleasant to extend.</li>
<li>Constant security upgrades required.</li>
</ul>

<p>All of these things separately are perhaps small and insignificant, but
together add up to create a certain barrier to blogging.</p>

<h2>Static files</h2>

<p>A blog is mostly static. Why should my article be stored in a database? It's hardly 
ever queried or searched. Comments are perhaps the exception here, but services
like <a href="http://disqus.com/">Disqus</a> implement commenting with embeddable JavaScript. And in general,
dynamic parts can be handled in client-side code.</p>

<p>So I looked to static site generators. There's about <a href="http://stackoverflow.com/questions/186290/best-static-website-generator">a million</a> of these,
the most prominent of which is <a href="https://github.com/mojombo/jekyll">Jekyll</a>, written by the github co-founder. 
Ultimately I chose <a href="https://github.com/lakshmivyas/hyde/">Hyde</a> for it's use of <a href="http://docs.djangoproject.com/en/dev/ref/templates/builtins/">Django templates</a> and Python.</p>

<h2>Migrating the old blog</h2>

<p>The old blog was written in HTML, but I wanted to switch to a format that can
be written faster, such as Markdown, Textile, or many others. To do this I used
a tool called exitwp, which parses the Wordpress export and generates Markdown
files appropriate for Jekyll. I <a href="https://github.com/borismus/exitwp">forked exitwp</a> and hacked it to generate
files better suited to Hyde. Thanks <a href="http://thomas.jossystem.se/">Thomas Frössman</a> for exitwp.</p>

<p>I did spent a fair amount of time tweaking the export, replacing Smart YouTube URLs
with real YouTube embeds, and eliminating Syntax Highlighter markup.</p>

<h2>Customizing hyde</h2>

<p>Hyde is basically built on top of Django templates, so is customizable with
template tags and filters, written in Python. It comes with a bunch of them,
including ones that parse markdown and other structured text formats into HTML.</p>

<p>Surprisingly I didn't need to customize Hyde much, although I did fix a couple of 
bugs, which the author, <a href="http://ringce.com/">Lakshmi Vyas</a> accepted. Anyway, I feel much more
future proof now than with Wordpress, since I can easily write extensions in
Python, or in the worst case take my Markdown-formatted blog posts and easily
export them to many different formats.</p>

<h2>Dynamic stuff</h2>

<p>The bit of custom JS for the site layout is written using jQuery.</p>

<p>I use the excellent <a href="http://softwaremaniacs.org/soft/highlight/en/">highlight.js</a> plugin to highlight code snippets. The
jQuery <a href="http://timeago.yarp.com/">timeago</a> plugin does a great job of converting absolute dates (April
1, 2010) to relative dates (about a year ago). Finally, Disqus powers comments,
which I migrated from the wordpress database.</p>

<h2>New design</h2>

<p>In my great wisdom, I decided that since there's so much flux in the blog, why not
add some more chaos by switching to a new layout too? Here's the old layout:</p>

<p><img src="/wordpress-to-hyde/old-design.png" alt="old" /></p>

<p>I recently heard from sage advice from some experienced writers. Among things
that stuck were:</p>

<ul>
<li>Optimal posting time is 9am PST</li>
<li>Writing personal posts is OK sometimes</li>
<li>Show personal information in the sidebar</li>
</ul>

<p>The last part is expressed in the new design. Overall the redesign was an
exercise in typography, CSS3 features and <a href="http://www.alistapart.com/articles/responsive-web-design/">responsive layout</a>. </p>

<p><a href="http://www.google.com/webfonts">Google fonts</a> is a nice collection of web fonts. I decided to switch from
Myriad (used on borismus.com) to a serif for the main body for a
change. I ultimately went with <a href="http://www.google.com/webfonts/list?family=PT+Serif">PT Serif</a>, which seemed like a nice
improvement to Georgia, which is probably my favorite serif web font.</p>

<p>I used a bunch (too many?) CSS3 features on the new site. Most of these are
CSS box-shadows, gradients, transformations and transitions. I used <a href="http://sass-lang.com/">SCSS</a>.</p>

<p>Also, following the responsive layout philosophy, I wanted the site to scale well
to a number of different resolutions. For example, the sidebar moves to the end of the 
articles if the window is too narrow. Also, in the <a href="/projects">projects page</a>, the number of 
columns of projects is flexible. This is achieved through <a href="http://www.w3.org/TR/css3-mediaqueries/">CSS media queries</a>.</p>

<p>For posterity, here is the new layout at the time of writing:</p>

<p><img src="/wordpress-to-hyde/new-design.png" alt="new" /></p>

<h2>New domain</h2>

<p>I also recently bought <a href="http://smus.com">smus.com</a> from a fellow in Germany, and will be migrating 
my site there. I'll keep the <a href="http://borismus.com">borismus.com</a> blog intact for a while, but will 
place an annoying banner there, and disable commenting. I guess I should also 
switch the feedburner feed to point to smus.com as well.</p>

<p>Not long ago, I wrote about how switching from webfaction and apache to slicehost
and NGINX greatly <a href="/lightweight-wordpress-on-slicehost/">enhanced page load time</a>. Well, this seems to have happened again.
Over the same time period, the average response time of my wordpress instance was 
<strong>934ms</strong>:</p>

<p><img src="/wordpress-to-hyde/old-perf.png" alt="old perf" /></p>

<p>Over the same time frame, my static blog, hosted on the same machine, responded on 
average in <strong>371ms</strong>:</p>

<p><img src="/wordpress-to-hyde/new-perf.png" alt="new perf" /></p>

<p>Oh, finally, the site is fully open source <a href="http://github.com/borismus/smus.com">on github</a>. </p>

<h2>Thanks</h2>

<p>Thanks to a bunch of people:</p>

<ul>
<li>Sol, Kat and Jon for giving useful design feedback!</li>
<li><a href="http://nlevin.com/">Noah Levin</a> for awesome CSS3 tweaks</li>
<li>Steve Losh for his <a href="http://stevelosh.com/blog/2010/01/moving-from-django-to-hyde/">Hyde migration</a> write-up</li>
</ul>

<p>You, for reading this post and continuing to read this blog. Until next time!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Chrome media keyboard shortcuts</title>
    <author><name>Boris Smus</name></author>
    <link href="/chrome-media-keys"/>
    
    <updated>2011-04-01T09:00:00-00:00</updated>
    
    <id>http://smus.com/chrome-media-keys</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Few people want to synchronize their burgeoning music libraries across
computers (SSDs are small), or even lift it into the clouds with
something like Amazon's Music Cloud (uploading would take forever). As a
result many are saying goodbye to iTunes and moving to web-based
streaming music services like <a href="http://listen.grooveshark.com/">grooveshark</a>, <a href="http://www.last.fm">last.fm</a>,
<a href="http://thesixtyone.com">thesixtyone</a>, <a href="http://rdio.com">rd.io</a> etc. This move has a significant UX drawback:
many keyboards come with multimedia keys to control your music player,
but these are useless if you use a web-based music player. This post
addresses this inconvenience with <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo/">Media Keys</a>, a Chrome extension
that lets you assign keyboard shortcuts in Chrome control web-based
music players (currently thesixtyone only).</p>

<h2>How it works</h2>

<p>The extension relies on two injected <a href="http://code.google.com/chrome/extensions/content_scripts.html">content scripts</a>: <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/key.js">key.js</a>, a
keyboard event listener injected into all pages and <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/t61.js">player.js</a>, a
music player controller, injected into the player page. </p>

<p>Key.js intercepts keyboard commands (ex. the next song key), and if they
match bound music player keyboard shortcuts (ex. next song key =>
change to the next song), it sends a message to player.js, which then
does the right thing (ex. changes to the next song) by simulating mouse
clicks.  Sounds good on paper, but the snag here is that you can't
easily do direct tab-to-tab communication in Chrome extensions, except
possibly through a long-lived port connection. However using long lived
connections doesn't make conceptual sense, since the lifespan of a tab
is relatively short. </p>

<p>So we go the long way with the help of a <a href="http://code.google.com/chrome/extensions/background_pages.html">background page</a>.</p>

<ol>
<li>On tab load, injected key.js messages the background page to
retrieve media key bindings</li>
<li>On music player load, injected player.js messages the background
page, reporting its tab ID. The background page <a href="http://code.google.com/chrome/extensions/messaging.html#connect">opens a Port</a>
through which to communicate with the player page</li>
<li>Matching keyboard events on all tabs send messages to the background
page. The background page then relays those messages through the
port to the music player</li>
</ol>

<p>Here's an abridged code sample from the <a href="https://github.com/borismus/Chrome-Media-Keys/blob/master/dispatch.html">background page</a>: </p>

<pre><code>chrome.extension.onRequest.addListener(
  function(request, sender, sendResponse) {
    switch(request.type) {
      case 'command':
        try {
          port.postMessage(request.command);
        } catch(error) {
          if (localStorage['autoload']) {
            chrome.tabs.create({url: SITE_URL});
          }
        }
        sendResponse({});
        break;
      case 'register':
        chrome.tabs.getSelected(null, function(tab) {
          port = chrome.tabs.connect(tab.id);
          sendResponse({});
        });
        break;
    }
  }
);
</code></pre>

<p>To summarize, this approach enables a client-side remote control for a
specific web application from any other page. This is potent stuff!</p>

<h2>Try it out</h2>

<p>Want to try it out? Media Keys works across all Chrome platforms.</p>

<ol>
<li>Install <a href="http://kevingessner.com/software/functionflip/">Function Flip</a> <em>(Mac only)</em></li>
<li>Check previous, pause/play and next buttons (F6, F7, F8 here) <em>(Mac
only)</em></li>
<li>Install the <a href="https://chrome.google.com/extensions/detail/cpgegiegacijlefhjkfodppcefjhgdeo">Media Keys</a> extension</li>
<li>Open up the extension options, bind the keys and save</li>
<li>Open up a new page and press the key bound to play</li>
</ol>

<p>I made a short screencast showing how to install configure and use the
extension.</p>

<iframe title="YouTube video player" width="600" height="368"
  src="http://www.youtube.com/embed/SrfsnU2gSyI" frameborder="0"
  allowfullscreen></iframe>

<h2>Wish list</h2>

<p>Just to recap, the keyboard shortcut binding pattern described above
injects a script into all tabs, which essentially listens to all key
events. A malicious developer could write a key logger watches username
and password fields, correlates to the current domain and sends
harvested data to some private server. </p>

<p>There are also several limitations to this keyboard shortcut binding
pattern. It simply won't work in the following cases:</p>

<ol>
<li>Chrome is in the background</li>
<li>Focus inside chrome is not on the page (ex. location bar)</li>
<li>Chrome is on a special page (ex. new tab page) where content scripts
don't get injected</li>
<li>The current page intercepts keyboard events and stops propagation
(ex. Google Docs)</li>
</ol>

<p>Keyboard shortcuts are super important to power users, and Chrome (OS)
surely won't leave us in the dust, so I'm looking forward to helping
address the security risks and practical limitations this approach as a
Chromium project contributor.</p>

<h2>Share and enjoy</h2>

<p>One last thing. If you've read my <a href="/chrome-extension-mashups/">previous post</a>, you know that I'm a
big fan of <a href="http://thesixtyone.com">thesixtyone.com</a> so my initial implementation
works for this service only. Making it work for other music streaming
services is just a matter of creating a customized player.js file for
your favorite music app, and tweaking the manifest to inject the new
player.js into the correct domain. Feel free to fork the 
<a href="https://github.com/borismus/Chrome-Media-Keys">project on github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Chronos: Chrome browsing metrics</title>
    <author><name>Boris Smus</name></author>
    <link href="/chronos"/>
    
    <updated>2011-03-21T09:00:00-00:00</updated>
    
    <id>http://smus.com/chronos</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Like most people, I'm slowly lifting most of my work into the cloud.
This leads to a lot of time spent in the browser. Just how much, I'm not
really sure. Enter <a href="https://chrome.google.com/extensions/detail/dbgohgmphghmoghphoiaghbopikmmgop/">Chronos</a>, a Chrome extension to track how much
time you spend on each domain you visit. Chronos gives a per-day
breakdown of time spent actively browsing. In addition to showing a
graphical summary of domain frequency, you also get a total time spent
in Chrome, and how much time your Chrome spends idle.</p>

<p>Chronos, named after the Greek god of time, quietly sits and monitors
keyboard and mouse events you generate. If you've been active recently,
the domain you're visiting gets recorded. The data structure that stores
this timing information persists on the client side using localStorage.
This data is never sent to any servers, so your browsing privacy is
preserved. Chronos' visualization is built out of HTML divs. Just for
fun, if you become inactive in Chrome, the Chronos icon in the
extensions toolbar fades out.</p>

<p>Some features that I would find useful to
add to Chronos revolve around productivity and time management:</p>

<ol>
<li>Chronos makes it really obvious which sites consume most of your
time. It would make sense to be able to enforce time limits spent on
sites, either by interfacing with an extension like <a href="https://chrome.google.com/extensions/detail/laankejkbhbdhmipfmgcngdelahlfoji">StayFocusd</a>,
or by replicating that functionality.</li>
<li>Many people find it helpful to be reminded to take breaks from
computing, either for RSI purposes or for general productivity.
Chronos already tracks activity levels in Chrome, so it could be
augmented to remind people to take breaks in a way similar to
<a href="http://tech.inhelsinki.nl/antirsi/">AntiRSI</a>.</li>
</ol>

<p>Have a great idea to add to Chronos? Let me know, or add it yourself! As
usual, the <a href="https://github.com/borismus/Chronos">source code</a> is on github.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
</feed>