<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"
      xml:lang="en"
      xml:base="https://smus.com">
  <title>Boris Smus</title>
  <link href="https://smus.com/atom.xml" rel="self"/>
  <link href="https://smus.com"/>
  <updated>2020-03-02T10:25:30-00:00</updated>
  <id>https://smus.com/atom.xml</id>
  
  <entry>
    <title>Visual Chronology of Science &amp; Discovery</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/visual-chronology-science-discovery"/>
    
    <updated>2020-02-11T09:00:00-00:00</updated>
    
    <id>https://smus.com/visual-chronology-science-discovery</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>As Newton wrote, “If I have seen further it is by standing on the shoulders of giants”. But whose giant shoulders did Newton stand on? And did those giants stand on the shoulders of other giants? And how about Newton’s successors, or people working in other fields? As far as I can tell, it’s giants all the way down.</p>

<p>Last year, I got my hands on a remarkable book, <a href="/books/asimovs-chronology-of-science-and-discovery/">Asimov’s Chronology of Science and Discovery</a>. It inspired me to produce a visual summary of human ingenuity, to see what one giant saw from the shoulders of another. After some experimentation, I turned it into an interactive visualization. You can play with it <a href="https://borismus.github.io/asimov/web/cross-shape#steel">here</a>:</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape/#steel"><img src="/visual-chronology-science-discovery/screenshot-steel.jpg" alt="Screenshot of the visual chronology centered at Steel." /></a></p>

<!--more-->

<p>One of the unnerving things about Isaac Asimov’s book was the visual table of contents for the first edition:</p>

<p><img src="/visual-chronology-science-discovery/original-visualization.jpg" alt="Winding visualization of Asimov’s Chronology in the first edition." /></p>

<p>Following this winding path is tricky to say the least. With this in mind, I carefully read the first quarter of the book, spanning from the beginning of time until 1700 CE, which amounts to about 300 entries, or about a quarter of those contained in the book. I took notes in a spreadsheet, providing a 1-2 sentence summary of each entry and manually extracting some key metadata. For each of Asimov’s entries, I captured the title, a couple sentences of description, the associated person’s name, and where the invention was created or discovery found, usually the country or empire.</p>

<p>In addition, I provided two extra fields which are more subjective and frankly, made up by yours truly, to bring the visualization to life: </p>

<ol>
<li><strong>Field</strong>: which domain was this discovery made. (How granular should this be? Should Science be split up into Chemistry, Physics, and Biology?)</li>
<li><strong>Dependencies</strong>: what older inventions and discoveries enabled this one. (This can be really tricky.)</li>
</ol>

<p>With this in place, Asimov’s linear chronology becomes a directed graph, and since we don’t know how to time travel, there are no time cycles. So what we have is a <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graph (DAG)</a>. It is not a tree since I allow each node to have multiple parents. For instance, the field of Geometry is built up logically from axioms. At the same time, the founding text of the field, Euclid’s Elements, is mostly derived work from many mathematicians working before Euclid in the Academy and Lyceum. So the DAG shows <strong>Geometry</strong> rooted both in <strong>Advanced Schools</strong> and <strong>Logic</strong>.</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape/#geometry"><img src="/visual-chronology-science-discovery/screenshot-geometry.jpg" alt="Screenshot of the visual chronology centered at Geometry." /></a></p>

<h2>Manual entry is tiring</h2>

<p>Sadly, the Chronology is not available as a text-based ebook, which complicates matters. As it turns out, optical character recognition (OCR) is not easy if your content is laid out like a newspaper:</p>

<p><img src="/visual-chronology-science-discovery/book-flow-ocr.jpg" alt="Excerpt of the book showing tricky text flow" /></p>

<p>Google Cloud Vision and <a href="https://github.com/tesseract-ocr/tesseract">tesseract</a> don’t do well on complex text flows, often failing to recognize section boundaries. For the image above, tesseract wrongly assumes that that the text flows in three full height columns.</p>

<p>I also tried to do automatic entity extraction using python’s <code>nltk</code> and <code>ne_chunk</code> to automatically generate inventors and locations but a quick experiment yielded too many entities for each entry, meaning that I’d have to make a manual pass regardless, and this step would provide minimal time savings.</p>

<h2>Playing cards and tech trees</h2>

<p>Each invention and discovery needs its own view. I gravitated to a card metaphor, which is often used in historical strategy games like Civ, role playing games like Diablo, and in many playing card games like Magic the Gathering. Here are a <a href="https://www.are.na/boris-smus/tech-tree-cards">few examples</a> that inspired me.</p>

<p>One challenge that I found ultimately insurmountable was to find good images for each entry. While you can find reams of royalty free images, and even download them automatically using tools like <a href="https://github.com/hardikvasa/google-images-download">googleimagesdownload</a>, finding a set that is visually consistent was tough. I did this manually for the first hundred inventions. Theoretically a style transfer model might be able to convert them all to a consistent look. But ultimately, they didn’t add much value to the visualization as a whole, so I ditched the idea.</p>

<p><img src="/visual-chronology-science-discovery/card-images.jpg" alt="Attempt to use images in the cards" /></p>

<h2>Chronological visualization</h2>

<p>To visualize the entries, I started with a naive approach: render all of the entries at once in a giant horizontal scrolling view, kind of like how tech trees work in Civilization-like games. Doing such a thing linearly makes no sense at all, since the first entry in the book is Bipedalis, dating to 4 million years ago (ma), followed by Stone Tools at 2 ma, Fire at 0.5 ma, then 8 entries later, Agriculture at 0.01 ma (10,000 years ago). This sort of timeline is best represented on a log-scale, which makes entries more reasonably spaced out, although still not perfect. Actually there’s quite a lot of variation in density. For example, this is the chronology of the first millennium CE (1000 years):</p>

<p><img src="/visual-chronology-science-discovery/chrono-first-millennium.jpg" alt="Chronology of first millennium" /></p>

<p>While this is just the 16th century (100 years, an order of magnitude shorter than above):</p>

<p><img src="/visual-chronology-science-discovery/chrono-sixteenth-century.jpg" alt="Chronology of 16th century" /></p>

<p>Even with the log-scale, inventions are by no means well distributed in time, with the 16th century far more visually dense than the first millennium. In both examples, links are especially obscure, since they often stretch out for many screens, and are basically impossible to trace from source to destination. </p>

<p>I tried variants of this view as well, involving collapsing entries into more compact default representations, and allowing them to be expanded for more detail. One promising variant involved expanding a selected entry and all transitively linked ones, but the predecessors and successors are still typically positioned far off-screen, so a lot of scrolling is required.</p>

<h2>Entry-centric visualization</h2>

<p>One of the shortcomings of a purely chronological view is that connections between entries are lost. Yet this is the most important part of this whole project, so I kept searching.</p>

<p>I opted for a more structured approach, one that lets you focus on a particular entry and at a glance see what technology led to it, and what technology it enabled. At the same time, I wanted to show the invention in its chronological context, in the spirit of Asimov’s book. So the current design takes a Cartesian approach, with one card centered at the origin, serving as the focus. Technologies that enabled the focused card are shown to the left and technologies that the focus enabled are shown to the right, along the x axis. Chronologically previous and next entries are shown along the y axis. </p>

<h2>Technology transcending fields</h2>

<p>Once the dependency graph is in place and each entry is associated with a field, one starts seeing interesting patterns in the data. </p>

<p>The discovery of magnetism lead to the invention of the compass, giving navigators confidence to traverse the ocean, in turn leading to the discovery of the new world:</p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape#compass"><img src="/visual-chronology-science-discovery/screenshot-compass.jpg" alt="Screenshot of the visual chronology centered at Compass." /></a></p>

<p>Melting glass beads and glass blowing enabled lens crafting and microscopes which in turn let careful observers see microorganisms in pond water, ultimately leading to great advances in medicine. </p>

<p><a href="https://borismus.github.io/asimov/web/cross-shape#microscope"><img src="/visual-chronology-science-discovery/screenshot-microscope.jpg" alt="Screenshot of the visual chronology centered at Microscope." /></a></p>

<p>Cross field dependencies like the ones I described above are visualized as dashed red arrows.</p>

<h2>Please help complete this project</h2>

<p>This visualization is backed by a spreadsheet which is <a href="https://docs.google.com/spreadsheets/d/1hDNXas7DzwglB95HV2_2u1utWAwBZR2hQHlMPz-fj5A/edit#gid=0">publicly viewable</a>, the result of a bunch of my own reading and summarizing. I’ve compiled a quarter of Asimov’s impressive tome by hand, but I’d love your help finishing the project.</p>

<p>If you’re game to help, here are the necessary pieces:</p>

<ol>
<li>A <a href="https://drive.google.com/file/d/190RDAxrUzu5m0d_zxQi98euIguBDb0qf/view?usp=sharing">PDF of the second edition of Asimov’s Chronology</a> I scanned in hopes of automating the whole process (see above).</li>
<li>A <a href="https://docs.google.com/spreadsheets/d/1uDeBCfcaVUfZFEK-0WJIb43dT6cqHHq9o6Uxn6PihLY/edit#gid=158368026">publicly editable spreadsheet</a> containing stubs for all of the inventions listed in the chronology from 1700 to 1993.</li>
</ol>

<p>Please read a chunk of the latter quarter of the book (starting 1700) and contribute it to the public spreadsheet. I hope you will be as excited as I was to learn a whole lot about the history of science and compile it into this format. Once the project is complete, it will be glorious! </p>

<p>I’ll update this post as more of the spreadsheet is filled out. Meanwhile, thanks for reading and stay tuned – I have a few follow up posts in mind already.</p>

<p><strong>Update Feb 23</strong>: People seem to be interested in the project; excellent! I’ve <a href="https://github.com/borismus/asimov">open sourced</a> the visualization code.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Books highlights of 2019</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2019/books-highlights-of-2019/"/>
    
    <updated>2019-12-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2019/books-highlights-of-2019/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I’ve been reading and summarizing for five years now (cue Ratatat). I’m not sure that it’s helped much with retention, but at the very least, having an easily searchable corpus of my book-related notes is worthwhile.</p>

<p>A decent year for fiction, I finally read <a href="/books/invisible-cities-by-italo-calvino/">Invisible Cities</a>, which I enjoyed not viscerally, but more as an art piece. I liked <a href="/books/the-fiddler-is-a-good-woman/">The Fiddler is a Good Woman</a> for the author’s ability to retell the same sordid tale from multiple perspectives. <a href="http://localhost:5500/books/diamond-age-by-stephenson-audio/">Diamond Age</a> was a collection of incredible ideas and surprisingly relevant for my job. I also enjoyed <a href="/books/hyperion-by-dan-simmons-audio/">Hyperion</a>, especially Sol Weintraub’s tale, which touched me deeply. <a href="/books/the-fifth-season-by-n-k-jemisin/">The Fifth Season</a> was interesting, but I have some identitarian reservations. I think my overall fiction highlight was <a href="/books/the-odyssey-translated-by-emily-wilson-audio/">Homer’s Odyssey translated by Emily Wilson</a>.</p>

<p>I read two non-fiction books which were a little too extreme for my tastes: <a href="/books/deschooling-society-by-ivan-illich">Deschooling Society</a>, which just read like an angry and ill considered screed and <a href="/books/radical-markets-by-posner-and-weyl/">Radical Markets</a>, which had some really interesting ideas, but presented as a whole seemed ridiculous. I really enjoyed <a href="/books/timefulness-by-marcia-bjornerud-audio/">Timefulness</a>, a whirlwind overview of geology, and <a href="/books/range-by-david-epstein-audio/">Range</a>, an ode to generalists. The standout non-fiction for me was <a href="/books/seeing-like-a-state-by-james-scott/">Seeing Like a State</a>, which will continue to turn over in my mind for years to come. I listened to fewer history lectures than usual, but can wholeheartedly recommend <a href="/books/the-story-of-medieval-england-audio/">The Story of Medieval England</a>. Instead, I focused on parenting books, which are pretty bad as a genre. That said, <a href="/books/the-new-father/">The New Father</a> served as a worthwhile companion to skim during the first year of Eliana’s life, and <a href="/books/yes-brain-by-siegel-and-bryson/">Yes Brain</a> had practical suggestions for digesting mindfulness into something that small children might be able to understand.</p>

<p>I also begun a new long-term book summarization project, which is to digest <a href="">Asimov's Chronology of Science and Discovery</a>, and visualize it as a Civ-style tech tree. Hopefully I’ll have something to report on that front in 2020!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Article highlights of 2019</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2019/article-highlights-of-2019/"/>
    
    <updated>2019-12-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2019/article-highlights-of-2019/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I’m still using Instapaper for reading online things, although in 2020, my aim is to read articles (even) less, in favor of books.</p>

<ul>
<li>Try to install a radar system with one job: Searching for chances to give a compliment. <a href="https://www.psychologytoday.com/us/blog/everybody-is-stupid-except-you/201812/compliments-how-get-happy-and-make-better-world">Compliments: How to Get Happy and Make a Better World</a></li>
<li>Warren Buffett’s metric of success is “Do the people you care about love you back?” <a href="https://www.gatesnotes.com/About-Bill-Gates/Year-in-Review-2018">What I learned at work this year</a></li>
<li>As Edelman points out, if YouTube really cared about our intentions and values, when we logged on to learn ukulele it would try to serve that need — and then send us off to practice! <a href="https://time.com/5505431/internet-feel-awful-dignity/">The Internet Can Make Us Feel Awful. It Doesn't Have to Be That Way</a></li>
<li>Wisdom as the ultimate virtue. Really solid framing in favor of centrism. <a href="https://www.shanesnow.com/articles/values/">How to be good: A theory about Virtues &amp; Values</a>
\<!--more--></li>
<li>From 1200 to 1745, 21-year-olds would reach an average age of anywhere between 62 and 70 years. <a href="https://www.bbc.com/future/article/20181002-how-long-did-ancient-people-live-life-span-versus-longevity">Do we really live longer than our ancestors?</a></li>
<li>Nick Cave isn’t just an awesome musician, but is also quite eloquent. “What we are actually listening to is human limitation and the audacity to transcend it.” <a href="https://www.brainpickings.org/2019/01/24/nick-cave-music-ai/">Music, Feeling, and Transcendence: Nick Cave on AI, Awe, and the Splendor of Our Human Limitations</a></li>
<li>Seismic air guns are widely used to map the sea floor. They are incredibly loud and disruptive to ocean fauna. <a href="https://www.nytimes.com/2019/01/22/science/oceans-whales-noise-offshore-drilling.html">Oceans Are Getting Louder, Posing Potential Threats to Marine Life</a></li>
<li>Unlike traditional two-by-four lumber, cross-laminated timber (CLT) consists of layers of wood glued together to form solid, thick panels that can be made in custom dimensions for anything from walls and floors to beams and roofs. CLT is being used to build mid-rise buildings, including big swaths of Google Sidewalk in Toronto. <a href="https://www.financialexpress.com/industry/skyscrapers-made-of-wood-make-a-return-nudged-by-google/1482316/">Skyscrapers made of wood make a return, nudged by google</a></li>
<li>Peter Turchin: “the way forward to sustaining and increasing the well-being of large segments of population is not to abolish government, but to evolve institutions that keep bureaucrats working for the benefit of the population, rather than themselves.” <a href="http://peterturchin.com/cliodynamica/an-anarchist-view-of-human-social-evolution/">An Anarchist View of Human Social Evolution</a></li>
<li>The scary thing about GPT-2-generated text is that it flows very naturally if you’re just skimming. <a href="https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/">Humans Who Are Not Concentrating Are Not General Intelligences</a></li>
<li>I disagree with Stratechery that the web is better today than it was when we were punching monkeys, but he writes terrifyingly about Spotify’s role in today’s podcast marketplace. <a href="https://stratechery.com/2019/spotifys-podcast-aggregation-play/">Spotify’s Podcast Aggregation Play</a></li>
<li>A stark look at the rising costs of digital goods and their implications for working poor. <a href="https://reallifemag.com/well-played-store-credit/">Video games are teaching users how to enjoy perpetually renting consumer goods rather than owning them</a></li>
<li>I thought the Albedo effect only applied to melting ice caps, but as it turns out, it also applies to clouds! <a href="https://www.quantamagazine.org/cloud-loss-could-add-8-degrees-to-global-warming-20190225/">A World Without Clouds</a></li>
<li>What came first, the moralizing God or the complex society? Evidence points to the latter, followed by Gods-as-divine-policemen. <a href="https://gizmodo.com/humans-built-complex-societies-before-they-invented-mor-1833436320">Humans Built Complex Societies Before They Invented Moral Gods</a></li>
<li>The American public is broadly pessimistic about its country’s future, and this pessimism correlates with education level. <a href="https://www.pewsocialtrends.org/2019/03/21/public-sees-an-america-in-decline-on-many-fronts/">Looking to the Future, Public Sees an America in Decline on Many Fronts</a></li>
<li>As wealthy kids are growing up with less screen time, poor kids are growing up with more. <a href="https://www.nytimes.com/2019/03/23/sunday-review/human-contact-luxury-screens.html">Human Contact Is Now a Luxury Good</a></li>
<li>Stop building to ship until you’ve first built to learn. <a href="https://uxdesign.cc/building-to-learn-977a8cd88ced?">https://uxdesign.cc/building-to-learn-977a8cd88ced?</a></li>
<li>A charitable look at Post-modernism holds that there is no privileged lens with which to view the world; that even empiricism is suspect, because it too has a tendency to reproduce and reify the power structures in which in exists. <a href="https://socratic-form-microscopy.com/2019/04/03/post-modernism-and-political-diversity/">https://socratic-form-microscopy.com/2019/04/03/post-modernism-and-political-diversity/</a></li>
<li>Texas produces more wind power than every other state in the country, four times as much as the runner-up, California. <a href="https://www.nytimes.com/interactive/2019/04/11/magazine/climate-change-exxon-renewable-energy.html?mtrref=www.instapaper.com&amp;gwh=A83B2941A03E83BDCAE36DF6FD0D3189&amp;gwt=pay&amp;assetType=REGIWALL">https://www.nytimes.com/interactive/2019/04/11/magazine/climate-change-exxon-renewable-energy.html?mtrref=www.instapaper.com&amp;gwh=A83B2941A03E83BDCAE36DF6FD0D3189&amp;gwt=pay&amp;assetType=REGIWALL</a></li>
<li>While I disagree that there may be a difference between personal and professional values, I found this brief article to be a useful exercise. <a href="https://blog.alexmaccaw.com/a-value-driven-life">https://blog.alexmaccaw.com/a-value-driven-life</a></li>
<li>Formidable pays employees $20/hr for contributions (outside of normal work time) to OSS and tech communities or even a personal hack project purely for fun and learning, as long as it’s released under an OSI license. <a href="https://formidable.com/blog/2019/sauce-program/">https://formidable.com/blog/2019/sauce-program/</a></li>
<li>Look at how everyone else is doing it (the likely local maxima) and then find a better way. <a href="https://www.nateliason.com/blog/local-maxima">https://www.nateliason.com/blog/local-maxima</a></li>
<li>A look into some of Duolingo’s limitations. Learning a language with an app should be a starting point, not the end.<a href="https://www.nytimes.com/2019/05/04/smarter-living/500-days-of-duolingo-what-you-can-and-cant-learn-from-a-language-app.html">https://www.nytimes.com/2019/05/04/smarter-living/500-days-of-duolingo-what-you-can-and-cant-learn-from-a-language-app.html</a></li>
<li>A Facebook co-founder wants the US government to fix Facebook. <a href="https://www.nytimes.com/2019/05/09/opinion/sunday/chris-hughes-facebook-zuckerberg.html">https://www.nytimes.com/2019/05/09/opinion/sunday/chris-hughes-facebook-zuckerberg.html</a></li>
<li>How might we design mediums in which “reading” is the same as “understanding”? <a href="https://andymatuschak.org/books/">https://andymatuschak.org/books/</a></li>
<li>Safe playgrounds lead to boredom, which tends toward rash stunts like turning somersaults on top of climbing frames and standing on the shoulders of others on the swings. So let’s make playgrounds a little more dangerous! <a href="https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html">https://www.nytimes.com/2019/05/10/well/family/adventure-playgrounds-junk-playgrounds.html</a></li>
<li>What Republicans want to do with I.C.E. and border walls, wealthy progressive Democrats are doing with zoning and NIMBYism. Ouch. <a href="https://www.nytimes.com/2019/05/22/opinion/california-housing-nimby.html">https://www.nytimes.com/2019/05/22/opinion/california-housing-nimby.html</a></li>
<li>We were certain that more communication would make everything better. Arrogantly, we ignored history and learned a lesson that has been in the curriculum since the Tower of Babel. <a href="https://www.wired.com/story/why-we-love-tech-defense-difficult-industry/">https://www.wired.com/story/why-we-love-tech-defense-difficult-industry/</a></li>
<li>What are signs that a player is alive? One strong sign is a player doing things outside of their domain. For example, Putin is a live player. The Russian state is doing things they haven’t done in a long time, like annexing Crimea… <a href="https://medium.com/@samo.burja/live-versus-dead-players-2b24f6e9eae2">https://medium.com/@samo.burja/live-versus-dead-players-2b24f6e9eae2</a></li>
<li>San Francisco is a stunningly beautiful and continuously innovative city, cursed to always be considered one or two generations past its peak, like a rowboat chasing the horizon. <a href="https://www.sfchronicle.com/oursf/article/San-Francisco-isn-t-what-it-used-to-be-13901994.php">https://www.sfchronicle.com/oursf/article/San-Francisco-isn-t-what-it-used-to-be-13901994.php</a></li>
<li>The hallmark of sociological storytelling is if it can encourage us to put ourselves in the place of any character. GoT used to be like that, but then turned into a story about heroes. <a href="https://blogs.scientificamerican.com/observations/the-real-reason-fans-hate-the-last-season-of-game-of-thrones/">https://blogs.scientificamerican.com/observations/the-real-reason-fans-hate-the-last-season-of-game-of-thrones/</a></li>
<li>An attempt to integrate cybernetics, counterculture, computing, and design. <a href="http://www.dubberly.com/articles/cybernetics-and-counterculture.html">http://www.dubberly.com/articles/cybernetics-and-counterculture.html</a></li>
<li>If AI were a great storyteller, it would be an alien bard who has studied our planet from orbit and still doesn’t get it; but the tales it tells embarrass us with their insights all the same. <a href="https://emshort.blog/2019/04/04/can-ai-tell-a-good-story/">https://emshort.blog/2019/04/04/can-ai-tell-a-good-story/</a></li>
<li>Quadratic Voting is too complicated for the average person, but the idea of having a finite number of tokens to distribute amongst a set of important issues is a good one. <a href="https://lawreview.uchicago.edu/publication/qv-or-not-qv-question-some-skepticism-about-radical-egalitarian-voting-markets">https://lawreview.uchicago.edu/publication/qv-or-not-qv-question-some-skepticism-about-radical-egalitarian-voting-markets</a></li>
<li>A whole, unmilled tree can support 50 percent more weight than the largest piece of lumber milled from the same tree. <a href="https://www.nytimes.com/2009/11/05/garden/05tree.html">https://www.nytimes.com/2009/11/05/garden/05tree.html</a></li>
<li>Where do ideas come from? <a href="https://nadiaeghbal.com/ideas">https://nadiaeghbal.com/ideas</a></li>
<li>Cliff Mass correctly predicted that we’d have fewer wildfires in the PNW, great example of regression towards the mean. <a href="https://cliffmass.blogspot.com/2019/05/will-puget-sound-experience-another.html">https://cliffmass.blogspot.com/2019/05/will-puget-sound-experience-another.html</a></li>
<li>Bungalows in places like Wallingford and Ballard that many now find charming were widely criticized as cheap when they began popping up in the early part of the 20th century. Who knows how Seattle’s modern builds will age? <a href="https://www.seattletimes.com/business/real-estate/a-teardown-a-day-bulldozing-the-way-for-bigger-homes-in-seattle-suburbs/">https://www.seattletimes.com/business/real-estate/a-teardown-a-day-bulldozing-the-way-for-bigger-homes-in-seattle-suburbs/</a></li>
<li>That skyscraper in Vancouver prominently seen from Granville bridge (northbound) is also designed by big.dk. <a href="https://www.dezeen.com/2018/11/30/movie-big-heatherwick-google-hq-progress/">https://www.dezeen.com/2018/11/30/movie-big-heatherwick-google-hq-progress/</a></li>
<li>Half of parents are fathers, yet 99% of the research on parenting focuses on mothers. <a href="https://www.bbc.com/future/article/20190606-how-to-be-a-good-father-to-a-newborn-son-or-daughter">https://www.bbc.com/future/article/20190606-how-to-be-a-good-father-to-a-newborn-son-or-daughter</a></li>
<li>Massive migration of G+ accounts from Google accounts gave people the impression G+ was a “ghost town.” <a href="https://onezero.medium.com/why-google-failed-4b9db05b973b">https://onezero.medium.com/why-google-failed-4b9db05b973b</a></li>
<li>Identify and avoid cargo cults in design thinking. <a href="https://medium.com/designing-atlassian/when-design-becomes-a-cargo-cult-2bb9a50aad53">https://medium.com/designing-atlassian/when-design-becomes-a-cargo-cult-2bb9a50aad53</a></li>
<li>80% of the aviation industry's emissions come from passenger flights longer than 1,500km - a distance no electric airliner could yet fly. <a href="https://www.bbc.com/news/business-48630656">https://www.bbc.com/news/business-48630656</a></li>
<li>‘Sleeping beauties’ are papers which didn’t have much influence early but come to be appreciated and important later. <a href="https://news.uchicago.edu/story/new-model-reveals-forgotten-influencers-and-sleeping-beauties-science">https://news.uchicago.edu/story/new-model-reveals-forgotten-influencers-and-sleeping-beauties-science</a></li>
<li>Our superpower is not raw intelligence but our ability to learn through imitation. Aggregated over generations, this becomes culture.<a href="https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/">https://slatestarcodex.com/2019/06/04/book-review-the-secret-of-our-success/</a></li>
<li>Banned about 70 years ago in Seattle, parent-in-law apartments were again allowed across the city starting in 1994 and backyard cottages since 2010. <a href="https://www.seattletimes.com/seattle-news/politics/seattle-city-council-oks-more-and-larger-backyard-cottages-restricts-mcmansions/">https://www.seattletimes.com/seattle-news/politics/seattle-city-council-oks-more-and-larger-backyard-cottages-restricts-mcmansions/</a></li>
<li>The Baumol effect: “A violinist can always choose to stop playing violin, retrain for a while, and work in a factory instead. Maybe in 1826, when factory owners were earning $1.14/hour and violinists were earning $5/hour, so no violinists would quit and retrain. But by 2010, factory workers were earning $26.44/hour, so if violinists were still only earning $5 they might all quit and retrain.” <a href="https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/">https://slatestarcodex.com/2019/06/10/book-review-the-prices-are-too-dmn-high/</a></li>
<li>Mistake theorists treat politics as science, engineering, or medicine. The State is diseased. Conflict theorists treat politics as war. Different blocs with different interests. <a href="https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/">https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/</a></li>
<li>It’s hard to separate the threat that technology poses from the threat that capitalism poses. <a href="https://www.gq.com/story/ted-chiang-exhalation-interview">https://www.gq.com/story/ted-chiang-exhalation-interview</a></li>
<li>Convenience is a value, and one we hold personally. Maybe it’s worth reconsidering this. <a href="https://onezero.medium.com/the-tyranny-of-convenience-2e7fa145ab4">https://onezero.medium.com/the-tyranny-of-convenience-2e7fa145ab4</a></li>
<li>We don’t want hectoring morality tales, as evidenced by The Odyssey’s exceeding popularity compared to Plato’s Republic. We want stories that let us be other characters: strong and cunning, defiant or afraid, good or bad or somewhere in between. <a href="https://putanumonit.com/2019/06/25/playstation-odysseys/">https://putanumonit.com/2019/06/25/playstation-odysseys/</a></li>
<li>A scene with active technological evolution has what musician Brian Eno called scenius. <a href="https://breakingsmart.substack.com/p/following-the-scenius">https://breakingsmart.substack.com/p/following-the-scenius</a></li>
<li>The Antidisciplinarathon is a compelling format to quickly broaden one’s purview, ramp up on new fields, and meet new interesting people. <a href="http://hypotext.co/antidisciplinarathon">http://hypotext.co/antidisciplinarathon</a></li>
<li>The vast resources of Silicon Valley have too often been applied to the problem of “what is my mother no longer doing for me?” <a href="https://slate.com/culture/2019/06/life-hacking-productivity-tech-silicon-valley-hacking-life-book-review.html">https://slate.com/culture/2019/06/life-hacking-productivity-tech-silicon-valley-hacking-life-book-review.html</a></li>
<li>AFAICT, Deinstitutionalisation => Homelessness. <a href="https://www.npr.org/2019/07/12/736612514/seattle-faces-backlash-after-easing-up-on-punishing-crimes-involving-mental-illn">https://www.npr.org/2019/07/12/736612514/seattle-faces-backlash-after-easing-up-on-punishing-crimes-involving-mental-illn</a></li>
<li>We would like to think there is some quick hack, or that we can learn something with it feeling like a simple game, but the true is that real learning is hard work and frustrating. <a href="https://medium.com/@gjdickens/distraction-disguised-as-learning-duolingo-codeacademy-and-why-you-should-get-back-to-basics-99a22a9e2c33">https://medium.com/@gjdickens/distraction-disguised-as-learning-duolingo-codeacademy-and-why-you-should-get-back-to-basics-99a22a9e2c33</a></li>
<li>China seems unlikely to do personalized learning well, but appear to be far ahead in many aspects of optimizing education. <a href="https://www.technologyreview.com/s/614057/china-squirrel-has-started-a-grand-experiment-in-ai-education-it-could-reshape-how-the/">https://www.technologyreview.com/s/614057/china-squirrel-has-started-a-grand-experiment-in-ai-education-it-could-reshape-how-the/</a></li>
<li>Some studies seem to indicate that not only do students retain less when reading digitally, they’re more likely to overestimate how well they comprehended the material. <a href="https://www.wired.com/story/digital-textbooks-radical-transformation/">https://www.wired.com/story/digital-textbooks-radical-transformation/</a></li>
<li>Because the 737 has kept the same name for 50 years, the FAA certification of the fourth-generation 737 in 2017 involved only light-touch approval. The only really rigorous test-result underpinnings were those of the 1967 machine. <a href="https://www.edge.org/conversation/timothy_taylor-polythetics-and-the-boeing-737-max">https://www.edge.org/conversation/timothy_taylor-polythetics-and-the-boeing-737-max</a></li>
<li>Criticize the existence of billionaires in general, their spending on yachts or mansions. But if you only criticize billionaires when they’re trying to save lives, you risk collateral damage to everything we care about. <a href="https://slatestarcodex.com/2019/07/29/against-against-billionaire-philanthropy/">https://slatestarcodex.com/2019/07/29/against-against-billionaire-philanthropy/</a></li>
<li>A resurrected Franklin wouldn’t have a news job inside The Washington Post; he’d have an anonymous Twitter account with a huge following that he’d use to routinely troll political opponents. <a href="https://www.wired.com/story/journalism-isnt-dying-its-returning-its-roots/">https://www.wired.com/story/journalism-isnt-dying-its-returning-its-roots/</a></li>
<li>Tech workers might not realize that their opposition to the work their companies do on military technology does not change the decision-making of the American leaders who choose to go to war. <a href="https://www.nytimes.com/2019/08/28/opinion/military-war-tech-us.html">https://www.nytimes.com/2019/08/28/opinion/military-war-tech-us.html</a></li>
<li>AI-writer centaurs are super interesting. <a href="https://www.vox.com/future-perfect/2019/8/30/20840194/ai-art-fiction-writing-language-gpt-2">https://www.vox.com/future-perfect/2019/8/30/20840194/ai-art-fiction-writing-language-gpt-2</a></li>
<li>“Isn’t school for learning math and science and reading,” he asked us one day, “not for teachers to tell us what to think about society?” <a href="https://www.theatlantic.com/magazine/archive/2019/10/when-the-culture-war-comes-for-the-kids/596668/">https://www.theatlantic.com/magazine/archive/2019/10/when-the-culture-war-comes-for-the-kids/596668/</a></li>
<li>Study after study has shown open offices to foster seclusion more than innovation. People in an open office put on headphones, talk less, and feel terrible. <a href="https://www.nytimes.com/2019/09/25/opinion/wework-adam-neumann.html">https://www.nytimes.com/2019/09/25/opinion/wework-adam-neumann.html</a></li>
<li>Great summary of arguments in favor of and also against Sortition - the system whereby members of representative bodies are picked at random, like jurors.<a href="http://edmundgriffiths.com/sortition.html">http://edmundgriffiths.com/sortition.html</a></li>
<li>Rather than maintaining my own lawn, much better to have a smaller garden and live right beside a great park? <a href="https://earther.gizmodo.com/lawns-are-an-ecological-disaster-1826070720">https://earther.gizmodo.com/lawns-are-an-ecological-disaster-1826070720</a></li>
<li>We’re not merely lighting a match to the Amazon and imperiling everything that lives in it with extinction, but also summoning creatures long dead to return to Earth’s surface and give up the ancient energy they took to the grave. <a href="https://www.theatlantic.com/science/archive/2019/08/amazon-fire-earth-has-plenty-oxygen/596923/">https://www.theatlantic.com/science/archive/2019/08/amazon-fire-earth-has-plenty-oxygen/596923/</a></li>
<li>In the last 20 years, some Inuit communities who had never seen robins before have had to invent a name for them: “Koyapigaktoruk.” <a href="https://earthsky.org/earth/decoding-climate-change-signals-arctic-treeline-tundra-alaska">https://earthsky.org/earth/decoding-climate-change-signals-arctic-treeline-tundra-alaska</a></li>
<li>Yedoma consists of thick layers of soil packed around gigantic lodes of embedded ice. Because Yedoma contains so much ice, it can melt quickly — reshaping the landscape as sudden lakes form and hillsides collapse. <a href="https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-siberia/">https://www.washingtonpost.com/graphics/2019/national/climate-environment/climate-change-siberia/</a></li>
<li>Wikis are just editable web pages; Twitter is just a way of sharing very short form writing; and Facebook is just a way of sharing writing and pictures with friends. Indeed, writing itself is just a clever way of ordering a small number of symbols on a page. While a medium may be simple, that doesn’t mean it’s not profound. <a href="https://numinous.productions/ttft/">https://numinous.productions/ttft/</a></li>
<li>For middle-class and poor families, the picture is different. Federal income taxes have also declined modestly for these families, but they haven’t benefited much if at all from the decline in the corporate tax or estate tax. <a href="https://www.nytimes.com/interactive/2019/10/06/opinion/income-tax-rate-wealthy.html">https://www.nytimes.com/interactive/2019/10/06/opinion/income-tax-rate-wealthy.html</a></li>
<li>The idea of the Anthropocene inflates our own importance by promising eternal geological life to our creations. It is of a thread with our species’ peculiar, self-styled exceptionalism. <a href="https://www.theatlantic.com/science/archive/2019/08/arrogance-anthropocene/595795/">https://www.theatlantic.com/science/archive/2019/08/arrogance-anthropocene/595795/</a></li>
<li>To Western ears, the combination of C and F# is grating, but Tsimane listeners rated this chord just as likeable as other chords that Westerners would interpret as more pleasant, such as C and G. <a href="https://cosmosmagazine.com/biology/perception-of-musical-pitch-varies-across-cultures">https://cosmosmagazine.com/biology/perception-of-musical-pitch-varies-across-cultures</a></li>
<li>It doesn’t seem too soon to ask: Is the vision of the VR or AR headset as a widespread consumer device just a dead branch hanging off technology’s evolutionary tree? <a href="https://www.fastcompany.com/90418190/the-one-thing-google-didnt-talk-about-at-its-keynote">https://www.fastcompany.com/90418190/the-one-thing-google-didnt-talk-about-at-its-keynote</a></li>
<li>Some examples of people quickly accomplishing ambitious things together. <a href="https://patrickcollison.com/fast">https://patrickcollison.com/fast</a></li>
<li>On-device AI could be clutch for detecting illegal logging. <a href="https://www.nytimes.com/2019/10/15/climate/indonesia-logging-deforestation.html">https://www.nytimes.com/2019/10/15/climate/indonesia-logging-deforestation.html</a></li>
<li>Technology that’s Actually Yours could be the next great counter-trend. <a href="https://alexdanco.com/2019/10/26/everything-is-amazing-but-nothing-is-ours/">https://alexdanco.com/2019/10/26/everything-is-amazing-but-nothing-is-ours/</a></li>
<li>Utrecht has a bike garage with sensors on the racks, making finding a free spot during rush hour much easier. <a href="https://www.nytimes.com/2017/09/06/world/europe/bicycling-utrecht-dutch-love-bikes-worlds-largest-bike-parking-garages.html">https://www.nytimes.com/2017/09/06/world/europe/bicycling-utrecht-dutch-love-bikes-worlds-largest-bike-parking-garages.html</a></li>
<li>By pushing a highly political agenda the Anxious, Social-Justice, Partisan (ASP) movement is undermining bipartisan efforts--and nothing important will be done unless both sides of the aisle are involved. <a href="https://cliffmass.blogspot.com/2019/10/the-real-climate-debate.html">https://cliffmass.blogspot.com/2019/10/the-real-climate-debate.html</a></li>
<li>The party launched the app, called Study the Great Nation, containing articles and videos about Xi’s activities or his ideology, “Xi Jinping Thought.” There is even a sense of competition, with users earning points for reading articles and commenting on them, and a leader board showing how users are faring in quizzes. <a href="https://www.washingtonpost.com/world/asia_pacific/chinese-app-on-xis-ideology-allows-data-access-to-100-million-users-phones-report-says/2019/10/11/2d53bbae-eb4d-11e9-bafb-da248f8d5734_story.html">https://www.washingtonpost.com/world/asia_pacific/chinese-app-on-xis-ideology-allows-data-access-to-100-million-users-phones-report-says/2019/10/11/2d53bbae-eb4d-11e9-bafb-da248f8d5734_story.html</a></li>
<li>Younger Palestinians were interested (in improving relations with Israel), but that there were none well-established enough in their careers yet to withstand the blowback. <a href="https://www.nytimes.com/2019/11/20/us/israel-arab-dialog.html">https://www.nytimes.com/2019/11/20/us/israel-arab-dialog.html</a></li>
<li>Clustering meetings and scheduling them near breaks is ideal. One of the best times is near the end of the day, or right before lunch. <a href="http://www.paulgraham.com/makersschedule.html">http://www.paulgraham.com/makersschedule.html</a></li>
<li>Terrifying: ISPs would recognize the obsolescence of the internet and support the Trinet only, driven by market demand for optimal user experience from GOOG-FB-AMZN. <a href="https://staltz.com/the-web-began-dying-in-2014-heres-how.html">https://staltz.com/the-web-began-dying-in-2014-heres-how.html</a></li>
<li>Saildrones are autonomous sailboats that act as mobile weather stations. Could we take advantage of this capability to improve observations of important storms? <a href="https://cliffmass.blogspot.com/2019/11/can-autonomous-weather-observation.html">https://cliffmass.blogspot.com/2019/11/can-autonomous-weather-observation.html</a></li>
<li>Prediction is overrated. What we really should be striving for, with our social science, is ability to bring about desirable outcomes and to avoid unwanted outcomes. <a href="http://peterturchin.com/cliodynamica/psychohistory-and-cliodynamics/">http://peterturchin.com/cliodynamica/psychohistory-and-cliodynamics/</a></li>
<li>If until now you were used to moving along roads and sidewalks, forget it! From now on we all walk through walls! <a href="http://www.bldgblog.com/2010/01/nakatomi-space/">http://www.bldgblog.com/2010/01/nakatomi-space/</a></li>
<li>What would it take to create an effective developmental learning environment for humanity? <a href="https://glazkov.com/2019/08/11/a-classroom-for-humanity/">https://glazkov.com/2019/08/11/a-classroom-for-humanity/</a></li>
<li>Great mathematicians each had their own strategies for generating ideas. Erdős sought hyper-focussed vigilance, Poincaré described lying in bed in a half-dream state as the ideal condition, Descartes famously loved to lounge in bed in the morning and think. <a href="https://www.newyorker.com/culture/annals-of-inquiry/the-myth-and-magic-of-generating-new-ideas?verso=true">https://www.newyorker.com/culture/annals-of-inquiry/the-myth-and-magic-of-generating-new-ideas?verso=true</a></li>
<li>On the Estonian border, another anekdot goes, a border guard is filling out Putin’s entry form. “Occupation?” the officer asks. “Not today,” Putin replies. “Just tourism.” <a href="https://www.theatlantic.com/ideas/archive/2019/11/russian-jokes-tell-deeper-truths-about-putin-and-trump/602713/">https://www.theatlantic.com/ideas/archive/2019/11/russian-jokes-tell-deeper-truths-about-putin-and-trump/602713/</a></li>
<li>The Reigns mechanic requires making pragmatic choices to stay alive, not always being ideologically consistent, and working to find a way out over course of generations, slowly discovering one’s true nature and power. <a href="https://emshort.blog/2018/01/24/reigns-her-majesty/">https://emshort.blog/2018/01/24/reigns-her-majesty/</a></li>
<li><a href="http://mkremins.github.io/blog/gardening-games/">In exploration games, to spend time in a place is to deplete it, to make it less and less interesting until there’s no longer any reason to stay. In gardening games, to spend time in a place is to enrich it.</a> </li>
<li>The intersection of Science Fiction and Religion often yields greatness. <a href="https://urbigenous.net/library/nine_billion_names_of_god.html">https://urbigenous.net/library/nine_billion_names_of_god.html</a></li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>WebUSB, Arduino, and Nunchucks!</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/webusb-arduino"/>
    
    <updated>2019-11-25T09:00:00-00:00</updated>
    
    <id>https://smus.com/webusb-arduino</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>WebUSB bridges two amazing universes: the open web and the maker movement. Web pages can now talk directly to external hardware over USB, and it works on both mobile and desktop (at least in Chrome). There are a <a href="https://github.com/webusb/arduino">few basic samples</a> out there, but for my own edification, I wanted to get my hands dirty. I hooked up a Wii Nunchuck to an Arduino, and built a webpage to plot sensor readings in real-time. Here’s the resulting <a href="https://www.youtube.com/watch?v=mqmtltjk66w">video</a> and <a href="/webusb-arduino/#" title="https://github.com/borismus/sensor-streamer">code</a>.</p>

<!--more-->

<p>I used this handy <a href="https://www.adafruit.com/product/345">breakout board</a> to hook up the Nunchuck to an Arduino Leonardo without having to cut cables and solder. The Arduino runs a sketch which reads sensor values over I2C and sends them to the host webpage over WebUSB. In this case, the host plots the sensor data as it streams in.</p>

<h1>The UX is a mixed bag</h1>

<p>Conveniently, any WebUSB device can be configured to broadcast a specific URL. As soon as you plug it in, Chrome displays a notification telling you that a new device was detected. Clicking the notification will take you directly to the advertised URL. Great!</p>

<p><img src="/webusb-arduino/notification.png" alt="Notification to go to a URL" /></p>

<p>To actually connect to the device, you need a user gesture (button press) to open up a native “Connect to USB device” dialog. You then pick the device from a list, and press the Connect button. Far from frictionless, but it makes sense given the web’s security model.</p>

<p><img src="/webusb-arduino/dialog.png" alt="Webpage wants to connect to device" /></p>

<p>Despite the inconvenience, this is still super interesting, especially for doing one-time setup for a new hardware device. Certainly preferable to requiring the user to install a junky app on their phone!</p>

<h1>Some minor caveats</h1>

<p>WebUSB won’t work with all Arduinos, only those that support a low level USB profile (eg. Emulating a mouse or keyboard). Specifically, the Leonardo is supported, but the Uno is not.</p>

<p>Arduino setup requires installing the <a href="https://github.com/webusb/arduino">WebUSB library</a> manually, and even changing some Arduino header files. This could definitely be streamlined. Also, beware the large number of Wii Nunchuck-related Arduino libraries, all of which seem subtly broken.</p>

<p>Looking forward to using WebUSB in upcoming hardware projects! Stay tuned, and hopefully I’ll have something to share soon.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Toddler’s First Music Box</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/toddler-music-box"/>
    
    <updated>2019-09-26T09:00:00-00:00</updated>
    
    <id>https://smus.com/toddler-music-box</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Toddler toys are stacked with blinking lights, loud attention seeking noises, and earworm songs. They are often made of plastic and sadly, feel cheap. My daughter deserves better! </p>

<p>So I set out to design her a perfect music box: an old concept infused with modern technology, without subjecting her to the hazards of screens. I wanted the box to play her favorite songs, be durable and portable, have long battery life, all while being a beautiful object. This is the result:</p>

<p><a href="https://www.youtube.com/watch?v=nS879aGP6O0"><img src="/toddler-music-box/final-box.jpg" alt="My daughter’s music box" /></a></p>

<!--more-->

<h1>Building a portable music player</h1>

<p>My first instinct was to try to build this using a Raspberry Pi,  but turns out running Linux has a few major downsides:</p>

<ol>
<li>Eeking out reasonable battery life is really difficult.</li>
<li>Boot times are very slow, in the 10s of seconds.</li>
</ol>

<p>An alternative to running embedded Linux is programming a prototyping board directly. So I dusted off an old Arduino Uno and discovered the <a href="https://www.adafruit.com/product/94">Adafruit Wave Shield</a>, which does exactly what I needed. It reads audio from an SD card, and plays it back through a speaker. When the Wave Shield kit arrived, I was somewhat shocked to see a bare PCB and all of the components in a little baggie. </p>

<p>Luckily, there’s an incredible Makerspace at work, so I dropped by after my actual work was done, picked up a soldering iron, and got to work. I’ve soldered before, but not much since my favorite class of all time: Making Things Interact at CMU, taught by Mark Gross. Initially daunting, I knocked it out in no time thanks to some really detailed instructions. The results were amazing. Fuse a bunch of metal and silicon together, attach a battery and 1.5 seconds later (that’s the boot time), a song is playing through the speaker! Any sufficiently advanced technology is indistinguishable from magic.</p>

<p><img src="/toddler-music-box/solder.jpg" alt="Soldering the Adafruit Wave Hat" /></p>

<h1>Randomly playing songs</h1>

<p>The <a href="https://learn.adafruit.com/adafruit-wave-shield-audio-shield-for-arduino/play6-hc">sample software</a> for the Wave Shield plays all of the .wav files on the SD card in lexical order. I wanted something a bit more delightful than playing the same playlist in the same order. So I wrote a program that plays a random song instead. Easy peasy, here we go.</p>

<p>I enumerated all of the wavs on the card and stored all of their filenames in a <code>char**</code> dynamically allocated on the heap. Oops! There’s barely enough space there to allocate a dozen file names, and I’d selected 60 songs. It appears that years of front-end UI engineering have dulled my low level embedded software development instincts. After fighting the C++ compiler about static 2D array allocation, I took a simpler and more memory efficient approach, first counting all of the songs, then picking a random song number to play. </p>

<p>Even generating random numbers is non-trivial, since there’s no reliable absolute clock to use as a seed - the device cold-starts every time. Instead, I’m using a technique which reads in analog inputs for a pseudo random input. This may or may not be a good idea, but seems to provide some variation. Anyway, the <a href="https://github.com/borismus/toddler-music-box">Arduino sketch</a> is in the GitHub repo.</p>

<h1>Prototyping in plastic</h1>

<p>One of the great perks of my work’s Makerspace is access to all sorts of awesome prototype manufacturing equipment, including a <a href="https://www.inventables.com/technologies/carvey">Carvey</a>. So I went to Rockler and bought a 5”x5” maple block, thinking I’d hollow it out into a box with the CNC machine. Not so fast! It’d take a mere 15 hours of drilling. </p>

<p>Rather than wait, I opted for a faster route: prototyping with 3D printing. So I began designing music boxes on paper, then in OnShape, then printing them using the <a href="https://www.prusa3d.com/">Prusa</a> printers. Seeing a design evolve from figment of imagination to tangible physical object is incredibly satisfying. However, once that satisfaction wore off, I can honestly say that the results were functional, but not at all aesthetically pleasing:</p>

<p><img src="/toddler-music-box/plastic-box.jpg" alt="Early 3D printed plastic box" /></p>

<h1>That old time wooden aesthetic</h1>

<p>So I went for a different strategy: buy a nice off-the-shelf box to house everything and just design its insides. This way, the 3D print is mostly hidden, and can be capped off with a laser cut or CNC milled wooden lid. I can learn about CNC joinery later.</p>

<p>I found a <a href="https://www.amazon.com/gp/product/B071WFSRBD/ref=ppx_yo_dt_b_asin_title_o06_s00?ie=UTF8&amp;psc=1">nice hexagonal box</a> on Amazon and bought two, discarding both lids. The bottom half of the clam would house the Arduino, Shield and battery, while the top would house the speaker. They’d be connected with a speaker wire and joined by wood hinges.</p>

<p>Designing the innards was a delight. <a href="https://www.onshape.com">OnShape’s</a> UI is excellent and responsive. The constraint system makes a lot of sense, sketching on arbitrary surfaces and then extruding them is amazingly powerful. The Assembly View and Edit-in-Context feature made aligning elements between upper and lower clamshells a cinch. This included the speaker wire port and holes for the microswitch. I experimented with a variety of designs for fastening lid to container. Initially, I opted for ambitious <a href="https://markforged.com/blog/embedding-nuts-3d-printing/">embedded nut</a> designs, but ultimately went with a self-tapping (into plastic) approach using <a href="https://www.homedepot.com/b/Hardware-Fasteners-Screws/Internal-Hex/Flat/N-5yc1vZc2b0Z1z0sfp4Z1z0sgtn">flat head screws</a>. For fastening electronics to the plastic, I printed offsets and used nuts.</p>

<p>After a few iterations of fastener tweaking, design adjustments, and measurement corrections, I had a top insert for housing the speaker, and a speaker grille to protect the speaker’s membrane, a bottom insert for housing the electronic core, and a cover to hide them. The <a href="https://github.com/borismus/toddler-music-box">STL files</a> are all in the GitHub repo. Here’s the result:</p>

<p><img src="/toddler-music-box/final-disassembly.jpg" alt="Finished wooden musical box" /></p>

<p>If you’re curious, the <a href="https://cad.onshape.com/documents/786c5f0b153cdd48f9b0f2f8/w/b4275650dc7aac134e30275f/e/4a850077e5ce208659a14aab">OnShape project is public</a>, you just need to make an OnShape account.</p>

<h1>Toddler user testing</h1>

<p>Once everything was in place, it was time for toddler testing! Luckily I have a very cute and curious user on retainer for the next 18 years. I asked her to open the box, and when she did, she began wiggling along to Cat Stevens’ “la-la-la”s. I’ve since corrected the poor initial music choice – the music box now plays a healthy milieu of classics from Soviet cartoons.</p>

<p>A few other things became immediately clear as a result of user testing:</p>

<ol>
<li>My daughter was completely fascinated by the small speaker wire running between the top and bottom lid. She kept pulling at it, and eventually the wire came out enough to prevent the lid from closing fully.</li>
<li>Predictably, she loves abusing the box in creative ways. At one point she was dancing on top of the box. Then she used it as a step stool to climb onto the couch. Later, she smashed the box so hard the microswitch toggled and restarted the music.</li>
</ol>

<p>In the current version, I’ve concealed the speaker wire in nylon casing and affixed it on both ends using zipties. This solves the first issue where the wire would prevent the box from closing. I’ve also padded the battery pack so that the contents of the box rattles less, hopefully making it less satisfying to shake and smash.</p>

<p>I’m not sure how much battery life I’ll get from this construction, but it’s been running off the same triple AAA pack for about a week of sporadic play with maybe a couple of hours of being actually on. The fact that the device is fully off when the lid is closed (and not secretly draining batteries) makes me happy.</p>

<p>Finally, massive thanks to Matthew Wilson and Jon Ward for their sage advice and friendly encouragement with this project. How might we imbue other beautiful everyday objects with magical abilities?</p>

<p>Until next time.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Link&#39;s Awakening LEGO Mosaic</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/zelda-lego"/>
    
    <updated>2019-09-13T09:00:00-00:00</updated>
    
    <id>https://smus.com/zelda-lego</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A parismonious use of pixels (160 x 114) and color (four-color greenscale) lends
itself super well to reproduction as a LEGO mosaic! LEGO has this amazing
service called
<a href="https://www.lego.com/page/static/pick-a-brick#shopxlink">Pick-a-Brick</a> where
you can buy spare parts that accidentally ended up in your vacuum cleaner's
dustbin. Or you can buy tiles and flats and make an awesome mosaic based on your
favorite 2-bit sprite. But how many tiles of each color do you need? Not to
worry, I've got you covered with this <a href="https://github.com/borismus/lego-mosaic/blob/master/count_colors.py">color counting
script</a>.
Fully assembled, two-bit LEGO Link makes an excellent coaster for the office
mug.</p>

<p><img src="/zelda-lego/link-mosaic.jpg" alt="Link coaster assembled" /></p>

<!--more-->

<p>I'm not much of a gamer, but Zelda is the one series that continues to occupy a
special place in my heart. Link's Awakening was my first, and remains my
favorite. Twenty five years later, the most memorable part of the game was of
course <a href="https://www.youtube.com/watch?v=NccSaUwoibM">the music</a>. But the
graphics were amazing in their own right. This to me is concentrated nostalgia:</p>

<p><img src="/zelda-lego/links-awakening.png" alt="Screenshot Link's awakening" /></p>

<p>The mosaic is based directly on the most iconic sprite from the game: Link and
his shield strolling sideways through Koholint Island. Here's the colorized
version from the DX version:</p>

<p><img src="/zelda-lego/awakening-link-side-16x16-scaled.png" alt="Link sprite" /></p>

<p>I tried making a Link mosaic from my second favorite, Zelda: Link to the Past.
Unfortunately, a 16-bit color palette is way harder to replicate with LEGO's
limited set of colors, and the result isn't worth sharing.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Avoiding local maximums</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2019/avoiding-local-maximums/"/>
    
    <updated>2019-05-22T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2019/avoiding-local-maximums/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Easier said than done.</p>

<p><img src="/assets/local-maxima.png" alt="Don't focus on local maxima: easier said than done." /></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Reading in 2018</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2019/reading-in-2018/"/>
    
    <updated>2019-01-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2019/reading-in-2018/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I read and listened to a few <a href="/books">books</a> in 2018. My highlights include
speculative fiction stories from Chinese authors: <a href="/books/the-truth-of-fact-the-truth-of-feeling-by-ted-chiang/">Ted
Chiang</a>, <a href="/books/three-body-problem-by-liu-cixin-audio/">Liu
Cixin</a>, <a href="/books/infinite-planets/">Ken Liu's translations
of</a>, Chen Qiufan, Xia Jia, Hao Jingfang. Also really
enjoyed <a href="/books/player-of-games-by-iain-m-banks/">Player of Games</a> and <a href="/books/darkness-at-noon-by-arthur-koestler-audio/">Darkness
at Noon</a>. On the non-fiction
front, I listened to two excellent history lectures: one on <a href="/books/the-fall-and-rise-of-china-audio/">modern
China</a>, another on the <a href="/books/living-the-french-revolution-audio/">French
Revolution</a>. I also read a pair of
books describing cyclical views of history: <a href="/books/the-fourth-turning-by-howe-and-strauss/">The Fourth
Turning</a> and a more scientific
take, <a href="/books/war-and-peace-and-war-by-peter-turchin/">War and Peace and War</a>.
A standout non-fiction favorite was <a href="/books/impro-by-keith-johnstone/">Impro</a>,
which I will definitely be returning to.</p>

<p>Most of my internet reading happens through Instapaper, which affords me a 
handy chronicle of articles that I enjoyed. Here are a few of them (excerpts are
not endorsements):</p>

<!--more-->

<ul>
<li>Between 1900 and 1910, Seattle’s population tripled, from just over 80,000 to
more than 230,000 people. <a href="http://archive.kuow.org/post/story-behind-seattles-obsession-craftsman-homes">The Story Behind Seattle's Obsession With Craftsman
Homes</a></li>
<li>The demo drew rave reviews from the technology press. <a href="https://www.1843magazine.com/technology/the-daily/no-googles-pixel-buds-wont-change-the-world">No, Google’s Pixel Buds
won’t change the
world</a></li>
<li>That’s home. That’s us. On it everyone you love, everyone you know, everyone
you ever heard of, every human being who ever was, lived out their lives.
<a href="https://www.brainpickings.org/2012/12/10/pale-blue-dot-motion-graphics/">Carl Sagan’s Pale Blue Dot, Animated in Motion
Graphics</a></li>
<li>Our choice is not between "regulation" and "no regulation." The code
regulates. It implements values, or not. It enables freedoms, or disables
them. It protects privacy, or promotes monitoring. <a href="https://harvardmagazine.com/2000/01/code-is-law-html">Code Is
Law</a></li>
<li>In other words: AIs are best at choosing answers. Humans are best at choosing
questions. <a href="https://jods.mitpress.mit.edu/pub/issue3-case">How To Become A
Centaur</a></li>
<li>Today’s belief in ineluctable certainty is the true innovation-killer of our
age. In this environment, the best an audacious manager can do is to develop
small improvements to existing systems — climbing the hill, as it were, toward
a local maximum, trimming fat, eking out the occasional tiny innovation — like
city planners painting bicycle lanes on the streets as a gesture toward
solving our energy problems. <a href="https://www.wired.com/2011/10/stephenson-innovation-starvation/">Innovation
Starvation</a></li>
<li>There is no reason to let a company make so much money while potentially
helping to radicalize billions of people <a href="https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html">YouTube, the Great
Radicalizer</a></li>
<li>Favoring the “soft” aspects of a college application is straightforwardly
beneficial to the more privileged at the expense of the less. <a href="https://jacobinmag.com/2018/03/sat-class-race-inequality-college-admission">The Progressive
Case for the
SAT</a></li>
<li>Science fiction allows us the distance to circumvent issue fatigue in our very
troubled times. <a href="https://www.nature.com/articles/d41586-017-08674-8">Science fiction when the future is
now</a></li>
<li>Isaiah Berlin, sobered by the 20th century’s failed utopias, has argued for a
more modest liberal pluralism that makes room for multiple, genuinely
conflicting goods. Family and work, solidarity and autonomy, tradition and
innovation are really valuable, and really in tension <a href="https://www.theatlantic.com/magazine/archive/2018/04/steven-pinker-enlightenment-now/554054/">When Truth and Reason
Are No Longer
Enough</a></li>
<li>I am not a relativist; I do not say "I like my coffee with milk and you like
it without; I am in favor of kindness and you prefer concentration camps"
<a href="https://www.cs.utexas.edu/users/vl/notes/berlin.html">Isaiah Berlin on
Pluralism</a></li>
<li>Unstructured, unsupervised time for play is one of the most important things
we have to give back to kids if we want them to be strong and happy and
resilient. <a href="https://reason.com/archives/2017/10/26/the-fragile-generation">The Fragile
Generation</a></li>
<li>The average net worth among adults in the 95th to 99th percentile is about
$1.7 million. Among the 0.1 percent, it’s about $60 million. Who, exactly, are
the aristocrats again? <a href="https://slate.com/business/2018/05/forget-the-atlantics-9-9-percent-the-1-percent-are-still-the-problem.html">Forget What the Atlantic Is Telling You. The 1 Percent
Are Still the
Problem</a></li>
<li>We are a species that strives not just for survival, but also for
significance. We want lives that matter. <a href="https://www.nytimes.com/2018/06/23/opinion/sunday/suicide-rate-existential-crisis.html">Is This an Existential
Crisis?</a></li>
<li>To support triangulation, we recommend a shift to a contributorship model,
similar to the credits that roll at the end of a film — a long list of
individuals with their contributions described fully and specifically <a href="https://www.nature.com/articles/d41586-018-01023-3">Robust
research needs many lines of
evidence</a></li>
<li>Do whatever you can’t stop thinking about. Documenting your findings in public
(regardless of outcomes!) is a worthy contribution to society, full stop. <a href="https://nadiaeghbal.com/independent-research">The
independent researcher</a></li>
<li>Various studies have identified cooperation as a core theme in popular
narratives across the world. ... nearly 80% of their tales concerned moral
decision making and social dilemmas <a href="http://www.bbc.com/culture/story/20180503-our-fiction-addiction-why-humans-need-stories">Our fiction addiction: Why humans need
stories</a></li>
<li>The transhumanist vision too easily reduces all of reality to data,
concluding that “humans are nothing but information-processing objects.”
<a href="https://medium.com/s/futurehuman/survival-of-the-richest-9ef6cddd0cc1">Survival of the
Richest</a></li>
<li>This means that the mistake must be at the root, at the very basis of human
thinking in the past centuries. ... It could also be called
anthropocentricity, with man seen as the center of everything that exists. <a href="https://www.americanrhetoric.com/speeches/alexandersolzhenitsynharvard.htm">A
World Split
Apart</a></li>
<li>Concoct one story where things get better, one where they get worse, and one
where they get weird. <a href="https://www.nytimes.com/2018/09/01/opinion/sunday/how-make-big-decision.html">How to Make a Big
Decision</a></li>
<li>Particularly sad is the belief that the majority of the homeless were drunks,
drug addicts, criminals or thieves. It is true that the category “homeless”
has come to include all of those people, but there are so many, many more who
are none of those things. <a href="https://crosscut.com/2018/09/why-i-started-giving-homeless">Why I started giving to the homeless</a></li>
<li>Perhaps unsurprisingly, Magic Leap is almost entirely built on the visions of
grown men wanting to immerse themselves in fantasy worlds. <a href="https://gizmodo.com/the-magic-leap-con-1829716266">The Magic Leap
Con</a></li>
<li>The really important kind of freedom involves attention, and awareness, and
discipline, and effort, and being able truly to care about other people and to
sacrifice for them, over and over, in myriad petty little unsexy ways, every
day. That is real freedom. The alternative is unconsciousness, the
default-setting, the "rat race"-the constant gnawing sense of having had and
lost some infinite thing. <a href="https://fs.blog/2012/04/david-foster-wallace-this-is-water/">This is Water</a></li>
<li>The big mistake in this pattern of failure is projecting your subjective lack
of comprehension onto the object you are looking at, as “irrationality.” We
make this mistake because we are tempted by a desire for legibility. <a href="https://www.ribbonfarm.com/2010/07/26/a-big-little-idea-called-legibility/">A Big Little Idea Called Legibility</a></li>
<li>There is something sinister and corrupt—Maoist—in the habit of assigning
people to categories. <a href="https://www.wsj.com/articles/america-is-addicted-to-outrage-is-there-a-cure-1543620811">America Is Addicted to Outrage. Is There a
Cure?</a></li>
<li>Like the open office, the loud restaurant seems to have overstayed its
welcome. That’s because loud restaurants are more profitable. <a href="https://www.theatlantic.com/technology/archive/2018/11/how-restaurants-got-so-loud/576715/">How Restaurants
Got So
Loud</a></li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Comparing classical music interpretations</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/classical-interpreters"/>
    
    <updated>2018-09-20T09:00:00-00:00</updated>
    
    <id>https://smus.com/classical-interpreters</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I built an audio player to easily compare multiple interpretations of the same
piece. Here's an interactive <a href="https://borismus.github.io/classical-interpreter/">demo</a>, and a video to give you a sense of how it
works:</p>

<p><video src="/classical-interpreters/two-goulds.mp4" style="display: block; margin: 0 auto;" controls /></p>

<!--more-->

<h2>What does it mean to interpret classical music?</h2>

<p>At first glance, sheet music is prescriptive: the composer has provided all of
the notes, the dynamics (forte, piano), tempo (lento, presto) and changes in
tempo (de/accelerando).</p>

<p>In practice, however, the interpreter has a lot of leeway. In some extreme
cases, such as the <a href="https://en.wikipedia.org/wiki/Cadenza">Cadenza</a> in solo concertos, the performer gets to
improvize a melody based on a chord progression. Some pieces include
ornamentation (eg. trills, etc) which are largely left up to the performer to
interpret.</p>

<p>That said, cadenzas and ornaments are somewhat rare. In general, every piece is
under-specified by the composer. This gives the performer a lot of leeway to
express themselves through the performance, selecting tempo, phrasing,
articulation and tone.</p>

<h2>Example: Bach's Goldberg Variations</h2>

<p>The Goldberg Variations were composed by Johann Sebastian Bach in 1741, and then
popularized by Glenn Gould in his debut album in 1955, transforming a work once
considered esoteric into one of the most iconic piano recordings.</p>

<p>In 1981, a year before his death, Gould recorded the pieces again. After a long
period of reclusion, he was able to revisit the variations and produce a
completely different take. In an interview, he said:</p>

<blockquote>
  <p>...since I stopped playing concerts, about 20 years, having not played it in
  all that time, maybe I wasn't savaged by any over-exposure to it...</p>
</blockquote>

<h2>Compare Gould's 1955 and 1981 recordings</h2>

<p>Both the <a href="https://youtu.be/Cwas_7H5KUs?t=1m55s">1955</a> and <a href="https://www.youtube.com/watch?v=zpsfhTxo5yw&amp;t=173s">1981</a> recordings are available on YouTube, of course.
I found that listening to two distinct performances is not the same as having one
integrated player. So I built one: a player specifically for comparing multiple
interpretations of the same piece.</p>

<p>Here is a demo that lets you compare the first variation from the Goldberg
Variations. <a href="https://borismus.github.io/classical-interpreter/">Try it out here</a>. You can use your keyboard to skip between
interpretations (↑, ↓) just as easily as you can seek within a track (←, →).
The mouse works as well. Note that I haven't tested at all on mobile. Sorry,
it's just a prototype and I'm on paternity leave 😇</p>

<h2>I also tried it on Mozart's Requiem</h2>

<p>I am a huge fan of Mozart's Requiem, and once came across an <a href="https://www.reddit.com/r/classicalmusic/comments/1xpqyh/what_is_the_best_recorded_performance_of_mozarts/">online thread
debating</a> which conductor's performance was the best. I soon
found myself listening to a dozen or so different versions of the same piece.
When I was a younger music appreciator, I would often wonder what the point of
a conductor <em>really</em> was. I no longer have this question.</p>

<p>Just to give you a taste for how different the interpretations are, here's an
example of three conductors performing the Introitus, the first movement in the
Requiem. <a href="https://borismus.github.io/classical-interpreter/?json=https://splendid-society.surge.sh/index.json">Check it out here</a>, but be patient as it may take a
minute to load and decode the audio. Böhm's brooding tempo and lumbering chorus
(ugh) contrasts especially well with Levin's crisp and minimalist take.</p>

<p><video src="/classical-interpreters/three-requiems.mp4" style="display: block; margin: 0 auto;" controls /></p>

<h2>Technical details</h2>

<p>For this prototype, I focused on creating a reasonable UI to play back and
interact with multiple time-aligned performances of the same piece. An <a href="https://borismus.github.io/classical-interpreter/goldberg/index.json">index
file</a> specifies metadata for each track, most importantly the URL to
the label file and the URL to the audio file. Each <a href="https://borismus.github.io/classical-interpreter/goldberg/gould-1955.txt">label file</a> is a
text file with lines in the format <code>START_TIME  END_TIME BAR_NUMBER</code>. </p>

<p>To create the label files, I manually annotated the waveform. Even with
Audacity's extremely useful <a href="https://manual.audacityteam.org/man/label_tracks.html">label track</a> feature, it was a lot
of manual work to go through the score, and find each bar's time
range in each recording. At the end of the day, I had start and end times for
each bar. For times that don't fall exactly on bar lines, I linearly interpolate
between the bar boundaries, which works reasonably well, but is sometimes a bit
off. More granular timing references would address this better, but that
currently means doing more manual labor. No thanks!</p>

<h3>Science, help me automate this, please</h3>

<p>An obvious question is how to automate the labor of synchronizing a recording to
a score. In general, I think this is an unsolved problem, especially for complex
tracks containing hundreds of instruments and varying levels of background
noise.</p>

<p>An promising approach that could work for solo piano music might be to use
something like <a href="https://magenta.tensorflow.org/onsets-frames">Onsets and Frames</a> to extract piano rolls and
then apply something like a Dynamic Time Warp (DTW) in piano roll space.  A more
general approach might be to synthesize each bar into raw audio (from MIDI), and
then align recordings to synthesized audio using something like DTW based on a
Constant-Q transform (CQT).</p>

<p>My brief and ill-guided attempts to <a href="https://musicinformationretrieval.com/dtw_example.html">do something like this</a> on real-world
examples didn't yield good enough results. Any ML/DSP experts want to take this
on?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Against retweets</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/against-retweets/"/>
    
    <updated>2018-09-09T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/against-retweets/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><img src="/assets/against-retweets.png" alt="Create and consume time for content of various lengths." /></p>

<p>One of these data points is not like the other. Also, isn't it neat that common
units of time duration fall nicely onto a logarithmic scale?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Make twitter great again?</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/make-twitter-great-again/"/>
    
    <updated>2018-09-06T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/make-twitter-great-again/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I used twitter the most while working in developer relations, mainly sharing
articles about web development that I thought deserved more attention. My feed
consisted of engineers, developer advocates, and web standards folks I
respected, all of whom had something interesting to say. Twitter was my interest
graph, in contrast with Facebook, which was just a poor facsimile of real life
relationships.</p>

<p>As my career and interests shifted, so did the people I followed on twitter.
I started following VR engineers, machine learning researchers, philosophers
and psychologists. But more tweeps meant more tweets. Soon I had a [twitter
list][smartpeople](https://twitter.com/borismus/lists/smartpeople) for people that I had reluctantly unfollowed. They had
interesting things to say, but said them too often.</p>

<p>Following someone on twitter is like having a direct line into their brain. If
they write about a subject you are passionate about, fantastic. If they have
broadly aligning interests, chances are they will turn you on to something neat,
or if you're lucky, life changing. Unfortunately, most of the time, twitter
brains are distracted, anxious, angry, or unhappy. Spastic tweet storms and
political ramblings deserve only one thing, a hearty unfollow.</p>

<p>Twitter's bad design decisions only compound the problem. In "Retweets Are
Trash", Alexis Madrigal <a href="https://www.theatlantic.com/magazine/archive/2018/04/the-case-against-retweets/554078/">writes</a>, "When Twitter introduced a retweet
button, in 2009, suddenly one click could send a post careening through the
network. The automatic retweet took Twitter’s natural tendency for amplification
and cranked it up." Apparently it's not hard to globally disable retweets using
a <a href="https://medium.com/@Luca/how-to-turn-off-retweets-for-everyone-99dd835c10f8">crude hack</a>. Anecdotally, it seems to have made my feed a bit calmer.</p>

<p>A quick update on my <a href="http://smus.com/notes/2018/lynched-on-twitter/">white supremacist spam problem</a>. Time seems to
have sorted it out, and the remaining squawks have been muffled by twitter's
<a href="https://support.twitter.com/articles/20169398">quality filter</a>. I also tried using Mastodon. It was a calming
experience at first, but when I followed a few people and dug around a little
bit, I was dismayed to see most design choices mirroring twitter's. I caught up
on RSS feeds, which I really like. But the RSS ecosystem continues to die. Of
nearly five hundred RSS feeds in my OPML file, more than half are broken links.
Occasionally the links are fixed, causing my reader to choke on a year's worth
of articles from a blog whose existence I've long forgotten about.</p>

<p>I can't quite decide whether or not to use twitter again. I keep thinking that I
once got a lot out of my original use of the platform. If I do return, I'll be
making use of the @reply quality filter, fully disable retweets, and unfollow
more aggressively. Whether this makes twitter great again remains to be seen.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Lynched on Twitter</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/lynched-on-twitter/"/>
    
    <updated>2018-08-26T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/lynched-on-twitter/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Last week, a <a href="https://twitter.com/borismus/status/798350680951910400">sarcastic twitter
reply</a> I posted two
years ago was picked up by an account called Blue Check Watch, looking for
verified twitter accounts disparaging white people. As a result, my account has
become inundated by a hybrid human-bot army of white supremacist trolls. Most
responses accused me of anti-white racism, completely missing what I thought was
obviously a sarcastic tone. Sadly, many of the attacks are really nasty and
ad-hominem. One user
<a href="https://twitter.com/CascadianDennis/status/1033190496456269824">posted</a> "Why do
people who say this always look like this?", alongside my twitter profile photo.
Others are
<a href="https://twitter.com/1CE1CEB4BY/status/1033197950703464448">explicitly</a>
<a href="https://twitter.com/NationDixie/status/1033395244014944256">anti-semitic</a>
messages that do not bear repeating here. Ominously, I received an email from
Twitter informing me that they have investigated <strong>my tweet</strong>, flagged for violating
their TOS and "have not taken any action at this time."</p>

<p>Writing my snarky tweet two years ago, I was a bit uncertain whether it was
within the bounds of social acceptability. A white guy telling another
white guy that white guys are "the worst", was intended to poke a bit of fun at
the self-flaggelation associated with progressive politics. There is only so
much self-censorship one can do while still remaining authentic. In a surprising
twist, my tweet was offensive to a group I hadn't even thought about in a way I
could not have predicted, two years after the fact.</p>

<p>Twitter lynchings are not new, but it's different when it happens to you. It has
not had any serious consequences (yet?), and until it does, I will continue to
view this as a funny story, and not something truly traumatic, as it is often
framed in the press. Still, this experience serves as a good reminder of a few
points:</p>

<ol>
<li>Twitter's UI for threaded conversations is terrible, and hardly anyone of my
virtual lynch mob even bothered to look to the tweet I was responding to.</li>
<li>My tweet was five words, twenty four characters. It was meant to be pithy,
because that is how twitter works. But brief posts beget brief responses,
which is fine between friends, but not fine when the whole world is watching.</li>
<li>Relatedly, sarcasm is hard to convey in this situation. Only two responses
suggested that maybe I was being sarcastic, imploring my attackers, "Don't be
lazy."</li>
<li>The fact that tweets have no expiry date means anything you have ever posted
can be used against you in surprising ways. It took two years for my tweet to
be picked up. James Gunn's incriminating posts were ten years old. Social
norms change quickly, and this is one argument in favor of ephemerality.</li>
<li>On that same note, it seems that the white supremacy movement has really
embraced the internet over the last couple of years in some sick symbiosis
with the POTUS. At the same time, it's striking that the result of someone's
pathetic search for anti-white sentiment yielded such a lackluster example.</li>
<li>Despite a general willingness for my views to be challenged, I don't have
energy or desire to engage with this mob. I made no attempts to respond to
any of my attackers. They appear to be victims of confirmation bias, stupid
and/or malicious, and I have little faith that they will listen.</li>
<li>There are some really nasty people on the internet. If triggered, they will
sherlock your religion, your employer, your family, and try to hit you where
it hurts the most.</li>
</ol>

<p>Lynchings aside, I've been trying to use twitter less in favor of consuming and
producing longer form content, like books, blogs, and conversations. Given that
my account is currently being inundated with anti-semitic spam, I have very
little incentive to continue using the service. One thing that has been keeping
me on the platform is the potential reach I can get through 6K followers. I
should remember that in the grand scheme of things, this is a small number, 5
orders of magnitude less than the lovely POTUS. I shouldn't grow attached.</p>

<p>And so, just in time for my tenth twitter anniversary, it's time for me to make
some changes. At the very least, I'll be taking a long break.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>The difficult middle</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/the-difficult-middle/"/>
    
    <updated>2018-07-21T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/the-difficult-middle/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Monism gives life meaning through a single all-encompassing doctrine. Examples
include some political ideologies, religions, and [variants of
rationality][chapman-rationality](https://meaningness.com/non-theistic-eternalism). History has not been kind to those that have
fervently followed such doctrines. But once you see flaws in Monism, it can be
tempting to decide that life has <a href="https://meaningness.com/extreme-examples-eternalism-and-nihilism">no meaning at all</a>, falling
into the trap of Nihilism.</p>

<p><img src="https://i.imgur.com/WXdU3ef.gif" alt="Stuck" /></p>

<p>The choice between these two extremes is a false one. [Rationalists know
this][third-alternative](https://www.lesswrong.com/posts/erGipespbbzdG5zYb/the-third-alternative). <a href="https://en.wikipedia.org/wiki/Middle_Way">Buddhists know this</a>. Philosophers know
this too. David Chapman writes a <a href="https://meaningness.com/">whole blog</a> devoted to exploring
the space in between. But I have found <a href="https://www.cs.utexas.edu/users/vl/notes/berlin.html">Isaiah Berlin</a>'s synthesis to
be the most compelling:</p>

<blockquote>
  <p>Sobered by the 20th century’s failed utopias, <a href="https://www.cs.utexas.edu/users/vl/notes/berlin.html">Berlin</a>
  argued for a more modest liberal pluralism that makes room for multiple,
  genuinely conflicting goods. Family and work, solidarity and autonomy, tradition
  and innovation are really valuable, and really in tension.  – [Alison
  Gopnik][gopnik](https://www.theatlantic.com/magazine/archive/2018/04/steven-pinker-enlightenment-now/554054/)</p>
</blockquote>

<p>In this view, we are condemned to an eternal balancing act, sometimes peering
too far into the meaningless abyss, and other times relaxing too deeply into
meaningful certainty. On balance, I hope to end up in this difficult middle.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Required reading for VR enthusiasts</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/required-reading-for-vr-enthusiasts/"/>
    
    <updated>2018-06-03T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/required-reading-for-vr-enthusiasts/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Several books, movies, and articles have helped to shape my opinions on Virtual
Reality. I've was initially surprised by some former colleagues that it is
possible to view these works as aspirational rather than cautionary. I generally
prefer the dystopian interpretation. </p>

<ul>
<li>📚 Ray Bradbury - <a href="http://www.veddma.com/veddma/Veldt.htm">The Veldt</a></li>
<li>📚 Neil Stephenson - Snow Crash</li>
<li>📚 Stanislav Lem - The Futurological Congress</li>
<li>📚 Vernor Vinge - Rainbow's End</li>
<li>📚 Ernst Kline - Ready Player One</li>
<li>🎥 Black Mirror S3 E2: <a href="http://www.theverge.com/2016/10/25/13401020/black-mirror-season-3-episode-2-playtest-recap">Playtest</a></li>
<li>🎥 The Matrix</li>
<li>🎵 Father John Misty - <a href="https://genius.com/Father-john-misty-total-entertainment-forever-lyrics">Total Entertainment Forever</a></li>
<li>📜 <a href="http://slatestarcodex.com/2014/06/03/asches-to-asches/">Asches to Asches</a></li>
<li>📜 Robert Nozick's <a href="https://en.wikipedia.org/wiki/Experience_machine">experience machine</a></li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Balancing technological pessimism</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/balancing-technological-pessimism/"/>
    
    <updated>2018-05-16T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/balancing-technological-pessimism/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="https://pessimists.co/">Pessimists archive</a> is a podcast that chronicles
pessimistic reactions to emerging technology as it was becoming mainstream.
Technology here is defined broadly, covering a broad range of topics: bikes,
coffee, pinball machines, vaccines, recorded music. The podcast is very
accessible, focused more on social and psychological issues and less on the tech
itself. </p>

<!--more-->

<p>In response to unreasonable technological enthusiasm ever present in my
industry, I have myself tended towards pessimism. After we launched Cardboard,
my small HCI team in Google Research became the de facto home of VR at Google.
Once word got out, engineers and designers flocked to our ranks, self selecting
for interest in VR. Suddenly I was surrounded by hundreds of VR enthusiasts,
while I was skeptical about the vision of VR as <a href="https://youtu.be/dxbh-TM5yNc?t=3m">"the last
platform"</a> and concerned about the social
implications of broad VR adoption.</p>

<p>I began to see myself as <a href="https://www.quora.com/World-War-Z-2013-movie/World-War-Z-2013-movie-Do-the-Israelis-really-have-a-10th-man-doctrine">the tenth
man</a>,
posing the question: what if VR isn't really a thing? I ended up much more
excited about <a href="https://atap.google.com/spotlight-stories/">Spotlight Stories</a>
without a headset, thought a bunch about augmented reality through audio, and
became increasingly alienated from my colleagues gushing with optimism, rushing
home to play Vive games in their carefully instrumented basements. This podcast
would sure have helped me put my VR pessimism into perspective. </p>

<p>One critique I had of the podcast concept is hindsight bias. All of the
technologies covered by Pessimists Archive ended up being successful, and time
has proven the doubters wrong. But how many emerging technologies were
successfully booed out of existence, or simply rejected by the market? And how
many market successes come at a significant cost that prescient pessimists
warned us about?  TV and smartphones come to mind.</p>

<p>That said, I highly recommend the podcast series. The episodes are short,
entertaining, and informative. I feel better equipped to recognize and overcome
the traps that ensnared previous generations of techno pessimists.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>How rationalists can win</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2018/how-rationalists-can-win/"/>
    
    <updated>2018-04-12T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2018/how-rationalists-can-win/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>The belief that <a href="https://wiki.lesswrong.com/wiki/Rationality_is_systematized_winning">"rationalists should win"</a> is widely held in the
rationalist community. So: does being a good rationalist actually help you win?
Certainly in some domains, like engineering and science, which focus on
quantification, systematization, and prediction. There, having a hyper-rational
mindset is clearly an advantage. As for winning at life, which I will take that
to mean leading to greater success in survival, evolution, and human
flourishing, I don't think rationality helps very much.</p>

<!--more-->

<p>The rationalist doctrine always favors quantification and optimization over
intuition. From a rationalist perspective, humans are buggy <a href="https://www.wired.com/2016/01/marvin-minskys-marvelous-meat-machine/">computers made of meat</a>. We are full of cognitive biases that cloud our judgement, and
cause us to do reliably poorly on <a href="https://en.wikipedia.org/wiki/Newcomb%27s_paradox">simple tasks</a> related to
probability. So even if you don't really know what probability to assign, it's
better to <a href="https://putanumonit.com/">"put a number on it"</a> and then <a href="https://wiki.lesswrong.com/wiki/Shut_up_and_multiply">”shut up and multiply”</a>.</p>

<p>The problem with the above approach lies with the words "systematic" and
"predictable". Herb Simon's idea of <a href="https://en.wikipedia.org/wiki/Bounded_rationality">Bounded Rationality</a>, suggests that
our human ability to be rational, to systematize and predict is inherently
limited. These limitations may come from our limited brains, but may also be
fundamental in nature. Chaos theory makes it really difficult for even the most
advanced computers to predict even simple systems, such as the position of the
<a href="http://awesci.com/double-pendulum-and-why-we-can-never-predict-weather/">double pendulum</a> after a small amount of time.</p>

<p>The world is far more complicated than one pendulum attached to another.
Insisting on making every decision in life based on a <a href="/making-better-decisions/">series of sound logical steps</a>, one quickly becomes paralyzed by the
complexity of myriad tiny choices. Gerd Gigerenzer's <a href="https://en.wikipedia.org/wiki/Ecological_rationality">ecological rationality</a> suggests that many things that appear illogical are actually
done for good reasons, ultimately leading to good outcomes. This idea is
directly at odds with the rational propensity for first knowing what is true
(epistemic rationality), and then making decisions in a series of logical steps.</p>

<p>Instead of being dogmatic about rationality, we should be better
consequentialists. Consider outcomes. Are rationalists more satisfied with their
lives than non-rationalists? Are they more productive?  Do they have fewer
regrets later in life? Do they have better relationships? Make more money?  Have
happier children? Impact their domains of knowledge more effectively?</p>

<p>It <a href="http://slatestarcodex.com/2018/04/02/are-the-amish-unhappy-super-happy-just-meh/">is hard to answer</a> the above questions definitively, but suppose we
had solid data, and it was clear that, Buddhists were more relaxed, Protestants
more productive, Amish happier, and Singularitarians more innovative than other
groups. Imagine we had a sense of expected consequences of being an adherent to
each type of lifestyle. Imagine further that you could take a personality test
and see how likely you would be to benefit from each. Then the rational thing to
do would be to choose one of the irrational doctrines to follow based on
expected outcomes if you were an adherent.</p>

<p>Of course, you want to remain open to changing your mind, so allow yourself to
indulge fully your chosen beliefs for a while. At the top of every year, wake up
as late as your doctrine allows and nurse your hangover, unless your beliefs
prevented you from drinking the night before. If your beliefs favors ice baths,
partake in the polar bear plunge. Then take a moment to re-evaluate your
beliefs.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Web-based voice command recognition</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/web-voice-command-recognition"/>
    
    <updated>2018-01-04T09:00:00-00:00</updated>
    
    <id>https://smus.com/web-voice-command-recognition</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="/web-audio-ml-features">Last time</a> we converted audio buffers into images. This time
we'll take these images and train a neural network using
<a href="https://deeplearnjs.org">deeplearn.js</a>. The result is <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">a browser-based demo</a> that 
lets you speak a command ("yes" or "no"), and see the output of the classifier
in real-time, like this:</p>

<p><video src="/web-voice-command-recognition/inference-demo.mp4" autoplay muted loop type="video/mp4"
style="width: 100%"></p>

<p>Curious to play with it, see whether or not it recognizes <em>yay</em> or <em>nay</em> in
addition to <em>yes</em> and <em>no</em>? <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">Try it out live</a>. You will quickly see
that the performance is far from perfect. But that's ok with me: this example is
intended to be a reasonable starting point for doing all sorts of audio
recognition on the web. Now, let's dive into how this works.</p>

<!--more-->

<h1>Quick start: training and testing a command recognizer</h1>

<p>Here's how you can train your own yes/no classifier:</p>

<ol>
<li>Go to the <a href="https://google.github.io/web-audio-recognition/train-model/?data_url=https://storage.googleapis.com/audio-recognition-data&amp;data_extension=mp3">model training page</a>. It will take a bit of time to download
the <a href="https://storage.googleapis.com/audio-recognition-data/yes.mp3">training</a> <a href="https://storage.googleapis.com/audio-recognition-data/no.mp3">data</a> from the server.</li>
<li><p>Click the train button, and you'll see a graph showing training progress.
Once you are ready (this will take a while, perhaps 500 iterations or 2
minutes, depending on your hardware), stop training, and press the save
weights (file) button.  This will download a JSON file.</p>

<p><img src="/web-voice-command-recognition//web-voice-command-recognition/2-training-graph.png" alt="Training graph" /></p></li>
<li><p>Then go to <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">the inference demo page</a>, press the load weights (file) button
and select the downloaded JSON file to load the trained model.</p></li>
<li><p>Flip the switch, grant access to the microphone and try saying "yes" or "no".
You'll see microphone and confidence levels indicated at the bottom of the
page.</p>

<p><img src="/web-voice-command-recognition/3-inference.png" alt="Inference screenshot" /></p></li>
</ol>

<p>The above is a mechanistic account of how the training example works.  If you
are interested in learning about the gory (and interesting) details, read on.</p>

<h1>Data pre-processing and loading</h1>

<p>Training a neural net requires a lot of training data. In practice, millions of
examples may be required, but <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">the dataset</a> we'll be using is small by
modern standards, with <em>just</em> 65,000 labeled examples. Each example is a
separate wav file, with the label in the filename.</p>

<p>Loading each training wav as a separate request turned out to be quite slow. The
overhead from each request is small, but when compounded over a few thousand
times, really starts to be felt. An easy optimization to load data more quickly
is to put all examples with the same label into one long audio file. Decoding
audio files is pretty fast, and so is splitting them into one second long
buffers. A further optimization is to use a compressed audio format, such as
mp3. <a href="https://github.com/google/web-audio-recognition/blob/master/train-model/scripts/preprocess.py"><code>scripts/preprocess.py</code></a> will do this concatenation for you,
producing this <a href="https://storage.googleapis.com/audio-recognition-data/yes.mp3">mesmerising result</a>.</p>

<p>After we "rehydrate" our raw audio examples, we process buffers of raw data into
features. We do this using the <a href="/web-audio-ml-features">Audio Feature extractor</a> I
mentioned in the <a href="/web-audio-ml-features">last post</a>, which takes in raw audio, and
produces a log-mel spectrogram. This is relatively slow, and accounts for most
of the time spent loading the dataset.</p>

<h1>Model training considerations</h1>

<p>For the yes/no recognizer, we have only two commands that we care about: "yes",
and "no". But we also want to detect the lack of any such utterances, as well as
silence. We include a set of <a href="https://storage.googleapis.com/audio-recognition-data/other.mp3">random utterances</a> as the "other" category
(none of which are yes or no). This example is also generated by the
<a href="https://github.com/google/web-audio-recognition/blob/master/train-model/scripts/preprocess.py">preprocessing script</a>.</p>

<p>Since we're dealing with real microphones, we never expect to hear pure silence.
Instead, "silence" is some level of ambient noise compounded by crappy
microphone quality. Luckily, the training data also includes background noise
which we mix with our training examples at various volumes. We also generate a
set of silence examples, which includes only the background audio.  Once we've
prepared our samples, we generate our final spectrograms as our input.</p>

<p>To generate these final spectrograms, we need to decide on buffer and hop
length. A reasonable buffer length is 1024, and a hop length of 512. Since we
are dealing with sample rate of 16000 Hz, it works out to a window duration of
about 60ms, sampled every 30ms.</p>

<p>Once we have labeled spectrograms, we need to convert inputs and labels into
deeplearn arrays. Label strings "yes", "no", "other", and "silence" will be
<a href="https://en.wikipedia.org/wiki/One-hot">one-hot</a> encoded as a <code>Array1D</code>s of four integers, meaning that "yes"
corresponds to <code>[1, 0, 0, 0]</code>, and "no" to <code>[0, 1, 0, 0]</code>. Spectrograms from the
feature extractor need to be converted into an <code>Array3D</code>, which can be fed as
input for the model.</p>

<p>The model we are training consists of two convolution layers, and one fully
connected layer. I took this architecture directly from the MNIST example
of deeplearn.js, and hasn't been customized for dealing with spectrograms at
all. As a result, performance is a far cry from state of the art speech
recognition. To see even more mis-classifications, try out <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=number">MNIST for
audio</a> which recognizes spoken digits (eg. "zero" through "ten").
I am confident that we could do better by following <a href="http://www.isca-speech.org/archive/interspeech_2015/papers/i15_1478.pdf">this paper</a>. A
real-world speech recognizer might not use convolution at all, instead opting
for an <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>, which is better suited to process time-series data.</p>

<p>Lastly, we want to tell the machine learning framework how to train the model.
In ML parlance, we need to set the <a href="https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)">hyperparameters</a>, which includes
setting the learning rate (how much to follow the gradient at each step) and
batch size (how many examples to ingest at a time). And we're off to the races:</p>

<p><img src="/web-voice-command-recognition//web-voice-command-recognition/2-training-graph.png" alt="Training graph" /></p>

<p>During training, the gradient descent algorithm tries to minimize cost, which
you can see in blue. We also plot accuracy in orange, which is occasionally
calculated by running inference on a test set. We use a random subset of the
test set because inference takes time, and we'd like to train as quickly as
possible.</p>

<p>Once we are happy with the test accuracy, we can save the model weights and use
them to infer results.</p>

<h1>Saving and loading model weights</h1>

<p>A model is defined by its architecture and the weights of its weight-bearing
nodes. Weights are the values that are learned during the process of model
training, and not all nodes have weights. ReLUs and flatten nodes don't. But
convolution and fully connected nodes have both weights and biases. These
weights are tensors of arbitrary shapes. To save and load models, we need to be
able to save both graphs <strong>and</strong> their weights.</p>

<p>Saving &amp; loading models is important for a few reasons:</p>

<ol>
<li>Model training takes time, so you might want to train a bit, save weights,
 take a break, and then resume from where you left off. This is called
 checkpointing.</li>
<li>For inference, it's useful to have a self-contained model that you can just
 load and run.</li>
</ol>

<p>At the time of writing, deeplearn.js didn't have facilities to serialize models
and model weights. For this example, I've implemented a way to load and save
weights, assuming that the model architecture itself is hard-coded. The
<a href="https://github.com/google/web-audio-recognition/blob/master/train-model/src/GraphSaverLoader.ts"><code>GraphSaverLoader</code></a> class can save &amp; load from a local store (IndexedDB),
or from a file. Ultimately, we will need a non-hacky way of saving and loading
models and their corresponding weights, and I'm excited for the near future of
improved ML developer ergonomics.</p>

<h1>Wrapping up</h1>

<p>Many thanks to <a href="https://twitter.com/nsthorat">Nikhil</a> and
<a href="https://twitter.com/dsmilkov">Daniel</a> for their hard work on deeplearn.js, and
willingness to answer my barrages of stupid little questions. Also, to
<a href="https://twitter.com/petewarden">Pete</a>, who is responsible for creating and
releasing the <a href="https://storage.cloud.google.com/download.tensorflow.org/data/speech_commands_v0.01.tar.gz">dataset</a> I used in this post. And thank you dear reader,
for reading this far.</p>

<p>I'm stoked to see how this kind of browser based audio recognition tech can be
applied to exciting, educational ML projects like <a href="https://teachablemachine.withgoogle.com/">Teachable
Machine</a>. How cool would it be if you could make a
self-improving system, which trains on every additional spoken utterance? The
ability to train these kinds of models in the browser allows us to entertain
such possibilities in a privacy preserving way, without sending anything to any
server.</p>

<p>So there you have it! This has been an explanation of voice command recognition
on the web. We covered feature extraction in the <a href="/web-audio-ml-features">previous
post</a>, and this time, dug a little bit into <a href="https://google.github.io/web-audio-recognition/train-model/?data_url=https://storage.googleapis.com/audio-recognition-data&amp;data_extension=mp3">model
training</a> and <a href="https://google.github.io/web-audio-recognition/inference-demo/?model=yesno">real-time inference</a> entirely in the browser.</p>

<p>If you build on this example, please drop me a note on
<a href="https://twitter.com/borismus">twitter</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Audio features for web-based ML</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/web-audio-ml-features"/>
    
    <updated>2017-12-15T09:00:00-00:00</updated>
    
    <id>https://smus.com/web-audio-ml-features</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>One of the first problems presented to students of deep learning is to classify
handwritten digits in the <a href="https://www.tensorflow.org/get_started/mnist/beginners">MNIST dataset</a>. This was recently <a href="https://deeplearnjs.org/demos/model-builder/">ported to
the web</a> thanks to <a href="https://deeplearnjs.org">deeplearn.js</a>. The web version has
distinct educational advantages over the relatively dry TensorFlow tutorial.
You can immediately get a feeling for the model, and start building intuition
for what works and what doesn't. Let's preserve this interactivity, but change
domains to audio. This post sets the scene for the auditory equivalent of MNIST.
Rather than recognize handwritten digits, we will focus on recognizing spoken
commands. We'll do this by converting sounds like this:</p>

<p><audio src="/web-audio-ml-features//web-audio-ml-features/left.wav" controls></audio></p>

<p>Into images like this, called log-mel spectrograms, and in the <a href="/web-voice-command-recognition/">next post</a>,
feed these images into the same types of models that do handwriting recognition
so well:</p>

<p><img src="/web-audio-ml-features/final-log-mel-spectrogram.png" alt="Final log-mel spectrogram." /></p>

<p>The audio feature extraction technique I discuss here is generic enough to work
for all sorts of audio, not just human speech. The rest of the post explains
how. If you don't care and just want to <a href="https://github.com/google/web-audio-recognition/tree/master/audio-features">see the code</a>, or <a href="https://google.github.io/web-audio-recognition/audio-features/">play with some
live demos</a>, be my guest!</p>

<!--more-->

<h1>Why?</h1>

<p>Neural networks are having quite a resurgence, and for good reason. Computers
are beating humans at many challenging tasks, from identifying faces and images,
to playing Go. The basic principles of neural nets is relatively simple, but the
details can get quite complex. Luckily non-AI experts can get a feeling for what
can be done because a lot of <a href="http://www.cs.ubc.ca/~van/papers/2017-TOG-deepLoco/">output</a> is <a href="https://www.youtube.com/watch?v=5h4R959O0cY">quite</a> <a href="http://prostheticknowledge.tumblr.com/">engaging</a>.
Unfortunately these demos are mostly visual in nature, either examples of
computer vision, or generate images or video as their main output. And
few of these examples are interactive.</p>

<h1>Pre-processing audio sounds hard, do we have to?</h1>

<p>Raw audio is a pressure wave sampled at tens of thousands times per second and
stored as an array of numbers. It's quite a bit of data, but there are neural
networks that can ingest it directly.  Wavenet does <a href="https://github.com/buriburisuri/speech-to-text-wavenet">speech to
text</a> and <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">text to speech</a> using raw audio sequences,
without any explicit feature extraction. Unfortunately it's slow: running speech
recognition on a 2s example took 30s on my laptop. Doing this in real-time, in
a web browser isn't quite ready yet.</p>

<p>Convolutional Neural Networks (CNNs) are a big reason why there has been so much
interesting work done in computer vision recently. These networks are designed
to work on matrices representing 2D images, so a natural idea is to take our raw
audio and generate an image from it. Generating these images from audio is
sometimes called a frontend in <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43960.pdf">speech recognition papers</a>. Just to
hammer the point home, here's a diagram explaining why we need to do this step:</p>

<p><img src="/web-audio-ml-features/front-end-diagram.png" alt="Audio processing vs. image processing" /></p>

<p>The standard way of generating images from audio is by looking at the audio
chunk-by-chunk, and analyzing it in the frequency domain, and then applying
various techniques to massage that data into a form that is well suited to
machine learning. This is a common technique in sound and speech processing, and
there are great implementations in <a href="https://github.com/librosa/librosa">Python</a>. TensorFlow even has a
<a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/wav_to_spectrogram/wav_to_spectrogram.cc">custom op</a> for extracting spectrograms from audio.</p>

<p>On the web, these tools are lacking. The Web Audio API can almost do
this, using the <code>AnalyserNode</code>, as I've shown <a href="https://borismus.github.io/spectrogram/">in the past</a>, but
there is an important limitation in the context of data processing:
<code>AnalyserNode</code> (nee <code>RealtimeAnalyser</code>) is <a href="https://stackoverflow.com/questions/45697898/web-audio-api-getfloatfrequencydata-function-setting-float32array-argument-data">only for real-time</a> analysis.
You can setup an <code>OfflineAudioContext</code> and run your audio through the analyser,
but you will get unreliable results. </p>

<p>The alternative is to do this without the Web Audio API, and there are
<a href="https://github.com/vail-systems/node-mfcc">many</a> <a href="https://github.com/oramics/dsp-kit">signal processing</a> <a href="https://github.com/corbanbrook/dsp.js/">JavaScript libraries</a> that might
help. None of them are quite adequate, for reasons of incompleteness or
abandonment. But here's an illustrated take on extracting Mel features from raw
audio.</p>

<h1>Audio feature extraction</h1>

<p>I found an <a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">audio feature extraction tutorial</a>, which I followed closely
when implementing this feature extractor in TypeScript. What follows can be a
useful companion to that tutorial.</p>

<p>Let's begin with an audio example (a man saying the word "left"):</p>

<p><audio src="/web-audio-ml-features//web-audio-ml-features/left.wav" controls></audio></p>

<p>Here's that raw waveform plotted as pressure as a function of time:</p>

<p><img src="/web-audio-ml-features/1-raw-audio.png" alt="Raw audio" /></p>

<p>We could take the FFT over the whole signal, but it changes a lot over time.
In our example above, the "left" utterance only takes about 200 ms, and most of
the signal is silence. Instead, we break up the raw audio signal into
overlapping buffers, spaced a hop length apart. Having our buffers overlap
ensures that we don't miss out on any interesting details happening at the
buffer boundaries. There is an art to picking the right
buffer and hop lengths:</p>

<ul>
<li>Pick too small a buffer, and you end up with an overly detailed image, and
risk your neural net training on some irrelevant minutia, missing the forest
for the trees. </li>
<li>Pick too large a buffer, and you end up with an image too coarse to be useful.</li>
</ul>

<p>In the illustration below, you can see five full buffers that overlap one
another by 50%. For illustration purposes only, the buffer and hop durations are
large (400 ms and 200ms respectively). In practice, we tend to use much shorter
buffers (eg. 20-40 ms), and often even shorter hop lengths to capture minute
changes in audio signal.</p>

<p><img src="/web-audio-ml-features/2-buffer-hop.png" alt="Break-up audio" /></p>

<p>Then, we consider each buffer in the frequency domain. We can do this using an
Fast Fourier Transform (FFT) algorithm. This algorithm gives us complex values
from which we can extract magnitudes or energies. For example, here are the FFT
energies of one of the buffers, approximately the second one in the above image,
where the speaker begins saying the "le" syllable of "left":</p>

<p><img src="/web-audio-ml-features/3-fft-buffer-linear.png" alt="Frequency of buffer" /></p>

<p>Now imagine we do this for every buffer we generated in the previous step, take
each FFT arrays and instead of showing energy as a function of frequency, stack
the array vertically so that y-axis represents frequency and color represents
energy. We end up with a spectrogram:</p>

<p><img src="/web-audio-ml-features/4-fft-spectrogram.png" alt="STFT spectrogram" /></p>

<p>We could feed this image into our neural network, but you'll agree that it looks
pretty sparse. We have wasted so much space, and there's not much signal there
for a neural network to train on.</p>

<p>Let's jump back to the FFT plot to zoom our image into our area of interest. The
frequencies in this plot are bunched up below 5 KHz since the speaker isn't
producing particularily high frequency sound. Human audition tends to be
logarithmic, so we can view the same range on a log-plot:</p>

<p><img src="/web-audio-ml-features/5-fft-buffer-log.png" alt="Frequency of buffer" /></p>

<p>Let's generate new spectrograms as we did in an earlier step, but rather than
using a linear plot of energies, use can a log-plot of FFT energies:</p>

<p><img src="/web-audio-ml-features/9-log-spectrogram.png" alt="STFT log spectrogram" /></p>

<p>Looks a bit better, but there is room for improvement. Humans are much better at
discerning small changes in pitch at low frequencies than at high frequencies.
The Mel scale relates pitch of a pure tone to its actual measured frequency. To
go from frequencies to Mels, we create a triangular filter bank:</p>

<p><img src="/web-audio-ml-features/6-mel-filterbank.png" alt="Mel filter bank" /></p>

<p>Each colorful triangle above is a window that we can apply to the frequency
representation of the sound. Applying each window to the FFT energies we
generated earlier will give us the Mel spectrum, in this case an array of 20
values:</p>

<p><img src="/web-audio-ml-features/7-mel-spectrum.png" alt="Mel spectrum" /></p>

<p>Plotting this as a spectrogram, we get our feature, the log-mel spectrogram:</p>

<p><img src="/web-audio-ml-features/10-mel-spectrogram.png" alt="Mel spectrogram" /></p>

<p>The 1s images above are generated using audio feature extraction software
written in TypeScript, which I've released publicly. Here's a <a href="https://google.github.io/web-audio-recognition/audio-features/">demo</a> that
lets you run the feature extractor on your own audio, and <a href="https://github.com/google/web-audio-recognition/tree/master/audio-features">the code on
github</a>.</p>

<h1>Handling real-time audio input</h1>

<p>By default the feature extractor frontend takes a fixed buffer of audio as
input.  But to make an interactive audio demo, we need to process a continuous
stream of audio data. So we will need to generate new images as new audio comes
in. Luckily we don't need to recompute the whole log-mel spectrogram every time,
just the new parts of the image. We can then add the new parts of spectrogram on
the right, and remove the old parts, resulting in a movie that feeds from the
right to the left. The <a href="https://github.com/google/web-audio-recognition/blob/master/audio-features/src/StreamingFeatureExtractor.ts"><code>StreamingFeatureExtractor</code></a> class implements this
important optimization.</p>

<p>But there is one caveat: it currently relies on <code>ScriptProcessorNode</code>, which is
notorious for dropping samples. I've tried to mitigate this as much as possible
by using a large input buffer size, but the real solution will be to use
<a href="https://drafts.css-houdini.org/worklets/#worklet-section">AudioWorklets</a> when they are available.</p>

<h1>Wrapping up</h1>

<p>An implementation note: here is a <a href="https://thebreakfastpost.com/2015/10/18/ffts-in-javascript/">comparison of JS FFT libraries</a> which
suggests the Emscripten-compiled KissFFT is the fastest (but still 2-5x slower
than native), and the one I used.</p>

<p>Here is a sanity check comparing the output of my web-based feature extractor to
that of other libraries, most notably <a href="https://github.com/librosa/librosa">librosa</a> and from <a href="https://github.com/tensorflow/models/blob/master/research/audioset/mel_features.py">AudioSet</a>:</p>

<p><img src="/web-audio-ml-features/mel-comparison.png" alt="Log mel feature comparison" /></p>

<p>The images resulting from the three implementations are similar, which is a good
sanity check, but they are not identical. I haven't found the time yet, but it
would be very worthwhile to have a consistent cross platform audio feature
extractor, so that models trained in Python/C++ could run directly on the web,
and vice versa.</p>

<p>I should also mention that although log-mel features are commonly used by
serious audio researchers, this is an active area of research. Another audio
feature extraction technique called <a href="https://arxiv.org/pdf/1607.05666.pdf">Per-Channel Energy Normalization
(PCEN)</a> appears to perform better at least in some cases, like processing
far field audio. I haven't had time to delve into the details yet, but
understanding it and porting it to the web also seems like a worthy task.</p>

<p>Major thanks to <a href="http://www.dicklyon.com/">Dick Lyon</a> for pointing out a few bugs in my feature
extraction code. Pick up his <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">"Human and Machine Hearing"</a> if you're ready
to delve deeper into sound understanding.</p>

<p>Ok, so to recap, we've generated log-mel spectrogram images from streaming audio
that are ready to feed into a neural network. Oh yeah, the actual machine
learning part? That's the <a href="/web-voice-command-recognition/">next post</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UIST 2017 highlights</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/uist-2017"/>
    
    <updated>2017-10-28T09:00:00-00:00</updated>
    
    <id>https://smus.com/uist-2017</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Picking up where I left off <a href="/uist-2014">3 years ago</a> with this year's UIST
highlight reel. As expected, the research creatively applied interesting
principles, but many applications were adorably contrived. Also, I miss academia!</p>

<!--more-->

<p><style>
.container {
    position: relative;
    width: 100%;
    height: 0;
    padding-bottom: 56.25%;
}
.video {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}
</style></p>

<h2>WhichFingers / characterizing end-to-end latency</h2>

<p>Both papers used a finger-mounted vibration sensor made from a piezoelectric
polymer, and implemented simple finger tap detection.  One showed how this
method could be used for measuring latency using a blinking screen. They did a
great job of measuring latency at every stage.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/s2iUJsm7JcI" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>The other paper had vibration sensors for each finger, and did time-based
correlation to determine which finger tapped a screen. They had a 30ms window
within which you had to fall to have unambiguous classification.  Interesting:
they also moved their vibration sensors up to the ring position (third phalanx),
and seemed to also get pretty good results. Also nice is that they <a href="http://cristal.univ-lille.fr/~casiez/whichfingers/">open sourced
everything</a>, and that the whole system is cheaper than $35.</p>

<h2>Carpacio: unobtrusive passenger &amp; driver ID</h2>

<p>The cheesily named project served to repurpose capacitative screens to
differentiate between drivers and passengers. Motivated by driver distraction,
cars are now shipping with on-screen UIs that are locked until you answer an
onscreen question: are you a driver or passenger?  This is counter-productive.
Edward Jay Wang from UW showed how the car system can identify who is
interacting with it automatically. Both seats are instrumented with signal
transmitters, transmitting a unique signal through each of the sitting people,
and the screen is instrumented with a receiver. The rest is signal correlation.
Then they did validation and got really good results (for research): &gt; 99%
accuracy.</p>

<p>Zooming out for a second, I really wish that this wasn't a problem that needed
to be solved. Physical controls work better in a car, and allow the driver to
interact with them more easily.</p>

<h2>Grabity: ungrounded haptics for grasping</h2>

<p>This project is an end-to-end look at an ungrounded device that provided haptic
feedback for grasping an object with your hand. They detailed each stage:
grabbing it, picking it up, feeling its weight, then moving it around and
feeling its inertia.</p>

<p><img src="/uist-2017/grabity.jpg" alt="Grabity steps in action" /></p>

<p>Weight is perceived by multiple senses in the human body: muscle spindles (eg.
in biceps), golgi tendons (eg. in elbow), mechanoreceptors sensing shear (eg. in
palm and fingers). Their idea: only use mechanoreceptors in hand, which lets you
simulate the equivalent of about 15g of weight. It was cool that they compared
the sensation to an actual weight with real users. I've seen prior work which
relies on asymmetric vibration to create virtual forces through skin
displacement. A few limitations: it looks pretty wacky, and users thought that
the constant vibration was annoying, but pretty cool tech demo.</p>

<h2>Dodecapen: 6DOF Pose Tracking System</h2>

<p>This project puts a dodecahedron-shaped AR marker on the end of a regular pen. A
cube didn’t work because of pose ambiguity, and other Platonic solids have
triangular faces that are relatively small in area.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/7Xczpq4VkHM" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>3D printing a perfect dodecahedron is hard, so they needed a calibration step
for it. The pen-tip also needs to be calibrated, since pens vary in size and
shape. Also the paper surface itself had to be calibrated. That's a lot of
calibrations! They used a mocap system with 8 markers to create ground truth.
Looked at the mean shortest distance and saw mm-level precision. They claim that
precision is comparable to a mocap system with 10 active cameras.</p>

<p>I also tried the demo, which worked very well. One caveat is that it relies on a
global shutter camera, so not sure how well it generalizes to smartphones yet.
Another big limitation here is that the dodecahedron needs to be in the camera’s
FOV.</p>

<h2>Inviso: 3D design environment to create sonic VR</h2>

<p>Anil Camci showed a web-based GUI to define point sources as well as regions for
playing back ambisonic soundscapes. Point sources could have multiple cones that
you could attach with a graphical tool, and also adjust their direction, angles,
and amplitude. Each point source, and the observer could also be animated over a
trajectory. You could then play it back through the Web Audio API. Most of the
design is done from a top-down view, since this is a good match to human
auditory perception, which is much better at lateral than vertical
source differentiation.</p>

<p><img src="/uist-2017/inviso.jpg" alt="Inviso screenshot" /></p>

<p>In the end he showed multiple examples of people using the system and creating
complex auditory scenes, including overlapping zones, and animated, multi-coned
point sources. Seems to be quite efficient for building up immersive auditory
environments.</p>

<p>I encouraged them to recreate classic spatial audio demos, like my favorite
barbershop example. The editor is <a href="http://inviso.cc/">available online</a>, and
just recently <a href="https://github.com/CreativeCodingLab/Inviso">open sourced</a>.</p>

<h2>SoundCraft: smart watch interaction using hand generated acoustics</h2>

<p>This project features a microphone array that fits onto a watch form factor.
They can localize sounds made by the other hand against itself (what is the
sound of one hand clapping?) or against another surface. Localization gives you
angles relative to the watch (angular resolution ~15 deg).</p>

<div class="container">
<iframe src="//www.youtube.com/embed/O1G-j1EBQh0" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>The applications were kind of weak, but having a small aperture (1/2 inch
between microphones) and wearable form factor was an interesting idea. They also
showed tracking the lateral position of a finger on the same hand as the watch,
and using the watch as an audio marker for an AR application.</p>

<h2>Data storage &amp; interaction using magnetized fabric</h2>

<p>Justin Chan from UW showed what can be achieved when conductive fabric (woven
out of conductive thread) is magnetized using a permanent magnet. First they
explored various properties of the medium, like how long it stays magnetized
(28% decrease per week), how far the field can be detected from (up to 1cm), how
its properties vary with weave pattern (better for dense patterns), and whether
magnetization decreases from washing (not very much). The basic idea is that you
can encode data onto a conductive thread by having conductive segments that can
be magnetized interleaved with insulated segments. A binary string is then
encoded by mapping 0 to one polarity and 1 to the other.</p>

<p><img src="/uist-2017/magnetized-fabric.jpg" alt="Magnetized fabric" /></p>

<p>They also showed use cases, such as writing and reading data from the clothing,
or even engraving data images (smallest pixel using a regular loom was 1.8cm^2)
onto clothing. They also showed some interactive examples where the clothing can
be read by a magnetometer on a phone or watch. This is interesting especially
because it doesn't require any batteries, and can be achieved using
off-the-shelf embroidery machines. </p>

<h2>iSoft: soft touch and stretch sensor</h2>

<p>This project uses electrical impedance tomography (EIT, which I know nothing
about, and wasn't explained) to detect touch and stretch in (1D) in a special
elastomer which is designed to be affordable. Discrete contact accuracy was
really good: 96%, using a small dataset. Distance error was ~10% of sensor size.</p>

<div class="container">
<iframe src="//www.youtube.com/embed/JVaYEl9nbME" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>Applications (as usual, weak): a bunch of instrumented objects (lamp, cup,
clothing, travel pillow, etc). Limitations: hard to make it big, hard to make it
arbitrarily shaped, doesn’t support multidimensional stretching.</p>

<h2>Mutual human actuation</h2>

<p>I missed most of the VR sessions because parallel tracks were more interesting,
but caught this one because it was super clever. Previously, Haptic Turk used
support people to lift and lower the player immersed in VR. TurkDeck was an
environment for players that is constructed by support people outside VR.
However, previous research found that players enjoyed the experience more than
the support people, so Mutual Turk (presented here) made all participants into
players. Two people are in VR, but they are playing different games that are
complimentary in force (eg. A is fishing, B is flying a kite.)</p>

<div class="container">
<iframe src="//www.youtube.com/embed/JKGdQjx-_BI" 
frameborder="0" allowfullscreen class="video"></iframe>
</div>

<p>This generalizes to two things: 1. shared props through which users exchange
forces, and 2. synchronization of state between the people involved. They had a
few other examples, like having one player fan a campfire, which created wind
for the other player in the middle of a storm. As far as practicality, obviously
this requires multiple people to be co-present, and also requires multiple
players to synchronize up to the action sequence.</p>

<h2>Demos, Posters, Contests</h2>

<p>Apparently you can print piezoelectric elements. I tried a set that emits low
frequency vibrations for haptic feedback. But you need a special print process
(won’t work on an inkjet).</p>

<p>Artem Dementyev built a robot that walks along human skin using pneumatics.
Supposed applications in health care.</p>

<p>Pepper’s cone was a great illusion from Steve Seitz’s student. Nice improvement
over the usual ghost setup, which has four discrete viewing points, where here
you can pick any point in a circle.</p>

<p>Gierad showed a poster about a multimodal sensing platform that plugs right into
the wall and which can track all sorts of activity in the house, using high
frequency accelerometers, microphones, and other sensors.</p>

<p>Hanchuan Li from UW showed a poster summarizing some of his PhD projects,
including passive sensing, IR based user localization, and various power
harvesting techniques.</p>

<p>Jihyeon Janel Lee showed her poster which automatically created accompanying
visualizations for travel podcasts via geographic entity extraction.</p>

<p>The good folks at i.am showed a way of making TTS sound better by extracting
contours from speech spoken by a person onto a TTS-synthesized example.</p>

<p>The student design contest featured creative ideas implemented with the help of
an Arduino Braccio robot arm. One project required multiple people to operate
one arm. Each person had to do a gesture to operate one degree of freedom of the
arm. Another project involved two front body-mounted robot arms that fed the
other person. Yet another featured an improv show where an actor operated a sock
puppet, and a robot arm spoke lines that were crowdsourced by the audience.</p>

<h2>Keynotes...</h2>

<p><strong>Gabriella Coleman</strong> kicked off UIST 2017 with a slightly off-topic, but
nevertheless interesting (and timely) keynote, summarizing hacker culture from a
historical and anthropological perspective. Much of it was review for me, but I
also learned a fair amount:</p>

<ul>
<li>The origin of "hacker" comes from the Tech Model Railroad Club from MIT. Now
James' fascination with model railroads is crystallizing.</li>
<li>Some of the earlier phreakers were blind, and phreaking was empowering. Some
were (sometimes additionally) gifted with perfect pitch, and could whistle the
correct frequency to emulate phone network signals.</li>
<li>The speaker's personal beliefs were very anti-US, and she was very aligned
with the hackers. Perhaps it is not coincidental that she is at McGill now.</li>
</ul>

<p><strong>Upon receiving a lasting impact award</strong>, Ivan Popyrev gave a short summary of his
storied career. I liked his frankness on the topic of VR: "With every VR fad,
the next fad is the AR fad". At one point he also suggested that we need to
break away from “heavy reliance on rich visual feedback and undivided attention
from the user" that are common in today’s UIs. I completely agree. </p>

<p>I liked his succinct vision: "How can we make the whole world augmented and
interactive?", which he then elaborates on as having two parts: </p>

<ol>
<li>Augment physical, tactile &amp; proprioceptive channels of human augmentation &amp; actuation.</li>
<li>Deliver novel UI at scale by integrating them into everyday physical objects.</li>
</ol>

<p>I should come up with one for my HCI work. </p>

<p><strong>Niki Kittur</strong> closed off the conference with a summary of his crowdsourcing work,
and I was quite glad to see CrowdForge prominently mentioned in the beginning.
He did a good job weaving a coherent narrative throughout the last five years of
his career, which covered disparate topics such as peer production,
crowdsourcing and web browsing. I often wonder how much of that storytelling is
premeditated, and how much of it is done post-hoc. He showed off some neat
projects, including Alloy (name perhaps forge-related?) and KA, as well as
<a href="https://iotabrowser.com">Iota, a task-oriented browser</a> inspired by V. Bush's vision of people
leaving trails of knowledge for one another.</p>

<p>I skipped the last few UISTs, and it was nice to say hi to some familiar faces,
especially from CMU. I also met some great people, including Danny Fan from an
<a href="http://i.am">audio startup</a> in LA, some talented grad students and fortuitously
re-connected with old acquaintances (Hi Trevor) who are apparently also based in
Seattle. </p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Memento Mori</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/memento-mori"/>
    
    <updated>2017-10-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/memento-mori</id>
    <content type="html">
      <![CDATA[
      <div>
        <blockquote>
  <p>The association of sundials with time has inspired their designers over the
  centuries to display mottoes as part of the design. Often these cast the
  device in the role of memento mori, inviting the observer to reflect on the
  transience of the world and the inevitability of death. – <a href="https://en.wikipedia.org/wiki/Sundial#Sundial_mottoes">Wikipedia</a></p>
</blockquote>

<p><img src="/memento-mori/watch-screenshot.png" alt="WatchKit screenshot" 
  style="margin: 0 auto; display: block; width: 200px" /></p>

<p>This rich tradition is now available on <a href="https://itunes.apple.com/us/app/memento-mori-apple-watch/id1294913922">Apple Watch</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>A respectful truce?</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/a-respectful-truce/"/>
    
    <updated>2017-08-24T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/a-respectful-truce/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's a fact that men greatly outnumber women in software engineering. As for
why, there is a fundamental disagreement between social constructivists and
evolutionary psychologists. </p>

<!--more-->

<p>Constructivists say that it’s because of systematic oppression against women.
Boys in the 80s grew up with computers, learned programming and made life a
living hell for their female classmates in CS101. <a href="http://www.npr.org/sections/money/2014/10/21/357629765/when-women-stopped-coding">Planet Money</a> has the
scoop. Evolutionary psychologists say that it’s because men and women have
evolutionarily caused <a href="https://heterodoxacademy.org/2017/08/10/the-google-memo-what-does-the-research-say-about-gender-differences/">differences in interests</a> and point to studies
indicating that progressive countries have larger gender representation gaps
because women are free to choose <a href="https://slatestarcodex.com/2017/08/07/contra-grant-on-exaggerated-differences/">careers based on interest</a>. I find
it likely that both positions contribute in part to the gender representation
disparity. Compelling arguments can be made for and against both, and both are
rooted in academic disciplines <a href="https://en.wikipedia.org/wiki/Replication_crisis">that are dangerously squishy</a>. As a
result, I don’t think we currently have a good scientific way of determining
this.</p>

<p>So what; science, schmience! Followers of the evo psych and constructivism are
at war. One continuously pisses the other off with their inflamatory rhetoric.
It is true that white males are historically privileged, but hammering “white
male privelege” into their heads predictably puts white males into a defensive
stance. Similarly, discussing how evo psych means that women are less interested
and therefore less capable of software engineering will predictably annoy
females, especially those rightfully proud of their software engineering
prowess, and tired of dealing with similar allegations for their whole career.</p>

<p>I've spent way too much time reading and thinking about this lately, and it
saddens me to conclude that the wisest course of action is to avoid discussing
this topic entirely (oops, too late). Scott Aaronson, Sarah Constantin and
Stacey Jeffery propose a <a href="https://www.scottaaronson.com/blog/?p=3389">respectful truce</a> between the two camps.
Advocates of evolutionary psychology should:</p>

<blockquote>
  <p>do everything they can to foster diversity, including by creating environments
  that are welcoming for women, and by supporting affirmative action, women-only
  scholarships and conferences, and other diversity policies and also agree
  never to talk in public about possible cognitive-science explanations for
  gender disparities in which careers people choose, or overlapping bell curves,
  or anything else potentially inflammatory."</p>
</blockquote>

<p>Meanwhile, social constructivists should:</p>

<blockquote>
  <p>avoid libelling [white men] as misogynist monsters, who must be scaring all
  the women away with their inherently gross, icky, creepy, discriminatory
  brogrammer maleness.</p>
</blockquote>

<p>Truce?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Filter playground</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/filter-playground"/>
    
    <updated>2017-08-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/filter-playground</id>
    <content type="html">
      <![CDATA[
      <div>
        <script src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=AM_HTMLorMML"></script>

<p><style>
iframe {
  width: 640px;
  height: 480px;
  border: 0;
}
</style></p>

<blockquote>
  <p>"You don't understand anything until you learn it more than one way." – Marvin Minsky</p>
</blockquote>

<p>In my <a href="http://chimera.labs.oreilly.com/books/1234000001552/">short Web Audio book</a>, I covered the <code>BiquadFilterNode</code>, but didn't
have any sense for how it worked. As I sat down to read <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">Human and Machine
Hearing</a>, it became clear that I needed to catch up on some digital
filtering fundamentals.</p>

<p>What follows is an introduction to digital filters via <a href="http://explorabl.es/">explorable
explanation</a> I built to help myself better understand some DSP concepts.
The approach I took was to try to present the concept as visually and aurally as
possible, maximizing opportunities to build intuition. I learned a lot in the
process. Read on for a introduction, jump ahead to the <a href="https://borismus.github.io/filter-playground/">Filter Playground</a>,
or check out this video:</p>

<iframe width="600" height="338" src="//www.youtube.com/embed/6OIOTpQYsts?rel=0&amp;controls=0&amp;showinfo=0" frameborder="0" allowfullscreen></iframe>

<!--more-->

<h1>Cycles are everywhere</h1>

<p>The world is full of examples of cyclical phenomena. Large cycles include
planetary motion, seasons, tides, and ocean waves. Societies are governed by
cycles: empires rise and fall, economies boom and bust, and fashion keeps
repeating itself. On an individual scale, the human lifecycle, eating and
sleeping, heartbeats and breathing, and locomotion are all periodic.  And so are
sound and light, the very nature of the world we percieve.</p>

<p>Wouldn't it be nice to understand and manipulate cyclical phenomena? A couple
hundred years ago, <a href="https://en.wikipedia.org/wiki/Fourier_transform">Fourier showed</a> that any repeating periodic signal,
regardless of its complexity, can be represented as a sum of sine functions,
paving the way for much deeper signal understanding. More recently, <a href="https://en.wikipedia.org/wiki/Nyquist%E2%80%93Shannon_sampling_theorem">Shannon
showed</a> that you can take analog signals and represent them as numbers,
which brings us to digital signal processing. For modifying digital signals, we
need to look to digital filters.</p>

<h1>The surprising link between periodic signals and difference equations</h1>

<p>Bear with me on a brief mathematical tangent. Let's start with a simple
difference equation that gives you an output sequence of numbers y[n] given
an input sequence x[n]:</p>

<div>
`y[n] = bx[n] + ay[n - 1]`
</div>

<p>Given `x = [1, 2, 3], a = b = 0.5`, we can calculate y[n] with simple
arithmetic, assuming that `y[-k] = 0`:</p>

<div>
`y[n] = 0.5 x[n] + 0.5 y[n - 1]`
`y[0] = 0.5 x[0] + 0.5 y[-1] = 0.5 * 1 + 0.5 * 0 = 0.5`
`y[1] = 0.5 x[1] + 0.5 y[0] = 0.5 * 2 + 0.5 * 0.5 = 1.25`
`y[2] = 0.5 x[2] + 0.5 y[1] = 0.5 * 3 + 0.5 * 1.25 = 2.125`
</div>

<p>And so, given `x[n] = [1, 2, 3]` and the above difference equation, we get
`y[n] = [0.5, 1.25, 2.125]`. Computers can do this sort of arithmetic really
really quickly. But so what? And what does this have to do with our goal of
manipulating periodic signals in general? Let's explore a bit more generally.</p>

<p>We can take any function x(t) and sample it numerically to get a sequence x[n].
For example, if we take `x(t) = t^2` and sample every integer from 1 to 10, we
would get `x[n] = [x(1), x(2), ..., x(10)]`, in blue. Next, calculate y[n] and
color it green:</p>

<p><img src="/filter-playground/sampling.png" alt="x(t) = t^2 sampled for the first 10 integers" /></p>

<p>Let's take several x(t), sample them into x[n], and see what the above
difference equation gives us for y[n]. When we plot x[n] in blue and y[n] in
green, we get the following graphs:</p>

<p><img src="/filter-playground/math-filter-plot.png" alt="Several discretized mathematical functions with x in blue, and y in
green" /></p>

<p>The first row shows the result of our difference equation on some mathematical
functions: `t^2`, `2^t`, `tan(0.1 t)`, and very little relationship
between x and y. The second row shows some sinusoidal functions with varying
period and phase in blue, sampled and run through the same difference equation
and shown in green. The pattern starts to become pretty clear: sine functions
are special in the same way. The generalized result is surprising and
awesome: if you take any sine function, and feed it into any difference
equation, you end up with another sine function  with the same frequency, but a
different amplitude (<a href="https://en.wikipedia.org/wiki/Linear_time-invariant_theory#Exponentials_as_eigenfunctions_2">gory details here</a>).</p>

<p>We've looked at just one specific difference equation: `y[n] = 0.5 x[n] + 0.5
y[n-1]`. Let's look at difference equations in general, and see how they
affect the relationship between x[n] and y[n]. </p>

<h1>Transfer functions describe the behavior of filters</h1>

<p>We just saw that for an input sine function with a frequency and amplitude, a
difference equation will produce an output sine function with another amplitude.
In the `sin(0.1 t)` graph above, we can see that the amplitude of the green
graph is 0.5. If we look across all frequencies `x(t) = sin(omega t)` and see
how difference equations change the amplitude for all `omega`. This will give
us the frequency response for this specific difference equation. But what
happens in general? </p>

<p>If we bring our difference equation into a canonical form, we can apply the
<a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a> and get a corresponding transfer function. The math to
derive this is complicated, but the bottom line is that for any difference
equation in the general form:</p>

<div>
`y[n] = 1/a_0 (b_0 x[n] + b_1 x[n - 1] + ...) - (a_1 y[n - 1] + a_2 y[n - 2] + ...)`
</div>

<p>The corresponding transfer function is of this form:</p>

<div>
`H(z) = (b_0 + b_1 z^(-1) + b_2 z^(-2) + ...) / (a_0 + a_1 z^(-1) + a_2 z^(-2) + ...)`
</div>

<p>Let's go back to our example, `y[n] = ax[n] + by[n − 1]`. We can
see that this fits the general form, with `a_0 = 1`, `b_0 = b`, `a_1 =
a`, with all of the other `a_i = b_i = 0`. Plugging in these values, the
transfer function for this example is:</p>

<div>
`H(z) = (b)/(1 + (-a)z^(-1)) = (bz)/(z - a)`
</div>

<p>This transfer function can tell us a lot about the behavior of the difference
equation, and ultimately its frequency response.</p>

<h1>Visual intuition around transfer functions</h1>

<p>But what the heck does this transfer function tell us?! Let's try to build some
visual intuition. Firstly you see that our H(z) is a rational polynomial.
This means the numerator and denominator both have roots.  Numerator roots are
called zeros and denominator zeros are called poles. From high school math,
recall that we can plot roots on a number line. Except in this case, we're also
interested in imaginary roots, and we can plot them on the complex plane. O's
represent zeros, X's represent poles. For our H(z), the numerator is `0.5 z`,
which has a root (zero) at zero, and the denominator is `z - 0.5` with a root
(pole) at 0.5, and so plotting poles and zeros, we get:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=pole-zero&equation=(0.5z)/(z-0.5)&mutable=false"></iframe>

<p>Here's how you can think about zeros and poles: if z is really close to a zero,
|H(z)| will be close to zero. If it's really close to a pole, |H(z)| will blow
up, approaching infinity. Now, imagine you took a rubber sheet and draped it
over the complex plane. Now put stones where the O's are, and telescoping tent
poles where the X's are, and then extended the tent poles to be really tall. You
would end up with a circus tent. This is the surface that we get by plotting
complex values of z (along the x-y plane) and using |H(z)| as the height
corresponding to each z.</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=COMPLEX&mutable=false"></iframe>

<p>We can recover the pole-zero diagram by taking a birds-eye view at the same
circus tent, and plotting the poles and zeros on the xy-plane:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=POLEZERO&mutable=false"></iframe>

<p>The neat thing about this circus tent is that it tells you the frequency
response of the filter. To do that, we look at the unit circle as it sits on top
of the circus tent diagram:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=transfer-function&equation=(0.5z)/(z-0.5)&tfmode=BODE&mutable=false"></iframe>

<p>Which we can unwrap into a frequency response plot by looking at `|H(e^(i
omega))|` with `omega in [0, pi]`:</p>

<iframe src="https://borismus.github.io/filter-playground/?view=bode-plot&equation=(0.5z)/(z-0.5)"></iframe>

<p>Now let's see to what our example filter does to a white noise sample. Here is
the frequency response of the sample played through our filter according to a
Web Audio <code>AnalyserNode</code> (you can also <a href="https://borismus.github.io/filter-playground/?equation=(0.5z)/(z-0.5)">hear the filter in the Filter
Playground</a>):</p>

<iframe src="https://borismus.github.io/filter-playground/?view=audio-player&view=audio-visualizer&equation=(0.1z)/(z-0.9)&muted=true"></iframe>

<p>Since noise has equal power across the frequency range, it is a good end-to-end
test. We expect the <code>AnalyserNode</code>'s frequency response to line up closely with
the bode plot in the previous figure, and they do.</p>

<p>The filter we created above is a low pass filter, meaning that it allows low
frequencies to pass, but attenuates high frequencies. Let's look at other kinds
of filters.</p>

<h1>More complex filters</h1>

<p>Our first filter had one pole and one zero. What happens if we make a filter
with two poles and two zeros? Here's an example:</p>

<p>`H(z) = (z^2-1)/(z^2-1.975z+0.99) `</p>

<iframe src="https://borismus.github.io/filter-playground/?view=pole-zero&equation=(z^2-1)/(z^2-1.975z+0.99)&mutable=false"></iframe>

<p>This example has two poles and two zeros, which is also called a Biquad filter.
The Web Audio <code>BiquadFilterNode</code> has <a href="https://www.w3.org/TR/webaudio/#idl-def-BiquadFilterType">8 different filter types</a>, all
of which are implemented with two poles and two zeros.</p>

<p>The filter in question is called bandpass filter, because it allows a band of
frequencies through, and attenuates the rest. Go ahead and open this in <a href="https://borismus.github.io/filter-playground/?equation=(z^2-1)/(z^2-1.975z+0.99)">the
filter playground</a>, and you'll see a variety of views of this
bandpass filter.</p>

<p>The point of a playground isn't just to look at other people playing, it's to
play with them!  So I invite you to try it out. You can generate a biquad filter
by selecting parameters with the filter wizard, or input any H(z) manually,
or move around poles and zeros visually. Using the pole-zero view, you can add
poles and zeros with buttons, or remove them by dragging them far enough out of
the unit circle. Check out <a href="https://youtu.be/6OIOTpQYsts">my YouTube video</a> for more examples of playing
around on the playground.</p>

<h1>Implementation notes and thanks</h1>

<p>I'd like to thank a handful of people for their help on this side project.
Firstly, to Raymond Toy for continuing to update the Web Audio API spec with
useful goodies. This side project wouldn't be possible without the recently
added <code>IIRFilterNode</code>.  Raymond has a few filter-related projects on the web,
including <a href="http://rtoy.github.io/webaudio-hacks/more/filter-design/filter-design.html">Digital Filter Design</a>, which lets you create more complex
digital filters using a cascade of second order filters.</p>

<p>I built the 3D complex function plots with <a href="https://github.com/unconed/mathbox">Mathbox</a>, a very powerful
WebGL based visualization toolbox. If you haven't seen it yet, check out <a href="http://acko.net/blog/how-to-fold-a-julia-fractal/">How to
Fold a Julia Fractal</a>, which is awesome in its own right, but also
illustrates the power of mathbox. It's also a great introduction to complex
numbers, which I glossed over in this here post. Huge credit to Steven Wittens
for both mathbox and the inspiring blog post.</p>

<p>Finally, my thanks to Dick Lyon for writing an <a href="https://www.amazon.com/Human-Machine-Hearing-Extracting-Meaning/dp/1107007534">interesting and challenging
book</a> and responding to my email (squee!), which ultimately inspired this
project.</p>

<h1>A mathematical appendage</h1>

<p>I tried to make the post understandable as possible by reducing analytical math
and leaning heavily on interactive illustrations. Inevitably I have waved my
hands and collected massive mathematical debt along the way, most notably
everything to do with deeply understanding the <a href="https://en.wikipedia.org/wiki/Z-transform">Z-transform</a>, but
also:</p>

<ul>
<li>The filter playground requires poles to be inside the unit circle. If a
pole is outside of the unit circle, the filter <a href="https://www.dsprelated.com/freebooks/filters/Stability_Revisited.html">will become
unstable</a>, meaning that it will tend to blow up a signal that is
fed into it.</li>
<li>High order IIR filters <a href="https://en.wikipedia.org/wiki/Wilkinson%27s_polynomial">tend to become numerically unstable</a>. The
solution is to break up high order rational functions into products of lower
order rational functions, but the filter playground doesn't currently do this.</li>
<li>In most real life applications, complex filters are implemented as a cascade
of first or second order filters chained together. This <a href="http://rtoy.github.io/webaudio-hacks/more/filter-design/filter-design.html">Digital Filter Design
example</a> illustrates it well.</li>
<li>You might have noticed that points on the pole zero plot of the filter
playground are never found floating alone on the complex plane. All of the
coefficients of the numerator and denominator polynomials are real since they
were taken from the original difference equation y[n], so by the <a href="https://en.wikipedia.org/wiki/Complex_conjugate_root_theorem">complex
conjugate root theorem</a>, their roots (poles and zeros) must be either
purely real, or appear in complex conjugate pairs.</li>
</ul>

<h1>Over and out</h1>

<p>Thank you for getting this far, especially because I suspect this post may fall
into a sort of uncanny valley: too technical for a casual reader, and too
trivial for a DSP expert. At the very least, building the filter playground
helped me wrap my head around digital filters. Ultimately I hope the filter
playground can serve as a useful teaching tool for DSP novices.</p>

<p>I'd love to hear whether reading the post, watching the video and playing with
the filter playground helped you better understand digital filters. Please let
me know <a href="https://twitter.com/borismus">via twitter</a> or <a href="http://smus.com/about/">by email</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>The end of endings</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/the-end-of-endings/"/>
    
    <updated>2017-07-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/the-end-of-endings/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Despite the obnoxious title, <a href="https://a16z.com/2017/07/15/popularity-addiction-thompson-alter/">this a16z podcast</a> was unusually insightful.</p>

<p>One big theme in the podcast can be summarized as an end of endings: a trend
away from one story with a coherent, finite arc, towards something neverending.
Amorphous TV shows like Lost, as well as Hollywood's discovery that a coherent
franchise like Star Wars can be milked for many more dollars are examples of
this. The stats backs it up: Hollywood moved from 10% of the hits of the 90s
being sequels to 50% today, as can be seen in the <a href="http://www.boxofficemojo.com/yearly/">box office data</a>.
Looking at TV shows through the same lens, each show is a one hour long movie,
followed by tens of sequels. And successful franchises aim to immerse their fans
(especially kids) into their universe with figurines, games, lunch boxes, bed
sheets.</p>

<p>The end of endings also shows up in user interfaces, and is epitomized by the
infinite scroller. Millions of souls fixate on the next pretty picture, the next
baby picture, the next outrage. Entertainment that never ends!</p>

<p>This is interesting in the context of attention. In the realm of meditation, the
challenge is to focus on something mundane, like your breath. In the realm of
entertainment, the challenge is to snap out of an immersive world carefully
constructed to consume your attention for as long as possible.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Intelligence, aliens, and self-improvement</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/intelligence-aliens-and-self-improvement/"/>
    
    <updated>2017-07-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/intelligence-aliens-and-self-improvement/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I recently enjoyed a podcast about intelligence, aliens and learning where Kevin
Kelly was interviewed by Sam Harris. Here are some things I liked about it:</p>

<ul>
<li><p>Kelly makes a great argument that with text we already have no way of knowing
what is true. We have to rely on the source. My fears of audio and video fake
news are reduced.</p></li>
<li><p>It's obvious that the world is changing quickly, which means lifelong learning
is necessary. One thing I haven't systematically understood is how I learn
best. How would you even figure that out?</p></li>
<li><p>Chimpanzees are amazing at <a href="https://www.youtube.com/watch?v=zsXP8qeFF6A">short term spatial
memory</a>, and do better than
humans in these cognitive tasks on average.</p></li>
<li><p>Currently, the best Chess players are human-computer hybrids. Harris claims
that computers alone will eventually be better than these hybrids, but 
his claim that "the ape will just be adding noise" is unsubstantiated.</p></li>
<li><p>About mid-way, the two discuss the analogy between AI and aliens. I haven't
thought of this either. But I wondered how salient that analogy was given that we
have a largely shared environment with whatever AIs we create. To the extent
that the AIs truly are alien, I wonder how much of a unifying force these
alien AIs could be for humanity. </p></li>
<li><p>I strongly agreed with Kelly when he says that building machine systems
enhances our self-understanding. Building VR rendering and head tracking
systems, you benefit from some human visual perception basics. Human
psychology is critical for building user interfaces. You need an understanding
of how human hearing works to build good lossy audio compression.</p></li>
<li><p>An interesting thought experiment that superscedes Asimov's laws of robotics:
what if we built AGIs that were terribly worried about what people think. Then
we reduce the machine menace problem to merely the evils of humanity. </p></li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Convincing the inconvincible</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/convincing-the-inconvincible/"/>
    
    <updated>2017-06-20T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/convincing-the-inconvincible/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I highly recommend two <a href="http://loveandradio.org/2014/02/the-silver-dollar/">podcast</a> <a href="http://loveandradio.org/2017/02/how-to-argue/">episodes</a> on the theme "how to argue",
featuring Daryl Davis, <em>the</em> black friend of white supremacists. Mr. Davis is
remarkably lucid in dealing with very difficult issues, and presents great tips
for convincing the inconvincible. </p>

<ol>
<li><p>Gather your information. Understand their position as well as you do your
own. Expect to hear things you will disagree with and keep your cool. </p></li>
<li><p>Invite them. Have a conversation not a debate. Debates get people's guards
up. Instead seek to understand them. People want to be heard and speak their
mind freely without retaliation. </p></li>
<li><p>Look for commonality. You can find something in 5 minutes and then build on
it. Look to other issues. Politics? As you find more commonality the
differences matter less. </p></li>
<li><p>Talking is not fighting. When the talking stops the ground becomes fertile
for violence (cf. Sam Harris). The longer you talk the more commonalities you
can find. </p></li>
<li><p>Patience.</p></li>
<li><p>Make a good first impression. Be respectful.</p></li>
<li><p>Don't be condescending or insulting even when you hear things you don't like.
Expect opinions that are ridiculous and facts that are wrong. Keep cool. </p></li>
<li><p>Don't explain somebody else's movement. Let them explain it and address the
points that they themselves define. Let them finish and don't cut them off
even though you probably know a lot about their ideas already. </p></li>
</ol>

<blockquote>
  <p>"While you are actively learning about someone else you are passively teaching
  them about yourself." – Daryl Davis</p>
</blockquote>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Climate metaquiz results</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/climate-metaquiz-results"/>
    
    <updated>2017-06-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/climate-metaquiz-results</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="/viewpoint-tolerance-through-curiosity/">Last week</a> I ran a <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">Climate metaquiz</a>, and 123 people responded.
The point of a metaquiz is to test how well political groups know the other
side, while questions on personal beliefs and knowledge about the climate are
secondary. Both the small sample size and potential sampling biases are
important caveats to keep in mind here. All that said, Republicans outperformed
Democrats on the factual part of the quiz, despite their low self-reported
self-confidence.  However, Democrats outperformed Republicans on the <em>metaquiz</em>
part, with Republicans tending to exaggerate levels of climate change-related
handwringing amongst Democrats, as well as their eagerness to exaggerate the
facts in the name of behavior change.</p>

<!--more-->

<p>Sanity checking, the self-reported data matches stereotypes. 60% of Republican
test takers believe climate change is less serious than the scientific
consensus, compared to 6% of Democrats. Democrat test takers were far more
concerned (2.8 on a 5-point likert scale) about climate change affecting them
personally and future generations than Republicans (1.8). Republicans were very
skeptical about water level increases by 2100, predicting an average of less
than 2 feet rise, while democrats were more concerned, predicting on average
more than 3 feet.</p>

<h1>Actual vs. predicted results</h1>

<p>Since this was a metaquiz, the focus was not on the participants' own beliefs
and knowledge of the climate, but their predictions about the other side. The
reason for the first sections was only to establish a baseline. Here are the
aggregated results. I calculate percentage error using this simple formula: <code>%
error = (actual - predicted) / actual</code>:</p>

<iframe src="https://docs.google.com/spreadsheets/d/1vvg3bjG841zwJJWIYhGLvBp40kBBAZUA_mL_tALDlj8/pubhtml
?gid=573016866&single=false&widget=true&headers=false" style="height: 400px"></iframe>

<h2>Interesting findings</h2>

<p>In aggregate, participants underestimated how well the other side did on the
quiz (total 10 points). Republicans predicted 4.84 (actual 5.9) for Democrats,
while Democrats predicted 4.95 (actual 6.39) for Republicans. About 20% of all
respondents thought they did better than the other side, as many thought
that they did worse, while the rest weren't sure. One exception here are
Republicans, who exhibited false modesty: 40% of them thought that the Democrats
would perform better.</p>

<p><strong>Exaggerating facts</strong>: Republicans thought Democrats would be much more
comfortable exaggerating scientific facts to convince others of environmentally
beneficial behavior change (predicted 2.9) more so than Democrats declared
(reported 1.6). On the flip side, democrats didn't think the Republicans would
be comfortable exaggerating scientific facts for the environment (predicted
1.4), and they were right (reported: 1.2). It was pointed out to me that the way
I phrased the question was quite leading: "It's okay to exaggerate scientific
facts in order to convince others to behave in a more environmentally friendly
way." but I'm not sure which way this would bias quiz and meta-quiz takers.</p>

<p><strong>Impact of climate change</strong>: Democrats accurately predicted Republican
lack of concern about climate change affecting them personally (reported: 1.8,
predicted: 1.7) and future generations (reported: 2.7, predicted: 2.3).
Republicans predicted Democrats would be slightly more concerned about the
climate than they were both personally (reported: 2.8, predicted: 3.5) and for
future generations (reported: 4.2, predicted: 4.5). The same trend was repeated
when asked about water level rise by 2100. Democrats correctly predicted that
Republicans would be conservative about future water level rises (reported: 1.8
ft, predicted: 1.8 ft), while Republicans were way off about what Democrats
would think (reported: 3.1 ft, predicted: 4.7 ft). This asymetry is intriguing.</p>

<p>I must again caveat all of this with the fact that 123 quiz takers does not
constitute a statistically significant sample, not to mention selection biases
that come from posting the quiz on twitter and some rationalist forums. Still,
it's interesting to see the consistency with which Republicans tended to
exaggerate Democratic positions more than vice-versa. One explanation is that
Paul Krugman was right <a href="http://cafehayek.com/2011/06/open-letter-to-paul-krugman-2.html">when he said</a> "A liberal can talk coherently
about what the conservative view is because people like me actually do listen."
Another explanation is that this disparity is due to the unique dynamics of
climate change, which is a much more important issue for Democrats than for
Republicans. It would be interesting to do more metaquizzes on other topics that
are more balanced in perceived importantness. Perhaps the metaquiz could be
framed as a "competition in understanding" between the two sides.</p>

<p>I find the metaquiz format to be an interesting one, serving a purpose similar
to the <a href="http://econlog.econlib.org/archives/2011/06/the_ideological.html">Ideological Turing Test</a>, but requiring less effort of everyone
involved (ie. essay reading or writing). Thanks again to the survey takers.  If
you left your email and are curious about how well you did on the metaquiz part,
let me know and I'll send you your personal results.</p>

<p>As always, please send me feedback on the metaquiz concept via twitter or email.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Viewpoint tolerance through curiosity</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/viewpoint-tolerance-through-curiosity"/>
    
    <updated>2017-06-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/viewpoint-tolerance-through-curiosity</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Polarization isn't necessarily problematic. Strictly defined, it refers to the
divergence of political extremes. In fact, a wider variety of opinions may
actually be a <a href="https://ratedzed.wordpress.com/2017/05/20/the-biased-media/">good situation</a>. Things start to go south when a tribal
us-verus-them mentality takes over, giving rise to an uncharitable view of the
other side. This thinking is especially common among the <a href="http://noahpinionblog.blogspot.com/2017/06/the-shouting-class.html">shouting
classes</a>:</p>

<blockquote>
  <p>Those that disagree with me must be stupid, evil, or both.</p>
</blockquote>

<p>Not only is this incorrect, but adhering to this position is actively bad for
society. It prevents finding common ground and encourages wild policy swings as
power is transfered from one uncompromising faction to the next. The same facts
can generate different viewpoints, each deserving of a spot in the marketplace
of ideas, even if we personally disagree with them.</p>

<p>With <a href="/debaters-friendly-disagreement/">Debaters</a>, Antonio and I tried to bring people that disagree
together. Sadly most people don't want to converse with the other side whom
they perceive to be their mortal enemies. The problem must be approached more
obliquely, taking into account human nature. This post is about using <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">quizzes
like this one</a> to lure people into learning more about the other side
by appealing to a powerful emotion: curiosity.</p>

<!--more-->

<h1>Metaquiz format</h1>

<p>I'm calling it a metaquiz, since it asks both about the test taker's own
knowledge and derived viewpoints, as well as their guesses about how well the
other side did on the same quiz. Interestingly, the only fuzzy thing here are
your own beliefs. Knowledge about other people's beliefs is factually
verifiable.</p>

<pre>
1. What do you believe about the issue?
2. What do you know about the issue?
3. What do you know about the other side?
     Evil: what does the other side believe about the issue?
     Stupid: what does the other side know about the issue?
</pre>

<h1>Why would anyone do this?</h1>

<p>In one word: curiosity.</p>

<p>According to the research in <a href="/books/psychology-of-curiosity/">this survey paper</a>, being a curious
person ("trait curiosity") doesn't correlate strongly to other traits like IQ,
age, and sex. Instead, it is situational context that can pique curiousity
("state curiosity"). This is good news for me, since anyone can become curious
about anything.</p>

<p>On the flip side, curiosity about something requires that you be interested in
the pertinent topic, and usually increases with knowledge.  Loewenstein explains
curiosity in terms of an information gap. The more you know about a subject, the
more you know what you don't know. The novice is proud of what he knows even if
it is small in the absolute sense, relating closely to the <a href="https://en.wikipedia.org/wiki/Dunning%E2%80%93Kruger_effect">Dunning–Kruger
effect</a>. Gaining expertise, the novice learns more about the scope of
the domain, and shifts to thinking in terms of what they don't know yet. This
reveals a gap that they strive to fill. This news is not so good, but
Loewenstein suggests curiosity inducing stimuli, which I hope to take into
account in designing the quiz:</p>

<ul>
<li>Directly questioning is itself curiosity inducing, especially with a guess and
feedback cycle found in quizzes. A study showed that the more immediate the
feedback, the higher the curiosity.</li>
<li>The nature of a good quiz is that you will sometimes be wrong. If you are
wrong about a fact related to the topic, you may learn about an information
gap which will increase curiosity about the topic. If your expectation about
the other group is violated (ie. violation of expectations), curiosity about
that group increases.</li>
<li>If you see yourself as knowledgeable about a topic, you may be curious about
how much you really know about it. This can be viewed in terms of
what Loewenstein describes as the "competence motive", the desire to master
your own environment. </li>
<li>Loewenstein brings up the example of someone laughing aloud as they read a
newspaper article. This posession of information by someone else is also
curiosity inducing. What does the other side know?</li>
<li>The <a href="https://en.wikipedia.org/wiki/Zeigarnik_effect">Zeigarnik effect</a> states that people remember uncompleted or
interrupted tasks better than completed tasks. The last part of the quiz (what
does the other side think) requires a critical mass of responders, which means
that results will take some time to produce. This may serve to increase
curiosity.</li>
</ul>

<h1>A metaquiz about climate change</h1>

<p>To test this format, I decided to pick a specific topic to reduce the scope of
questions. Climate change is a good topic for several reasons. </p>

<ul>
<li>Pro: the topic is increasingly polarizing along party lines, most recently
after the US withdrawal from the Paris Agreement.</li>
<li>Pro: it is well grounded in science, which means there are plenty of hard
facts that can be verified and serve as a baseline of truth.</li>
<li>Con: climate change consistently shows up last in terms of topics that the US
public cares about.</li>
</ul>

<p>I put together a quiz following the template above, which first surveys your
climate change-related beliefs, then quizzes you on climate knowledge, and
finally asks you to guess what the other side believes and how they did on the
quiz. </p>

<p>If participants do poorly on the "other side" section, they may begin to wonder:
"maybe they're not all stupid?". Then, if it turns out that they have
incorrectly stereotyped beliefs of the other side, they might wonder "maybe
they're not all evil?". If participants do poorly on the quiz itself, they may
learn something about climate change, which isn't such a bad thing either.</p>

<p>So if you have a few minutes, <a href="https://docs.google.com/a/google.com/forms/d/e/1FAIpQLSf98rBi2CgU4b62kBFmAeX989ZGZrLW74cQitDdPT7wG906Xw/viewform">please try the climate metaquiz</a> and send it to
everyone you know, especially friends (or enemies!) on the opposite side.
Dziękuję!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Remembering my personal wiki</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/remembering-my-personal-wiki/"/>
    
    <updated>2017-06-01T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/remembering-my-personal-wiki/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Before I started writing this blog, I used <a href="https://moinmo.in">MoinMoin</a> as a personal wiki.
It was self-hosted on a Linux box in my parents' basement. I used it for
everything: jotting down ideas, as a calendar, storing guitar tabs, tracking
workout routines, course planning, and personal finances. I wrote on the wiki
actively from about 2002 to about 2008. Here are some blasts from the past:</p>

<ul>
<li>My Lego fandom started to shift from into robotics. To be clear,
this is just a fancy way of saying that I was still <a href="http://wiki.z3.ca/Bartender/">playing</a> <a href="http://wiki.z3.ca/CardDealer/">with</a>
<a href="http://wiki.z3.ca/VerticalCrossBeams/">legos</a> into my early twenties.</li>
<li>Origami interests also shifted into the <a href="http://wiki.z3.ca/OrigamiSoftware/">digital realm</a>.</li>
<li>I <a href="http://wiki.z3.ca/Network/">maintained a network</a> of about ten Linux boxes named after
<a href="http://wiki.z3.ca/Aleph/">Hebrew</a> <a href="http://wiki.z3.ca/Zayin/">letters</a>. What a pain in the ass!</li>
<li>Wrote a few computer games, like this uninspired <a href="http://wiki.z3.ca/ArcadeTank/">tank game</a>, and this
totally awesome <a href="http://wiki.z3.ca/Sponge/">pong variant</a> that was played inside a sphere.</li>
<li>Discovered <a href="https://en.wikipedia.org/wiki/Second-system_effect">second system syndrome</a> when trying to rewrite [a web based
RPG][alienation](http://wiki.z3.ca/Alienation/) I originally wrote in 2000.</li>
<li>Had some interesting, still relevant <a href="http://wiki.z3.ca/GpsRpg/">ideas foreshadowing</a> Augmented
Reality gaming.</li>
<li>Catalogued delicious Japanese food in <a href="http://wiki.z3.ca/JapaneseRestaurants/">Vancouver</a>.</li>
<li>Smashed a Palm Tungsten <a href="http://wiki.z3.ca/TungstenDestruction/">with a hammer</a> out of frustration,
attempted to use a <a href="http://wiki.z3.ca/TreoBluetoothHeadsets/">Bluetooth headset with my Treo</a> and dreamed of a
<a href="http://wiki.z3.ca/DreamGadget/">better gadget</a>.</li>
<li>Hoping to <a href="http://wiki.z3.ca/MatrixWorkouts/">avoid the gym</a>, I got into <a href="http://wiki.z3.ca/Kettlebells/">kettlebells for fitness</a>,
and even thought about starting a <a href="http://wiki.z3.ca/Gireviki/">kettlebell community</a>.</li>
<li>Saved <a href="http://wiki.z3.ca/Guitar(2f)Gravedigger/">guitar</a> <a href="http://wiki.z3.ca/Guitar(2f)WildMountainThyme/">tabs</a> I played with the intention of
<a href="http://wiki.z3.ca/LibTab/">doing something techy</a> with them later.</li>
</ul>

<p>Embarassing and endearing, <code>wiki.z3.ca</code> serves as a time capsule of my late
teens and early twenties. Je me souviens.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Short posts using split Markdown</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/short-posts-using-split-markdown/"/>
    
    <updated>2017-05-28T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/short-posts-using-split-markdown/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="https://github.com/borismus/lightning">Lightning</a>, the static blog generator that powers this site, is
awesome for long form content. But it does not work well for short form content,
since creating each post requires a bunch of up-front effort:</p>

<ol>
<li><p>I navigate to the right place, recalling the current year (!) and deciding on
the slug for the post (eg. <code>content/posts/2017/how-to-fix-the-world</code>).</p></li>
<li><p>I manually create the <code>index.md</code> file and then specify the redundant
human-readable title and the posted date within it:</p>

<pre><code>How to Fix the World
===
posted: May 28, 2017

Listen to me, I have the answers.
</code></pre></li>
</ol>

<p>After writing the post, the title will inevitably have changed, so I rename the
slug to match: <code>mv how-to-fix-the-world how-to-slightly-improve-the-world</code>.
Finally I publish and tweet, and hopefully the world becomes infinitesimally
better.</p>

<p>I once tried to <a href="/easier-link-blogging/">make it easier to keep a lightning-style link blog</a>, but
ended up scrapping the solution because it was buggy and required a Mac.</p>

<h2>Split markdown pages</h2>

<p>My aim was to remove the up-front costs I described above. Appending to the top
of an existing file and having the engine automatically generate a slug based on
the title seemed like the right approach. This means that one markdown file
results in multiple HTML pages. The latest <a href="https://github.com/borismus/lightning">Lightning</a> has been
updated with split markdown files, and refactored into classes, making it easier
to add features in the future. To give a sense of what these split markdown
files look like, take a look at <a href="https://github.com/borismus/smus.com/blob/master/notes_all.md">this file</a>.</p>

<p>Split markdown files power two new sections on this site: a [book
reviews section][books](/books), and a <a href="/notes">notes section</a>. I'm hoping that this
added convenience will encourage me to write shorter, more focused posts with
increasing frequency.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Analogies from the industrial revolution</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/notes/2017/analogies-from-the-industrial-revolution/"/>
    
    <updated>2017-05-24T09:00:00-00:00</updated>
    
    <id>https://smus.com/notes/2017/analogies-from-the-industrial-revolution/</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Several centuries after the industrial revolution, machines still can't do all
physical things humans can, not even by a long shot. Drawing a masterwork,
singing an aria, playing a musical instrument, manipulating a soft object,
cutting hair, taking care of the sick are all examples of human-only tasks. The
industrial revolution gave machines the ability to do crude, physically
demanding work at scale, but it did not address all of physicality.</p>

<p>Just like the industrial revolution did not automate all aspects of physical
work, there is good reason to believe that the AI revolution will not solve
all aspects of intellectual work. Where the line will be drawn remains to be
seen, but I would bet that one aspect of human cognition that isn't really at
risk from any immediate AI-based disruption are creative pursuits. Many well
respected professions like doctor, lawyer, and engineer have both a rote
component that lends itself easily to automation, and a creative one. Some
examples:</p>

<ul>
<li><p>Curing patients boils down to the application of known heuristics to
triage specific cases, but if you are a MD-PhD studying a new area of medicine
or tackling a new disease, it's a much more creative and speculative process. </p></li>
<li><p>The legal profession involves finding the right precedents relevant to the case
at hand, but articulating a convincing argument or deciding how to work out a
new situation where there are now laws yet? Creativity.</p></li>
<li><p>Designing a marketing webpage for a clothing brand is pretty formulaic. But
inventing way of solving a problem using new technology? Totally creative. </p></li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Debaters: friendly disagreement</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/debaters-friendly-disagreement"/>
    
    <updated>2017-03-05T09:00:00-00:00</updated>
    
    <id>https://smus.com/debaters-friendly-disagreement</id>
    <content type="html">
      <![CDATA[
      <div>
        <blockquote>
  <p>We have a choice. We have two options as human beings. We have a choice
  between conversation and violence. That's it. </p>
  
  <p>– Sam Harris</p>
</blockquote>

<p>As technological progress plows forward, human nature is unchanged. We each look
at the world through our own lens. In a previous post, I found that <a href="/hot-bread-delicious-deadly/">translating
a query between English and Russian greatly determines search results</a>.
In the same way that language matters, so do religious views, culture, political
leanings, and much more. Here's a recent example highlighting a news
source-based lens on the same topic (Nancy Pelosi and Russia):</p>

<p><img src="/debaters-friendly-disagreement/pelosi.png" alt="Nancy pelosi russia on nytimes vs. breitbart" /></p>

<p>Humanity has always been divided, and in hindsight, the unifying promise of the
internet was a techno-utopian dream. By shrinking the world into a "global
village" (famously coined by communication theorist <a href="http://www.marshallmcluhan.com/biography/">Marshall McLuhan</a>)
we have balkanized into increasingly specialized sub-cultures and increased
cross-cultural conflicts. More recently, personalized search results, curated
social network feeds only serve to deepen the divide.</p>

<p><a href="https://catma-847d6.firebaseapp.com/topics">Debaters</a> is a new side project which aims to bring you and someone
with an opposing view into a private, friendly, anonymous conversation. It's
still in development, but I want to share it with you both as a milestone and
to get early feedback.</p>

<!--more-->

<h2>The problem</h2>

<p>We are social animals. Rather than starting with a blank slate and using our
brilliant brains to arrive at independent conclusions, we prefer to jump to
our conclusions first through social means, and then rationalize why we are
right. Once we <em>know</em> the answer, it's unlikely that we will change our minds.
Because of my-side bias (aka confirmation bias), arguments in favor will stick,
while arguments against will be easily swatted. Entrenched in our socially
defined beliefs, our social circles and personalized information sources quell
potential for dissent, while strengthening our worldview. On a macro scale, this
leads to a polarized society. We can tolerate anything <a href="https://slatestarcodex.com/2014/09/30/i-can-tolerate-anything-except-the-outgroup/">but the outgroup</a>.</p>

<p>In light of the above, we are unlikely change our minds. But if you are one of
those rare people that are open to changing their mind, you may have read
articles like <a href="https://wiki.lesswrong.com/wiki/How_To_Actually_Change_Your_Mind">this one</a>. However, it's far easier to be understand
the theory of mind changing than it is to actually change your mind on a
specific issue. Many public intellectuals are actively involved in conversations
that test their limits, but normal folks like you and me don't often get the
chance.</p>

<p>I've only attended one <a href="https://www.meetup.com/Bay-Area-Conservatives/">conservative meetup</a>. I chose not to reveal my
identity as a crooked centrist, feeling that this would impede further
conversation. I think a similar thing happens in many cities: there must be
Trump supporters (dozens of them?) among us, but they seem to keep a low
profile. Sad!</p>

<p>The way we get better at anything is through practice, which in this case means
to actively test ourselves on new ideas and with new people. Projects like
<a href="http://www.livingroomconversations.org/">Living Room Conversations (LRC)</a> in the real world, or <a href="https://www.reddit.com/r/changemyview/">Change My View
(CMV)</a> online try to create an environment that enables conversations where
we can practice actual open mindedness.</p>

<h2>Some problems with existing mind changing tools</h2>

<p><a href="http://www.livingroomconversations.org/">LRC</a> requires getting a group of people together physically, and have a
structured conversation about a controversial topic. This is difficult to do
since you must find a group of friendly but disagreeing people in-person. I'd
love to try it, but haven't been able to find a more right-leaning
co-facilitator yet. It is also a social risk, since you are likely pulling in
people from your social circle. Presumably you have briefed them on the plan and
they have consented, but conversations may still escalate and feelings can
easily be hurt. In addition, the prospect of a serious, structured conversation
with close friends sounds quite awkward to me.</p>

<p>Online, <a href="https://www.reddit.com/r/changemyview/">CMV</a> is great but has its own problems, despite the efforts of
well meaning and intelligent moderators. Some users that start threads seem to
use CMV as a way of pressure testing their own view. They get all of the counter
arguments, learn how to counter them, and get even better at rationalizing away
any future doubts. Some respondents may make arguments whether or not they
actually think that way just for the sake of deltas. As a subreddit, CMV
users tend to fall into Reddit's skewed demographics. This means less potential
for viewpoint diversity. Lastly, CMV is public and not truly anonymous. This
encourages people to be clever rather than honest, although there is no shortage
of <a href="https://www.reddit.com/r/The_Donald/">subreddits</a> whose members prefer honesty over cleverness.</p>

<h2>What is Debaters?</h2>

<p>So, to address some of the shortcomings of existing approaches like CMV and LRC,
I've been working on a side project provisionally called <a href="https://catma-847d6.firebaseapp.com/topics">Debaters</a>.</p>

<blockquote>
  <p>Debaters enables one-on-one conversations on a controversial topic, with
  someone of the opposing view. You may not be convinced by their arguments,
  but your conversation may lead to a better understanding on both sides.
  Interaction with someone with a different viewpoint will lead to reduced
  animosity toward their whole group.</p>
</blockquote>

<p><img src="/debaters-friendly-disagreement/debaters.png" alt="Screenshots of Debaters" /></p>

<p>I took a bit of time off recently and got carried away on the implementation,
with <a href="https://twitter.com/abmcosta">Antonio's help</a> on the UX front, and now it's live on
<a href="https://catma-847d6.firebaseapp.com/topics">https://catma-847d6.firebaseapp.com/topics</a>. If you visit, you will be
presented with a list of issues and asked to opine on each. After you provide
your opinion you are matched to someone with the opposite view, then you engage
in a conversation about the issue.</p>

<p>Let me address one common question up-front: trolling. I do not think trolling
is much of an issue for Debaters. Trolls want to make an impact. In other words,
they want to either reach a lot of people, or affect some people in a
significant way. In a 1:1 conversation, their reach is limited. In an anonymous
context, the amount of personal harm a troll can inflict is limited too.</p>

<h3>Crowdsourced beta testing</h3>

<p>To work out bugs and test out the platform, I took to Mechanical Turk. That's
right, I paid people to have an argument in the spirit of Monty Python's
Argument Clinic:</p>

<iframe width="600" height="337" src="//www.youtube.com/embed/kQFKtI6gn9Y" frameborder="0" allowfullscreen></iframe>

<p>Kidding aside, Turkers effectively became poorly paid ($0.25 per session) QA
testers. I asked them to try out Debaters and answer a question or two.
Meanwhile, I would assume the position of devil's advocate (as needed) and we
would have 5-10 minute long conversations. This helped iron out the bugs and
prioritize features.</p>

<p>Getting people to use the service without bribes was hard, mainly because
Debaters is a marketplace. Two people are required to answer the same question
differently to get matched. So inevitably, the first respondent needs to wait
while a match is found. Attention spans are short, and Debaters users are few.
Debaters attempts to address this by taking advantage of <a href="https://developers.google.com/web/fundamentals/engage-and-retain/push-notifications/">web
notifications</a>. Once a match is made, you are notified through a
browser notification. By this point though, you may be less likely to be up for
a conversation.</p>

<p>One of my milestones for the first version of Debaters was to facilitate a
conversation between two people I didn't know. I managed to do this by actively
promoting it on Twitter while also paying users on Mechanical Turk, creating a
critical mass so that people would get matched without too much waiting. This
worked out, and finally I had a half organic conversation. This one was about
a federal minimum wage. Dustin answered "Not sure", Lawrence answered "No". In
case you are wondering, Debaters assigns names and avatars randomly.</p>

<pre><code>Dustin Collier:    hi Lawrence
Lawrence Castillo: hi dustin
                   guessing these names are not real
Dustin Collier:    hehe. mine isn't, dunno about yours :)
Lawrence Castillo: i was scared for a second and thought they were real but
                   thats good
Dustin Collier:    is yours really lawrence!!!!!
                   or did you forget your name for a sec
Lawrence Castillo: no no
                   i saw your name and was like "oh shit people can
                   see names"
                   glad they're fake
Dustin Collier:    ah yea
                   anonymous.
                   u dont like minimum wage?
Lawrence Castillo: i think federal minimum wage, at least how we've been
                   talking about it is pretty flawed
Dustin Collier:    how so?
Lawrence Castillo: like, the minimum wage in nebraska should be very
                   different from the minimum wage in nyc
                   if we want a minimum wage it needs to be a percent of
                   cost of living
Dustin Collier:    ah yeah, cost of living adjusted
Lawrence Castillo: the idea of 15 dollars is kind of crazy
                   people in ny are still poor, and business can't pay it
                   in rural areas
                   i feel that way about most federal laws though
Dustin Collier:    yea i agree, but that's not even on the table
                   bernie was all like "$15"
Lawrence Castillo: yeah i loved the energy but...
</code></pre>

<p>The next milestone is to have a fully organic conversation, where both sides
arrive at Debaters without monetary incentives, but out of legimiate interest.</p>

<h2>Problems with Debaters</h2>

<p>Now that the first version of Debaters is released, the technical problems have
been addressed, and the UX is in an early but usable state. The fundamental
problem is <strong>how to attract users</strong>. </p>

<p>I think that the name "Debaters" connotes exactly the wrong thing. Debates are
something you win, and invoke a high school debate club. The name is also
suggestive of conflict, which people generally tend to avoid. Unfortunately I
was unable to come up with a catchy alternative.</p>

<p>That said, the name is not the limiting factor on user acquisition; there are
more fundamental forces at play. In today's political climate, people want to be
upset and angry. We are constantly outraged, and <a href="https://www.nytimes.com/2017/02/27/opinion/the-uses-of-outrage.html">some view it as a good
thing</a> that builds social cohesion. We don't want to change our minds,
that would be like colluding with the enemy. After all, <a href="https://wiki.lesswrong.com/wiki/Arguments_as_soldiers">arguments are
soldiers</a>. I disagree.</p>

<p>Conversations with people that hold different views is like getting kids to eat
their vegetables. It's good for them, but they aren't necessarily going to like
it.</p>

<h2>Tricking people into friendly debate</h2>

<p>A common tactic for getting kids to each their vegetables is to disguise them as
something else. Could a similar approach be taken with Debaters?</p>

<p>One avenue might be to target people that want to proselytize their ideas.
They might come to Debaters to sway others about one issue they are passionate
about, and then become engaged in another conversation on another issue, where
they are more likely to listen. This is pure theory. Maybe proselytizers are
certain about everything.</p>

<p>Another avenue might be to target neurotic people. This has sort of been tried
in the form of <a href="http://asteroidsclub.org/">The Asteroids Club</a>. This project is framed as a
"non-debate on America's biggest problems, which are hurtling toward us through
space and time at an alarming rate of speed". Unfortunately it hasn't taken off
yet.</p>

<p>People are inherently curious. Projects like <a href="http://wolfmanproductions.com/haider-hamza/">Talk to an Iraqi</a> and <a href="https://www.theswedishnumber.com/">The
Swedish Number</a> have been effective at attracting an audience. Haidar
Hamza's public booth seems to have also been effective at bringing up difficult
political issues. Could we take advantage of this curiosity by surfacing
something unusual about your future interlocutor?</p>

<p>And yet resorting to trickery may not work. Even a more oblique form of it,
<a href="https://en.wikipedia.org/wiki/Nudge_theory">nudging</a>, has <a href="https://www.theguardian.com/commentisfree/2014/apr/24/nudge-backlash-free-society-dignity-coercion">had significant opposition</a>. But, as Sam
Harris starkly puts it, the only tools we have for changing minds are
conversation and violence. My opinion? I'd like to avoid the latter, so intend
to continue thinking about and building in this difficult but incredibly
important problem space.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Headlines, meet sparklines: news in context</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/headlines-meet-sparklines-news-in-context"/>
    
    <updated>2017-02-17T09:00:00-00:00</updated>
    
    <id>https://smus.com/headlines-meet-sparklines-news-in-context</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>News reporting suffers from two major issues I'd like to tackle. The first is a
bias towards negative, emotionally laden events. The second is the difficulty of
capturing information about gradual changes.</p>

<p>These two deficiencies distort our perception. They make it easy for demagogues
to claim that the world has gone to shit. The data tells a different story, as
the late <a href="https://www.ted.com/talks/hans_rosling_shows_the_best_stats_you_ve_ever_seen">Hans Rosling</a> was fond of reminding us. My hypothesis is that
if base rates were provided in a compelling way alongside news stories (or even
headlines), the public would be better informed. The challenges are many: first,
getting and analyzing the data, but even more important, presenting it in a
reasonable way.</p>

<p>In this post, let's explore what that would entail, from data collection, to
analysis, to visualization. We'll go through a couple of examples.</p>

<!--more-->

<h2>The problems with news</h2>

<p>I've already complained about the news in a <a href="/front-page-blues">previous blog post</a>, but
this time around, I'd like to hone in on two specific issues: negativity and
gradual changes:</p>

<ul>
<li><p>News is generally biased toward negative, emotionally laden events. A
terrorist rampage that claims five victims is practically guaranteed to make the
front page, while a cure that saves five hundred certainly wouldn't.</p></li>
<li><p>News does not inform about gradual changes. Many important
processes, such as climate change, are gradual. Like boiling a frog, there are
no specific events to report on, so they get no coverage in the news (until the
frog dies).</p></li>
</ul>

<p>The goal here is for perception to approach reality. I will assume that you
agree with me that this is a worthy goal to pursue. Otherwise, we now return you
to your regularly <a href="https://www.socialistalternative.org/">scheduled</a>
<a href="http://www.breitbart.com/">program</a>.</p>

<h2>Headlines invite questions</h2>

<p>I went through some recent news stories (on <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia</a>), asking some simple
questions. For example:</p>

<p><style>
table#headline-question {
  font-size: 70%;
}
table#headline-question td {
  padding: 1em;
  text-align: left;
}
</style></p>

<table id="headline-question">
<tr>
<th>Headline</th><th>Questions</th>
</tr>
<tr>
<td>The death toll from the Rigopiano avalanche rises to 29.</td><td>How frequent are avalanche deaths? What about just in
Italy? What are some big recent avalanches?</td>
</tr>
<tr>
<td>Ken Wyatt is sworn in as the first Indigenous Australian to serve in
Australia's cabinet.</td><td>What is the population of Indigenous Australians?
What is the racial breakdown in Australia's cabinet? What about other countries?
What about historically?</td>
</tr>
<tr>
<td>The Kremlin arrests four people, one from Kaspersky Lab and three from the
Federal Security Service, reportedly on treason charges for passing information
to America's CIA.</td><td>How many arrests does the Kremlin typically make? How
many for treason? How about the US government?</td>
</tr>
</table>

<p>Firstly, to even ask the question requires a skeptical mindset. Secondly,
finding the data requires time and research. Lastly, presenting the data in a
compelling way takes some thought and creativity. Keeping in mind that I make no
claims to any of the above, let's give it a shot.</p>

<h2>Why are base rates important?</h2>

<p>The questions above attempt to get at the <a href="https://en.wikipedia.org/wiki/Base_rate">base rates</a> relevant to the
news stories, which is important context to get a better understanding:</p>

<blockquote>
  <p>It may at first seem impressive that 1000 people beat their winter cold while
  using 'Treatment X', until we look at the entire 'Treatment X' population and
  find that the base rate of success is actually only 1/100.</p>
</blockquote>

<p>It is also well known from a large number of psych studies that people are
<a href="https://en.wikipedia.org/wiki/Base_rate_fallacy">really bad at integrating base rates</a> into their thinking. Maybe
this is why they are so rarely featured in the news? My hope is that by
pairing each headline with a bit of base rate information, we can become better
informed and address both negativity and get a better sense for trends over time.</p>

<h2>Exhibit A: avalanche deaths (time series data)</h2>

<p>Let's start with a simple quantitative (if morbid) example: Avalanche deaths.
We can better understand just how extreme the Rigopiano avalanche was if we put
it into context. But what sort of context makes sense? If we consider geography,
we can imagine concentric circles around Rigopiano.</p>

<p><img src="/headlines-meet-sparklines-news-in-context/rigopiano.png" alt="Possible geographic context for the Rigopiano avalanche" /></p>

<p>On one extreme, we could consider other avalanches at Rigopiano specifically.
But for most people, especially outside of Italy, this is too specific.
Expanding our search, we could consider all of the Apennines (the mountain range
containing Rigopiano), but I found that getting data for avalanche fatalities in
this region was challenging. The outermost circle of the map above represents
the European Alps, which does not include the Apennines. But it is the
geographically closest region with readily available data.</p>

<p><a href="https://docs.google.com/spreadsheets/d/1PyX0vav_NPziiaL9LWKhPOhTQLY-mcMYqYBl_VjUSmg/edit#gid=1197783313">This spreadsheet</a> contains data that I extracted from <a href="http://www.geogr-helv.net/71/147/2016/gh-71-147-2016.pdf">Avalanche
fatalities in the European Alps: long-term trends and statistics</a>,
which includes contiguous coverage from 1970 to 2015. Naturally, the paper
didn't link to a data set, so I had to create the spreadsheet by visually
inspecting the graph (ouch).</p>

<p>The paper contains some interesting findings. For example, the number of
avalanche deaths in controlled terrain (eg. ski resorts, where ski patrol
engages in <a href="https://en.wikipedia.org/wiki/Avalanche_control">avalanche control tactics</a>) has decreased
significantly, but that the number of avalanche deaths in uncontrolled terrain
remains significant (in the Alps, 100 yearly) and stable. Note that the numbers are
not adjusted for the increasing global population, or for the increasing numbers
of back country tourists.</p>

<p><img src="/headlines-meet-sparklines-news-in-context/alps-graph.png" alt="Avalanche deaths in the European Alps between 1970 and 2015" /></p>

<p>One of the things that becomes clear is the important distinction between
controlled and uncontrolled accidents. We now have context for better
understanding the tragedy at Rigopiano: it was a controlled accident that will
send the statistics for 2017 through the roof. Let's see it in the context of
other significant avalanches (controlled and not) over the years. The following
claimed more than 20 people since 1970, according to Wikipedia:</p>

<p><img src="/headlines-meet-sparklines-news-in-context/avalanches-since-1970.png" alt="Significant global avalanches since 1970" /></p>

<p>Now we are armed to the teeth with data, but how do we present inline in the
news? There are <a href="http://flowingdata.com/2017/01/24/one-dataset-visualized-25-ways/">tons of ways</a> of visualizing data in a compelling
way, but in this case we want it to appear in-situ in a digital newspaper. Why
not start with <a href="https://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR">Tufte</a>-inspired sparklines, since they are compact and
can be placed adjacent to a headline.</p>

<p><style>
iframe#avalanche {
  border: 0;
  height: 200px;
}
</style></p>

<iframe id="avalanche" src="avalanche-example.html"></iframe>

<p>From the first graph, we can immediately see that significant avalanches are
rare, so this event is definitely newsworthy, but didn't claim as many lives as
some of the most fatal ones, even recently. The second sparkline shows that
avalanches on controlled terrain (in the Alps) claim fewer lives, which makes
Rigopiano even more significant. Then, to satisfy our curiosity, the third
sparkline shows annual avalanche fatalities on uncontrolled terrain is
persistently high (c. 100 yearly). We now have some context to better understand
this story.</p>

<p>A quick note on technology. The above is a slightly modified version of <a href="https://github.com/phuu/sparksvg.git">Spark
SVG</a>. I added a few things to the basic <code>bar.svg</code>:</p>

<ul>
<li>Set y-axis scale for fair comparisons across different graphs.</li>
<li>Labels (x, y) values on hover.</li>
<li>Ability to transpose the graph.</li>
</ul>

<p>As an aside, I was amused to discover the <a href="http://caaml.org/">Canadian Avalanche Association Markup
Language (CAAML)</a>, which is a "standard for the electronic representation
of information pertinent to avalanche safety operations". I had naively hoped to
one day escape XML by becoming a ski bum. Not so fast!</p>

<h2>Exhibit B: cabinet composition</h2>

<p>Let us now turn our attention to Ken Wyatt, the newly appointed member of the
Australian cabinet. How ethnically diverse is the Australian cabinet? At
minimum, we can look at base rates for ethnicity in the Australian cabinet. In
1997, the cabinet was 100% white, but now with Wyatt's joining, the cabinet is
96% white (he is the only non-white member). Not much to visualize yet, so let's
expand our scope.</p>

<p>Consider three metrics: % female, % non-white and % non-christian for each
cabinet, and compare them across three cabinets: US, Australian and Canadian,
between two years: 1997 and 2017. I've collected this data <a href="https://docs.google.com/spreadsheets/d/1r6e92Xf4h8e7T83lrex-BghQilswGh_Hj-xmOaZCasc/edit?usp=sharing">in a
spreadsheet</a>. It was a fair amount of work to skim Wikipedia
pages for six sets of cabinet members to try to gleam gender (easy), ethnicity
(tricky) and religion (hard). While there are surely mistakes in the
spreadsheet (please <a href="/about">email me</a> if you find one), it should be good
enough for broad strokes. My own position would favor a qualified cabinet over a
diverse one, but all things being equal, a cabinet that is representative of the
general population is a good thing. Here's the headline with data alongside:</p>

<p><style>
iframe#cabinet {
  border: 0;
  height: 200px;
}
</style></p>

<iframe id="cabinet" src="cabinet-example.html"></iframe>

<p>To summarize, <a href="/headlines-meet-sparklines-news-in-context/cabinet-sign.jpg">I've seen stronger cabinets at IKEA</a>!</p>

<p>One thing that is clear from the above is that indeed, Australia has a very
white cabinet. Of course, in the spirit of representation, we should be
comparing ethnicity numbers to the general population, but I'll leave that out
for now (for reference, 3% is indigenous, and ~10% is non-white).</p>

<p>Another thing the chart above shows are trends over the 20 year period.
Australia's cabinet is becoming more female, while staying roughly as white and
as Christian. Canada's cabinet has become vastly more representative in gender,
ethnicity and religion. In stark contrast to Canada, the US has actually
regressed in diversity on all fronts. Over the last 20 years, its has become
more male dominated, more Christian, and more white. The last is especially
disappointing since the US is far more ethnically diverse than Canada and
Australia put together (at "just" 72% white).</p>

<p>I should mention a couple caveats. First, I really fudged the % Christian
calculations, since it was so difficult to accurately determine religion for
many cabinet members. Also, this analysis would greatly benefit from more data
points. For example, Clinton's cabinet in 1997 was quite diverse but probably
became even more diverse under Obama, but that data point is missing. Getting
additional data points for Canadian and Australian cabinets is more challenging,
since there are no term limits, and cabinet members flow more freely in and out,
change roles inside them, and sometimes even hold multiple offices. Lastly
thanks to the <a href="https://twitter.com/borismus/status/831641415604064256">good people on Twitter</a>, who sent me many constructive
suggestions for improvement. I still think it's a bit too information dense,
but it has come a long way.</p>

<h2>Summing up</h2>

<p>We looked at two headlines: one clearly well suited for contextualizing through
data visualization (longitudinal time series), and another somewhat less so,
regarding the composition of the Australian cabinet. In both cases, my
understanding of the world has been enriched by the context that data
surrounding it provided.</p>

<p>Of course, there are many ways to <a href="https://en.wikipedia.org/wiki/Misleading_graph">mislead with graphs</a>, and sparklines
can succumb to some of them. The axes are unlabeled, so the time scale is
unknown unless specified. Nor is it clear whether or not the y-axis has been
deliberately truncated. As a result, it can also be unclear whether or not
graphs can be cross-compared. In the cabinet example, I had to explicitly
specify that all of the cabinet sparklines have a maximum value of 50% to
facilitate this visual comparison, and exposed raw data on mouse hover.</p>

<p>Imagine headlines from <a href="https://en.wikipedia.org/wiki/Portal:Current_events">your favorite news source</a> enhanced with a bit of
longitudinal base rate for context. This would bring more clarity to the news,
giving readers a better sense for general trends, as well as putting the event
in a broader context. In many cases, the broader context is actually pretty
positive: avalanche deaths in controlled areas have gone down drastically,
cabinets in many developed nations are becoming more representative.</p>

<p>Some headlines may not fit the mold I'm proposing. Many of them are
anecdotal in nature, like gossip stories, where You Won't Believe What Happened,
because it's such a unique situation. A certain president doing certain crazy
shit comes to mind.  For other stories it can be very challenging to acquire the
data required, like the Kremlin FSB arrest story. (I may or may not be privy to
that sort of information. If I told you, I'd have to kill you.)</p>

<p>One downside to this whole thing is that it requires a journalist to do more
work: data sleuthing, careful thought about presentation, possibly even
implementing a new visualization. This work has intrinsic value, since it forces
the author to broaden their understanding of the subject, and then whittle it
down to the substantive kernel for public consumption. But ultimately, just like
I really enjoy <a href="http://www.pewsocialtrends.org/interactives/what-do-police-think/">Pew Research's</a> approach to visualizing polls, headlines
with visualizations of relevant base rates would make for a much more
informative and interesting read, and ultimately make us better informed
citizens. What do you think?</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Tools for making better decisions</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/making-better-decisions"/>
    
    <updated>2017-02-01T09:00:00-00:00</updated>
    
    <id>https://smus.com/making-better-decisions</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>In a famous letter dating back to 1772, Benjamin Franklin described how he made
decisions to a friend who was facing a dilemma. Franklin's method involved
enumerating pros and cons of an argument, and then attempting to weigh one
against the other to ultimately decide which of the two possibilities to pursue.
Franklin wrote:</p>

<blockquote>
  <p>My way is to divide half a sheet of paper by a line into two columns; writing
  over the one Pro, and over the other Con. Then, during three or four days
  consideration, I put down under the different heads short hints of the different
  motives, ... I endeavor to estimate their respective weights.</p>
</blockquote>

<p>This post attempts to modernize Franklin's method to attempt to overcome some of
its shortcomings. Once we have gathered our thoughts in one place using this
spreadsheet format, we can, with the help of others or using (aspirational) AI,
assist the decision maker to help them combat common mistakes.</p>

<!--more-->

<h2>Modern tools for decision making</h2>

<p>Franklin's method is explicitly qualitative: "...the weight of the reasons
cannot be taken with the precision of algebraic quantities". Of course, this has
not stopped many scientists and engineers from attempting to create quantitative
tools that assist in decision making, called <a href="https://en.wikipedia.org/wiki/Decision_support_system">decision support systems</a>.
However these are mostly targeted at companies and not individuals. I
<a href="https://1000minds.com">tried</a> <a href="https://meenymo.com/">a couple</a> and failed to find one that was simple enough
for my purposes.</p>

<p>As a result, it seems that the state of the art for individuals hasn't advanced
much beyond Franklin's method. Product comparisons are one notable exception:</p>

<p><img src="/making-better-decisions/product-comparison.png" alt="Product comparison chart example" /></p>

<p>What if we could take product comparison charts, but make them a bit more
quantitative, and then apply the technique to decision making?</p>

<h2>Decision support spreadsheets</h2>

<p>Simply stated, a topic is controversial (or a decision is difficult) if:</p>

<blockquote>
  <p>...there are good arguments on all sides. Good thinking involves balancing these
  arguments in a quantitative way, taking into account their relative strengths
  and weaknesses.</p>
</blockquote>

<p>Inspired by this and other parts of Jon Baron's <a href="https://www.amazon.com/Thinking-Deciding-4th-Jonathan-Baron/dp/0521680433">Thinking and
Deciding</a>, I made <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=0">a spreadsheet</a> attempting to codify what he
describes as the "search-inference" process. Here's an example of an imaginary
rocket scientist deciding between two job offers based on two goals, resulting
in a 2x2 sheet:</p>

<p><img src="/making-better-decisions/decision-spreadsheet.png" alt="Screenshot of Google Sheets decision spreadsheet" /></p>

<p>Structurally, it works like this:</p>

<ul>
<li>Columns are possible courses of action (eg. NASA vs SpaceX).</li>
<li>Rows are goals that you are trying to achieve (eg. improve the world, work
with great people).</li>
<li>Cells contain evidence pertaining to the associated possibility (row) and goal
(column). In this case, the NASA job would improve the world by enabling much
faster space travel.</li>
</ul>

<p>There are also numbers involved:</p>

<ul>
<li>Each goal (row) has a number between 1 and 5 under it, corresponding to how
important the goal is to you. The higher the number, the more important.</li>
<li>Each piece of evidence (sub-cell) has a weight to the right between -2 and 2.
Positive weights are pros, negative ones are cons.</li>
</ul>

<p>A <a href="https://docs.google.com/spreadsheets/d/1HBjUBa1NaD2jGr4QzN1prIbQgCBFqcjtHnRkXuenIHs/edit#gid=374695355">second sheet</a> does all of the calculations. Each cell reduces to a weight in a
decision matrix. Ultimately, each possibility (column) is given a score between
0 and 1. The recommended course of action is the possibility with the highest
score. So the above 2x2 spreadsheet is converted into this decision matrix.</p>

<p><img src="/making-better-decisions/decision-spreadsheet-calculations.png" alt="Second sheet of Google Sheets decision
spreadsheet" /></p>

<p>Then, given these weights, we do a simple calculation for each possibility
(column): a normalized, weighted sum. So for the first possibility, we calculate:</p>

<pre><code>(0.6 * 0.9 + 0.8 * 0.63) / (0.6 + 0.8) = 0.75
</code></pre>

<p>We do the same for each possibility, and the one with the highest resulting
score is the "best" course of action.</p>

<h3>Advantages of this method</h3>

<p>Even geniuses like Franklin have a limited capacity for holding multiple
thoughts in their heads at once. After laying out all of the arguments, Franklin
wrote, "the whole lies before me, I think I can judge better". With all of the
possibilities, goals and evidence in one place, you too can be like Franklin.</p>

<p>As for the specifics of my spreadsheet above, I can't claim that this method is
optimal or even particularily good (though feedback on this would be
appreciated). I created it as a placeholder, loosely inspired by Baron,
Franklin, and other less rigorous approaches I've tried in the past. As it
turns out, this method is essentially an example of <a href="https://en.wikipedia.org/wiki/Analytic_hierarchy_process">Analytic Hierarchy Process
(AHP)</a>.</p>

<p>Rather than sticking to Franklin's two column split, this spreadsheet is
somewhat more complex, but there are some advantages:</p>

<ol>
<li>Most decisions <a href="http://lesswrong.com/lw/hu/the_third_alternative/">aren't actually binary</a>, and this is captured by
having multiple columns.</li>
<li>The method makes the notion of your goals and their relative importance
explicit.</li>
<li>Rather than pros and cons, we collect evidence that helps you decide about a
goal and a possibility, which can then be graded numerically.</li>
</ol>

<p>Despite the mechanistic appearance of this approach, Baron emphasizes the
nonlinearity of the thinking process. As you collect evidence, you may uncover
new possibilities and goals. With all of the evidence laid out, you can begin
asking better questions, attempting to fight known failure modes in human
thinking.</p>

<h3>Reducing and increasing complexity</h3>

<p>One significant challenge with the above approach is that of assigning weights.
At the moment, my method involves coming up with two kinds of weights: goal
weights (eg. how important is it for you improve the world, really?), and
evidence weights (eg. is space travel really such a world improving thing?).
This method is flexible enough to be easily simplified. For example:</p>

<ol>
<li>Evidence weights can be simplified by scoring pros as +1, and cons as -1.</li>
<li>Goal weights can be simplified by binary ranking (eg. 1 is critical, 0 is
nice-to-have).</li>
</ol>

<p>A potentially better approach is known as <a href="https://en.wikipedia.org/wiki/Potentially_all_pairwise_rankings_of_all_possible_alternatives">PAPRIKA</a>, which establishes
weights based on a bunch of pair-wise comparisons. This might work well, and
could actually be useful for capturing additional points of evidence. To get a
feeling for it, there's a consumer-oriented decision support system <a href="https://meenymo.com/">called
MeenyMo</a> that does this. The process is quite tedious though, involving
tens of comparisons like this:</p>

<p><img src="/making-better-decisions/meenymo.png" alt="MeenyMo's PAPRIKA style comparisons" /></p>

<p>The other downside of PAPRIKA is that it requires discrete categories (eg. cost
of living: cheap, moderate, expensive).</p>

<h2>Thinkos: inevitable irrationality</h2>

<p>People aren't perfect, and neither is our thinking. Biases are sort of like
thinking bugs that make our thoughts less rational. Irrational thinking leads to
conclusions that are further from the actual objective truth. This is, as I
hope you'll agree, undesirable.</p>

<p>Now that we're on the same page, Baron suggests that certain tactics that can
help us make better decisions by improve thinking and reducing bias. These he
broadly describes as "active open mindedness":</p>

<ul>
<li>Seek alternative possibilities. Anchoring bias tends to favor the first
possibilities you generate, but it is entirely possible that you haven't
searched enough.</li>
<li>Formulate goals better. What are you actually trying to achieve? (eg. "protect
walls from child's scribbling" vs. "prevent child from scribbling on walls").</li>
<li>Look for counterevidence (eg. if there are strong pros, see if there are some
cons too).</li>
<li>Avoid belief overkill, which happens when there is a strong correlation
between different goals (eg. most people are against capital punishment because
it is both ineffective and immoral, whereas those <em>for</em> capital punishment are in
favor because it's effective and moral. But why do both go together? They
should be unrelated.)</li>
<li>Allocate time that is proportionate to the importance of the decision.
Franklin's method suggests to take "three or four days consideration" to
capture evidence, and then "a day or two of further consideration" to let it
all settle.</li>
</ul>

<p>There are <a href="https://en.wikipedia.org/wiki/List_of_cognitive_biases">many other biases</a> that can lead to bad decisions. The above
serves as an example of some thinkos that can be reduced with external help:
other people or software.</p>

<h2>Summing up...</h2>

<p>In some facets of life, it is impossible to apply this level of rigor.
Quantifying your love for a person, for example, feels cold hearted and
calculating, and I try to avoid it. Ironically, one of the most famous uses of
Franklin's method was used by Charles Darwin in deciding whether or not to marry
Emma Wedgwood. For what it's worth, the method appears to have worked, with
Darwin emphatically scribbling "Marry, Marry, Marry, QED" after his
calculations.</p>

<p>It is hard to fully discount the role of feeling. The quality of the
rational decision making process depends heavily on your ability to formulate
your true goals and possibilities, and collect all of the evidence and score it
correctly. Gut feeling, (or as Kahneman says, System 1 thinking), can actually
incorporate many arguments that one might not even be able to formulate, and yet
those intangibles may end up being incredibly important.</p>

<p>And lastly, there is the question of practicality. Life is dynamic and
circumstances can change quickly. For the spreadsheet-powered decision maker,
this means constant revision, which can be complicated and time consuming. I
experienced this first hand, attempting to use this method to help make a career
move. Just when I thought I had established the teams that would have me,
another one emerged, and I had to re-enter additional evidence, remove options
that seemed appealing, but in retrospect were duds, and re-calibrate weights.</p>

<p>Thinking cannot be reduced to a spreadsheet, but when used in moderation, I hope
that this method can be useful for some.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Front page blues</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/front-page-blues"/>
    
    <updated>2016-12-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/front-page-blues</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>According to the <a href="https://www.americanpressinstitute.org/journalism-essentials/what-is-journalism/purpose-journalism/">American Press Institute</a>,</p>

<blockquote>
  <p>News is that part of communication that keeps us informed of the changing
  events, issues, and characters in the world outside.</p>
</blockquote>

<p>There are many ways for news to be uninformative or even outright misleading.
Two trends in particular have received a lot of attention recently. The first is
social recommendation systems and selective unfollowing, which creates a
reality-distorting echo chamber. The second is fake news, which sure is in vogue
these days, and is obviously a problem that we should tackle.</p>

<p>This post is about a different trend: <a href="https://twitter.com/hamandcheese/status/801893793540796416">real news presented with misleading
frequency</a>. The issue at stake is the media's ability to inform its readers
and serve the public interest.</p>

<!--more-->

<h1>Real news, dubious frequency</h1>

<p>If you are a New York Times reader, you may have noticed a certain individual
prominently mentioned in the newspaper over the last several months. I wanted to
know just how much, and started daily screenshots on August 3rd, 2016. To my
great surprise, this was exactly one hundred days before the announcement of the
45th president-elect. My analysis was very simple.</p>

<ol>
<li>Does the lead story (that is, top-left corner article) include "Trump" in the
title? <strong>(50 / 100 days)</strong></li>
<li>Does the "Trump" appear in any headline above the fold? <strong>(86 / 100 days)</strong>.</li>
</ol>

<p>I should note some limitations of this approach. This process itself was more
manual than I would have liked. My script took screenshots only, so I did not
have access to the markup of the page. I tried auto-reconstructing the images using
<code>tesseract</code>, but the results were not perfect, <code>pdfgrep</code> had only limited use,
and I had to visually inspect the page to get my data analyzed. The data are <a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vT-7Cz1s9YpvlXDz9ejuJa_0JP9pV6coeAMA2j_R0KxEZZpnZ4daMsNOdI86qWgDIwUyZhy8rUAs-2Y/pubhtml">in
this spreadsheet</a>, and I've made the <a href="https://drive.google.com/open?id=0B4Nj-yDXjBs_S3kwZXVqanZNdEU">raw screenshots</a>
available too.  There are non-technical problems too. The web version of the New
York Times is updated on a more frequent than daily basis, so my screenshots
missed some versions. Also, notions of "lead story" and "above the fold" don't
really make sense online. I arbitrarily defined "the fold" to be 1280 px, as in
this example:</p>

<p><img src="/front-page-blues/nytimes.png" alt="Example NYT screenshot from September 16th" /></p>

<p>As for the results? Half of the NYT's headlines had "Trump" in the name, and the
frequency matched my intuition. It sure did feel like this dude was getting a
whole lot of coverage. In fact, he got far more coverage than my method reveals,
since many lead articles without explicit mention of his name in the title were
still mostly about him.</p>

<p>I was amused to see long runs of adjacent days of headlines about the
president-elect-to-be followed by many days of respite. For example, from August
9th to 17th, "Trump" was featured consecutively, only to be broken by the
news of Zika having spread to Florida.  The longest respite took place between
September 19th and 27th, when the nation's focus switched to the Manhattan bomb
scare. Things heated back up on October 8th, and stayed hot until the 18th, when
his boasts about groping women were released to the public.</p>

<p>Just for fun, I looked through some <a href="https://en.wikipedia.org/wiki/Portal:Current_events">other news</a> that lost the
contest for most important story of the day. Here are three randomly chosen
examples:</p>

<ul>
<li><p>On August 14th, the NYT focused on G.O.P. politics rather than the Russian and
Syrian jets which conducted 26 airstrikes across the Idlib province, killing
122 civilians.</p></li>
<li><p>On September 8th, the NYT decided that the future potential president's vows
to bolster military capacity and raise spending were more important than Wells
Fargo's agreement to pay $190 million to settle a case involving deceptive
sales that pushed customers into fee-generating accounts they never requested.</p></li>
<li><p>On October 21st, the NYT featured a story about a presidential candidate
threatening to reject the election result rather than the Watts Bar Nuclear
Plant, which was the first U.S. nuclear reactor to enter commercial operation
in 20 years.</p></li>
</ul>

<p>I claim that the distribution of coverage in the days running up to the election
did a bad job of keeping us informed of significant events, and went against the
public interest.</p>

<h3>On not being well informed</h3>

<p>Only 15 reporters were present when Harry Truman announced the use of nuclear
weapons against Japan. <a href="https://www.amazon.com/Six-OClock-Presidency-Presidential-Television/dp/0275935981">By 1990</a>, nearly 2000 reporters held
passes to the White House pressroom. The presidency has become an increasingly
important focal point for the media. But as I write here in 2016, the coverage
of a mere presidential candidate has come to eclipse absolutely everything else.
Reasons why this is bad:</p>

<ul>
<li><p><strong>Opportunity cost</strong>: People have limited attention. By devoting half of the
headlines to one issue, we are inevitably less well informed about other
things.</p></li>
<li><p><strong>Reporting on outrage</strong>: The majority of the headlines were about his
Rump's outrageous statements: "twitter barage taunts Ryan as weak and ineffective",
"clung to birther lie for years, and still isn't apologetic", "assails his
accusers as liars and unattractive", "failing efforts to tame his tongue". Is
that truly newsworthy?</p></li>
</ul>

<h3>On not serving the public interest</h3>

<p>In one interpretation, "public interest" taken literally means providing the
public with what they are interested in. And if the object of interest is a
particular individual, so be it! That's how YouTube works: you watch a lot of
cat videos, YouTube learns you are into cat videos and gives you more cat
videos. This is problematic <a href="http://www.timewellspent.io/">in and of itself</a>. But capital-J journalism
that the New York Times is associated with is held to a different standard.</p>

<p>The literal definition leaves a lot to be desired. My view of public interest is
more paternalistic and centers on that which is beneficial for a <a href="http://reutersinstitute.politics.ox.ac.uk/publication/journalism-democracy-and-public-interest">well
functioning democracy</a>. In that view, here are a few reasons why focusing
so heavily on Cheeto Jesus is counterproductive:</p>

<ul>
<li><p><strong>General fairness</strong>: No individual is important enough to have half of
all stories in a newspaper be about them.</p></li>
<li><p><strong>Playing into his little hands</strong>: The NYT gave a person widely regarded as
"remarkably narcissistic" fifty percent of all headlines. Thanks guys!</p></li>
<li><p><strong>Conditioning effects</strong>: By reporting so heavily on a single topic,
people become morbidly obsessed with it. People wonder, "What's going to
happen next?" instead of moving on to more important stories.</p></li>
</ul>

<p>Events have a timeline, but if the object of obsession is an individual, it's
the gift that keeps on giving.</p>

<h1>Engagement, clicks, and the bottom line</h1>

<p>Why did the New York Times cover that despicable man to the extent that they
did? I venture a couple of guesses:</p>

<ol>
<li>Genuine fear of him being elected, and thus a desire to warn the people of
his evil ways.</li>
<li>A desire to increase views, catering in part to the morbid fascination
of their readers.</li>
</ol>

<p>All things considered, I lean more towards the second possibility. While I can't
rule out the first guess completely, it seems to be contradicted by the
<a href="http://www.nytimes.com/interactive/2016/upshot/presidential-polls-forecast.html">explicit confidence</a> the NYT had in a Democratic win. Back in March,
Vox <a href="http://www.vox.com/2016/3/3/11148296/donald-trump-media">asked the same question</a> and answered it simply:</p>

<blockquote>
  <p>The media covers him a lot because his campaign is fascinating and people are
  interested in it.</p>
</blockquote>

<p>Notably, at the time, 13 percent of Vox stories were about the short fingered
vulgarian, but generated 26 percent of their readership. With statistics like
that, guess what topic gets covered more? As for public interest, the author of
the piece doubted that his attention-getting tactics would continue to work in
the candidate's favor. Carte blanche justifying continued coverage? How
convenient!</p>

<p>The internet has changed the media in a fundamental way. Even great publications
like the New York Times haven't quite figured out how to balance their
(understandable) corporate need for profit with their journalistic
responsibility to the public's interests. Our culture of not paying for news
content leads to media companies to seek other sources of income, mostly in the
form of ads. But you get what you pay for! Rather than covering the world in a
balanced way, the world is covered in a way that more people will want to read.</p>

<p>The result is a killer combination of consumer driven demand (oh my god, what is
he going to say next?), and a desire for publications to maximize ad revenue.
The term is "engagement", and <a href="https://medium.com/@edelwax/is-anything-worth-maximizing-d11e648eb56f#.bt1ua0z6g">Joe Edelman</a> does a great job of explaining
why this is a dangerous thing to maximize.</p>

<p>If newspapers did not need to maximize engagement to be profitable, there would
be room to make decision that are aligned with actual public interest.</p>

<h1>Problems of novelty</h1>

<p>What makes a story newsworthy? Two ingredients at the very least,</p>

<ol>
<li>It must have happened recently, and</li>
<li>it must be of sufficient interest to the public.</li>
</ol>

<p>The thing is, it takes time and effort to decide whether something is of
sufficient interest to the public. So there is an inverse tradeoff between the
ability to deliver both. The faster you deliver news, the more shortcuts must be
taken to measure public interestingness.</p>

<p>The general trend in the media is one of spacetime compression. Before the
telegraph, it would take months for transatlantic news to travel. The fastest
way to get information across was a slow moving frigate. Before Gutenberg, it
would take weeks more for the newly arrived information to spread. Over the last
three centuries, the news cycle shrunk down to a daily basis. Today, since
everyone has a mini-printing press in their pocket, the news period is
arbitrarily small. According to <a href="http://www.tristanharris.com/2016/05/how-technology-hijacks-peoples-minds%E2%80%8A-%E2%80%8Afrom-a-magician-and-googles-design-ethicist/">Tristan Harris</a>, the average
person checks their phone 150 times a day.</p>

<p>With people checking the news every 10 minutes, there is a lot of pressure for
journalists to produce more content, more often. And indeed, there's a
lot of content online. But more does not mean better. Increasingly, news is
re-aggregated and re-published. Commentary is cheap and can be attached to a
brand or personality, which means more clicks. Investigative journalism takes
time. Unfortunately, by the time your investigation is finished... oh look a
squirrel!</p>

<p>It's tempting to blame the media for this, but the media is a reflection of our
collective psyche magnified by modern technology. We need to value novelty less
and learn how to delay gratification. Take a lesson from wine tasting and
meditation. Accept that news happens and let it breathe undisturbed for a period
of time. After all, hindsight is 20/20.</p>

<h1>Any publicity is good publicity</h1>

<p>Systematic flaws with today's media (see above) make it easy for cynical
operators to exploit. Fake news happens when the media-savvy operator runs a
news website. Stories are completely fictional, but negatively resonate with the
public, creating pure click bait. <em>Oh my god, <a href="https://www.buzzfeed.com/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook?utm_term=.rwlPqdb6Po#.ds5Yo0mwYE">Hillary sold weapons to
ISIS</a>!</em></p>

<p>In another, even more cynical and dangerous variant, the media-savvy operator
mostly says and does outrageous things that may or not be true, purely for
attention. <em>Oh my god, he said he would date his daughter</em>! Minor celebrities
like <a href="https://swagbymilo.com/">M</a> have perfected this technique, but the Orange One is a true
master. Sometime in July, his wife gave a speech which was <a href="http://www.politico.com/magazine/story/2016/07/donald-trump-2016-convention-melania-trump-speech-dark-art-of-pr-214083">blatantly
plagiarized</a>. His response to the controversy?</p>

<blockquote>
  <p>“Good news is Melania’s speech got more publicity than any in the history of
  politics,” he said, “especially if you believe that all press is good press!”</p>
</blockquote>

<p>Straight from the horse's mouth. On the internet, we have an expression: <a href="http://rationalwiki.org/wiki/Don't_feed_the_Troll">"don't
feed the troll"</a>. I guess the New York Times didn't get the memo?
Whatever the case may be, the news media just helped feed the United States of
America to the biggest troll ever.</p>

<h1>This is bad, what do we do?</h1>

<p>Here are some possible things to try:</p>

<ol>
<li><p>Switch to a weekly news digest. At least this way events that happened early
in the week will have had time to settle. The Guardian has a <a href="https://www.theguardian.com/weekly">weekly
version</a>, but I struggled to find a subscribable <a href="http://www.nytimes.com/newsletters">weekly news
digest for the New York Times</a>.</p></li>
<li><p>Read a more balanced news source that is not driven by engagement. My new
favorite source for daily news is <a href="https://en.wikipedia.org/wiki/Portal:Current_events">Wikipedia's Current Events</a>. News
via Wikipedia has a nice side benefit: context for the story is readily
available in the form of other Wikipedia articles!</p></li>
<li><p>Focus on more international content. The US is a special snowflake, but it's
not <em>that</em> special. The world is increasingly global, but papers like the NYT
tend to weigh US politics very heavily. <a href="http://www.bbc.co.uk/worldserviceradio">BBC World Service</a> may be a
remedy.</p></li>
<li><p>Support your favorite news source through a subscription. This will reduce
their dependency on ads, which hopefully means less click-bait.</p></li>
<li><p>Don't give Demagogues a platform. In this post, I have taken the Voldemort
tactic: avoid mentioning their name.</p></li>
</ol>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>VR View 2.0: JavaScript API</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-view-2.0"/>
    
    <updated>2016-11-14T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-view-2.0</id>
    <content type="html">
      <![CDATA[
      <div>
        <p><a href="https://github.com/googlevr/vrview">VR View</a> was just updated to version 2! This release includes some nice
new features, the main one of which is a JavaScript API. This allows VR Views to
be much more interactive. You can now load new content dynamically, play and
pause videos, and add hotspots that link from one piece of 360 imagery to
another. Here's a simple auto-advancing 360 slideshow showing some of my recent
escapes around Seattle...</p>

<!--more-->

<div id="vrview"></div>

<script src="//storage.googleapis.com/vrview/2.0/build/vrview.min.js"></script>

<script src="index.js"></script>

<p>The <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a> show more complex and interesting examples.
The <a href="https://developers.google.com/vr/concepts/vrview-web">docs</a> are also updated to reflect VR View's new capabilities.</p>

<h1>Other new things</h1>

<p>Also added some other features:</p>

<ul>
<li>WebVR 1.1 support for compatibility with <a href="https://webvr.info/">Chrome WebVR</a> builds.</li>
<li>Programmatic playback controls and volume setting.</li>
<li>Support for handling clicks, taps, VR button presses.</li>
<li>Automatic panning mode for desktop.</li>
</ul>

<h1>Cardboard camera compatibility</h1>

<p>I captured the photos with the very handy <a href="https://itunes.apple.com/us/app/cardboard-camera/id1095487294?mt=8">Cardboard Camera</a>. But before
I could embed them into the VR View above, I had to do a conversion step.</p>

<p>VR View expects stereo images to be in ODS format, which is a square JPEG with
the left eye sphere stacked on top of the right eye sphere. Both spheres are
projected onto 2:1 rectangles using equirectangular projection.</p>

<p>The native Cardboard Camera format is different. Cardboard Camera produces an
image of the left eye only. The right JPEG is base64 encoded and embedded in an
XMP header, alongside other <a href="https://developers.google.com/streetview/spherical-metadata">Photo Sphere XMP metadata</a>. The images don't
need to be full photospheres, and may be cropped. Stopping a pano capture
mid-way, for example, will create a half-sphere. Also, the north and south poles
of the sphere are never captured, since the sweep is horizontal.</p>

<p>Now that you know more than you wanted about photosphere file formats, you can
forget it all. I've streamlined the conversion process through a web-based
<a href="https://storage.googleapis.com/cardboard-camera-converter/index.html">Cardboard Camera to ODS convertor</a>.</p>

<h1>Future work</h1>

<p>I was initially overjoyed by Safari 10's support for inline video texture
playback, which lets us finally play spherical video without <a href="http://stackoverflow.com/questions/29621199/three-js-video-textures-in-ios-play-back-in-a-separately-launched-player-ideas">gross
hacks</a>. Unfortunately, their current video texture rendering performance
is pretty abysmal. I'm getting about 10 FPS on a 2K (2048 x 2048) spherical
video in Cardboard mode, while Chrome, even on older Android hardware performs
substantially better.</p>

<p>Many thanks to <a href="https://twitter.com/lbayliss">Leon Bayliss</a> for writing the <a href="https://storage.googleapis.com/vrview/2.0/examples/index.html">official samples</a>
and test the API, and to <a href="https://twitter.com/aerotwist">Paul Lewis</a> for implementing tree shaking to
substantially reduce the size of the library.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Election 2016</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/election-2016"/>
    
    <updated>2016-11-13T09:00:00-00:00</updated>
    
    <id>https://smus.com/election-2016</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Back in June, I gave Trump a coldhearted 55% chance of winning the 2016 US
election. You don't have to believe me, since I recorded it in my
<code>Predictions.md</code> file, and never on <a href="http://gjopen.com">gjopen.com</a>, where it
belongs. My assessment was mostly based on anecdotal observations that recent,
related polls have been terribly wrong. Brexit and then Trump's surprise
Republican nomination both came as a complete surprise to experts from all
sides.</p>

<p>But, despite my dire predictions, it somehow didn't <em>feel</em> that I could be right
on the eve of the election. When the final result was revealed, I was just as
disturbed as everyone else. In retrospect, I attribute my "successful"
prediction mostly to luck combined with my apparently contrarian tendencies,
rather than to skill. Nearly a week after the announcement of President Trump,
I'm still processing the verdict. Two big questions loom: 1) Why did he win?,
and 2) Why didn't we see it coming?</p>

<!--more-->

<h1>Who voted for him?</h1>

<p>Trump's electoral base was quite diverse. Working class whites were only a small
part of his base. I was surprised to learn that 37% of 18-29 year olds, and 29%
of hispanics voted for him, and that the average Trump voter had an annual
income of $71K.</p>

<p><strong>Single issue voters</strong>. Many people value only one issue, or weigh a single issue
so heavily that others pale in comparison. Fundamentalist Christians may really
want a pro-life president. If they care enough about that issue, they would be
willing to deal with a Buffoon, and forgive all the hate, racism and bigotry in
the world.</p>

<p><strong>People with nothing to lose</strong>. I suppose that the economic situation many people
are facing are worse than priveledged people like me can fully appreciate. If
you are low on Maslow's heirarchy, liberal values tend to fall by the wayside.
When offered a chance to burn it down and start fresh, people with nothing are
willing to oblige.</p>

<p><strong>Tired of walking on eggshells</strong>. The left won the culture war. In many
environments: at work, in universities, etc, expressing a dissenting opinion
puts you in dire straits. Loud and self-righteous activists have a trained ear,
and will be incredibly quick to label you a racist or a bigot for merely
entertaining certain notions, or bringing up controversial questions despite no
intended harm.</p>

<p>This atmosphere leads to reduced viewport diversity. Without the ability to have
an honest, civil conversation about difficult topics, people have fewer
opportunities to change their minds, and become deeper and deeper entrenched in
their current beliefs.</p>

<p><strong>Genuine xenophobes</strong>. Certainly some of Trump's electoral base are actually
racist immigrant haters. It's very hard to imagine a racist voting for Hilary.
But I continue to believe that the visibility of this small group is magnified
by media bias. Turns out it's really interesting to read about crazed people on
the fringe.</p>

<p>But as a side note, a group's support of a candidate doesn't imply that all or
most of the candidate's supporters are members of the that group. And this
statement holds even for deplorables: when the candidate is Trump and the group
is the KKK.</p>

<h1>Why did he win?</h1>

<p><strong>Attention economy</strong>. Goebbels supposedly said "If you repeat a lie often enough,
people will believe it, and you will even come to believe it yourself." Despite
his purported hatred of the media, I think Trump was really helped by it. The
amount of free publicity Trump's shenanigans received even from the most liberal
publications like NYTimes is staggering (and any publicity is good publicity).</p>

<p><strong>Terrible alternatives</strong>. It's hard to get excited about a candidate because they
won't burn the country down. Many people simply could not in good conscience
support Clinton because of legimiate grievances.  Yet Trump is clearly a
complete Buffoon. And principled voters found themselves stuck between a rock
and a hard place, hence the low turnout.</p>

<p><strong>Complacency due to expected outcomes</strong>. In the weeks leading up to the election,
many papers prominently featured polls leaning heavily in favor of Clinton. The
NYT ran an election forecast on their front page which depicted the race being
closest in July, giving Clinton a 30% lead over Trump. The day before the
election, Clinton had a whopping 70% lead. Why would you go out and vote if you
know that it's going to be Clinton anyway?</p>

<h1>Why didn't we see it coming?</h1>

<p>Experts and laymen love to tell you what will happen in the the next five years.
Yet with Trump and Brexit, we have collectively been unable to predict what will
happen the next day. Given this sad observation, whatever model we are using is
clearly broken. We must look inward, critically questioning many deeply held
assumptions about the world, or be prepared for a lot more surprises.</p>

<p><strong>Polling is broken</strong>. A lot of predictions rely on polls. 538 was based mainly on
aggregating existing poll data. Nate Silver's theory was that by including
enough polling companies, inacuraccies in each poll would be ironed out.
Unfortunately, if all of the polls are systematically skewed, this approach is
screwed. And I think it is! Most polling is done by phone, which is quite
different from a secret ballot. Imagine you are a disenfranchised voter and a
pollster from Gallup calls you, you would naturally tell them to go fuck
themselves.</p>

<p><strong>Filter bubbles distort reality</strong>. On election day, my twitter feed proudly
announced that "I'm With Her". The day after, when Trump won, people mourned,
observed that it was the anniversary of Kristallnacht, ushered in the antichrist
and took to the streets yelling "Not My President!". Yet of half the voting
population (not my twitter feed though) celebrated a 'uuuuuge victory.</p>

<h1>Parting thoughts</h1>

<p>The world is shrinking, and this is not always a good thing. Long ago, you would
be born into a village, hang out with the butcher and the baker and be forced,
by virtue of your birth, to listen to the candlestick maker's racist ramblings.
Affordable air travel allowed us to self-organize according to professions,
beliefs, and lifestyles. Social networks are hypersonic airplanes for the mind.
Did someone tweet something you didn't quite like? Relief is just one unfollow
away. And so we end up in an increasingly polarized world, a bimodal
distribution with increasing peak separation.</p>

<p>I am deeply concerned about the political future of the US. Now is the time for
supposedly open minded liberals like me to prove it. This means not running away
to Canada, but accepting the democratically elected president. It means turning
inward and trying to understand why the world behaves so differently from our
internal model, and starting by fixing the model. I recommend <a href="https://www.amazon.com/Righteous-Mind-Divided-Politics-Religion/dp/0307455777">Righteous Mind by Jon
Haidt</a>
as a relevant starting point.</p>

<p>Without conversation, there is no hope.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Ray Input: WebVR interaction patterns</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ray-input-webvr-interaction-patterns"/>
    
    <updated>2016-10-11T09:00:00-00:00</updated>
    
    <id>https://smus.com/ray-input-webvr-interaction-patterns</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>What would the web look like if there were no scrollbars, no mouse cursors, and
no clickable links? That's what VR is like today. On one hand, this is great!
Developers are completely free to build however they want, leading to a lot of
interesting experiments. On the other hand, it takes a lot of engineering effort
to just get basic interactions up and running. Furthermore, it lacks
consistency. The alluring promise of being able to navigate from world to world
may be diluted by the frustration of having to rediscover new interaction
paradigms every time.</p>

<p>While sane interaction defaults are badly needed, baking them into the platform
violates principles of the <a href="https://github.com/extensibleweb/manifesto">Extensible Web</a>. With that in mind, I
implemented a basic Ray-based interaction library called <a href="https://github.com/borismus/ray-input">RayInput</a>, which
provides reasonable defaults for interacting with 3D objects in and outside of
VR. Here's what the interaction looks like on various platforms:</p>

<iframe width="600" height="340" src="//www.youtube.com/embed/gjj2XQYC998" frameborder="0" allowfullscreen></iframe>

<!--more-->

<h2>What does Ray Input actually do?</h2>

<p>Ray Input aims to provide reasonable interaction defaults, relying on the
hardware available for each platform:</p>

<ul>
<li>On desktop, look around by dragging, interact by clicking.</li>
<li>On mobile, look around via magic window or touch pan, interact by tapping.</li>
<li>In VR, interaction depends on a reticle or on a ray.
<ul>
<li>If there is no controller (eg. Cardboard), use a gaze based reticle to
interact with objects.</li>
<li>If there is a 3DOF controller (eg. Daydream), apply an arm model and
interact with objects using a ray emanating from the controller.</li>
<li>If there is a 6DOF controller, interact with objects using the ray.</li>
</ul></li>
</ul>

<p>Of course, you may want to customize your interactions on a per-platform basis.
For example, if you are developing an application primarily for the Vive, you
may want to take advantage of the specific richness that a Vive controller
provides. Ray Input is not meant to be prescriptive, merely to provide
reasonable defaults.</p>

<h2>API</h2>

<p>The library's API is documented on the <a href="https://github.com/borismus/ray-input">github page</a>, and I also provide a
<a href="https://borismus.github.io/ray-input">simple example that uses Ray Input</a> to pick items from a 2D menu.</p>

<h2>Arm models for orientation-only controllers</h2>

<p>If a VR controller is present, Ray Input defaults to using a ray-based input
method, which behaves much like a laser pointer.</p>

<p>The Daydream View controller is not position tracked. The only pose information
it provides is the orientation, which is in the same coordinate system as the
head. Where should we position such orientation-only (3DOF) controllers? In
particular, where should the ray come from? Having it emanate from the stomach
or head, like the arm of an exotic god, would be very unnatural. So we need to
be slightly more clever.</p>

<p>Enter the arm model, which, given a controller orientation, spits out a
plausible controller position. Obviously the position it provides is only a
reasonable guess, and may not correspond to the controller's actual position.
But it sure is a lot better than nothing. This sort of problem is common in
graphics and robotics, and can be solved with inverse kinematics.</p>

<p>In this case, we follow a simpler approach. Most of the <a href="https://github.com/borismus/ray-input/blob/master/src/orientation-arm-model.js">code to do
this</a> is lifted from a native implementation of Daydream arm model.
To debug it, I built a very rough simulator, which lets you specify the
orientation of a virtual head and hand, run it through the model, and visualize
the resulting pose of the controller:</p>

<p><a href="https://borismus.github.io/ray-input/daydream-simulator.html"><img src="/ray-input-webvr-interaction-patterns/arm-model.png" alt="Daydream arm model simulator" /></a></p>

<p>As always, very open to feedback, bug reports, and pull requests via
<a href="https://github.com/borismus/ray-input">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Copresence in WebVR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/copresence-webvr"/>
    
    <updated>2016-08-08T09:00:00-00:00</updated>
    
    <id>https://smus.com/copresence-webvr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>The web platform is uniquely great for networked copresence. To demonstrate, I
built a multi-user chat prototype that uses peer-to-peer audio and data
connections to establish a virtual audio experience.  Voices are spatialized
based on the position and orientation of each participant (using Web Audio).
Also, you can shrink and grow, which, in addition to changing your avatar's
size, pitch shifts your voice. Large avatars have deep, god-like voices, while
smaller ones start to sound very mousey!</p>

<iframe width="560" height="395" src="//www.youtube.com/embed/FPJDNQJt2DQ" frameborder="0" allowfullscreen></iframe>

<p>Check out the <a href="https://borismus.github.io/copresence-vr/">demo for yourself</a>. It works on desktop (mouse look and
spacebar triggers movement), on mobile (magic window) and in VR (through the
<a href="https://webvr.info/">WebVR API</a>, via <a href="https://github.com/borismus/webvr-polyfill/">the polyfill</a>).</p>

<!--more-->

<h1>Better together: copresence is compelling</h1>

<p>The best things in life are enjoyed in good company. Virtual experiences are no
exception. My fondest gaming memories were from two decades ago with close
friends huddled around a CRT, whether it was Morris the Moose and Blombo the
Elephant <a href="https://3drealms.com/catalog/wacky-wheels_16/">racing around</a> the track, or co-strategizing in <a href="https://www.youtube.com/watch?v=hBrYtNTOTyE">Civ</a>. It
wasn't so much about the games, more about the people, and the experience of
being there together.</p>

<p>Putting a computer on your face greatly increases your odds of having an
isolating experience.  One of the biggest downsides of VR is that social
experiences are much harder to produce. While physically copresent VR is
possible, it presents logistical challenges. And since you are fully immersed in
a virtual world, the physical presence of your friends is nearly irrelevant.
Given the constraints, perhaps the best remedy to loneliness is to provide
networked friends. This can be fun too! <a href="https://en.wikipedia.org/wiki/Warcraft:_Orcs_%26_Humans">Orcs and Humans</a> over PBX, anyone?</p>

<h1>WebAudio + WebRTC + WebVR = ❤</h1>

<p>The web is the ideal platform for building copresent VR experiences. VR
copresence requires low latency connections between peers. It also requires a
real time audio channel, with a much smaller emphasis on remote video, since the
user is wearing a headset and their face is obscured. The powerful Web Audio
API has long been available on all modern browsers, and is well equipped for
processing audio of all sorts: spatialization, effects. WebRTC is widely
available too, with <a href="http://www.apple.com/safari/">one unfortunate exception</a>. And with the exception
of Service Workers and company, if you're on the web, you have connectivity. </p>

<p>Thanks to some <a href="http://crbug.com/121673">excellent bug squashing</a>, it's now possible to pipe
remote WebRTC streams into a Web Audio context. This enables devs to spatialize
and otherwise manipulate the remote stream to their heart's content.
Specifically, the prototype I'm launching today has a few fun audio features:</p>

<ul>
<li><p>Each remote stream is spatialized based on the pose of the peer using the
<code>PannerNode</code> (see <a href="/spatial-audio-web-vr/">my previous post</a> about this for more details).</p></li>
<li><p>Remote streams are analyse for voice activity, using an <code>AnalyserNode</code> to
inspect the frequency content between 300 Hz and 3400 Hz (the typical human
vocal range), and doing a simple thresholding. This is then used to animate
the Southpark-style avatar's mouth.</p></li>
<li><p>Changing the size of your avatar also changes how you hear your peer's voice.
I'm using the <a href="https://github.com/mmckegg/soundbank-pitch-shift">soundbank-pitch-shift</a> library to achieve this, courtesy of
<a href="https://twitter.com/cwilso">Chris Wilson</a> and <a href="http://twitter.com/MattMcKegg">Matt McKegg</a>.</p></li>
</ul>

<h1>Technical details: in the weeds with WebRTC</h1>

<p>Hoping to avoid learning the intricacies of WebRTC, which is a fairly low level
and intimidating API, I started exploring higher level abstractions around it.
The most popular wrapper I found was <a href="http://peerjs.com/">peer.js</a>, but unfortunately the
project doesn't seem to be actively maintained, and relies on a special Node.js
WebSocket server which, in my experience, often drops clients.</p>

<p>So I moved to Firebase which, in my implementation, performs the duty of
signaling server, and also maintains a roster of all connected users and their
current state. For each connected user, we store their display name (which
clients can set), and the room ID (if the user is currently in a room).</p>

<pre><code>{
  username: 'Your Name',
  roomId: 'A Random Room Identifier'
}
</code></pre>

<h2>Bird's eye view of WebRTC</h2>

<p>Having moved away from peer.js, I could no longer afford to let the intricacies
of WebRTC be handled by some third party, and had to get into the weeds. It was
especially important to understand how to handle multiple <code>RTCPeerConnections</code>
necessary for the case with more than peer-to-peer. Although I found the docs to
be quite obtuse, the core of the WebRTC API is fairly straight forward:</p>

<ol>
<li><p>The caller (A) gets its local stream and uses the signal server to send an
"offer" message to the callee (B), which includes information about A's local
stream.</p></li>
<li><p>The callee (B) gets A's "offer" and registers A's local stream as its remote
stream. It then gets its own local stream, and responds A's offer via the
signal server, sending an "answer" message to the caller (A), which contains
its own local information.</p></li>
<li><p>The caller (A) gets B's "answer" and registers B's local stream as its remote
stream. At this point, both A and B have basic information about one
another's local and remote streams.</p></li>
<li><p>At this point, A and B exchange ICE (Interactive Connectivity Establishment)
Candidates to work out the details of how to establish a peer-to-peer stream.
Eventually, when both sides are satisfied, we have contact.</p></li>
</ol>

<p>Hopefully the above serves as a useful summary. It certainly will be for me, as
I found the existing WebRTC documentation confusing. Many of the samples connect
to themselves, which does not give a great sense of what the protocol between
clients should actually be.</p>

<p>At the ICE stage, invoke more acronyms! STUN and TURN come into play in trickier
network topologies (ie. those involving NAT servers).  Google already provides a
STUN server by default, and I ended up using a <a href="http://xirsys.com/">free service</a> for TURN
server support. Each <code>RTCPeerConnection</code> is initialized with the specific STUN
and TURN servers that we use.</p>

<h1>Copresence is essential for VR</h1>

<p>Given the inherent isolation of virtual reality, copresence becomes an even more
compelling ingredient than ever before. Copresence is essential for VR, and the
web is a great place to make it happen. <a href="https://borismus.github.io/copresence-vr/">Try it out</a> with a friend, or
using two of your own devices. Oh, and if you find bugs, please let me know via
<a href="https://github.com/borismus/copresence-vr">github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Inspirata: for what inspires you</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/inspirata"/>
    
    <updated>2016-05-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/inspirata</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>My site has a little section called <a href="http://smus.com/inspiration/">Clippings</a>. It's meant as a visual
record of some of the things I've found inspiring on the web. How do I add
new items to this visual record? Well, I'm glad you asked!</p>

<p>About a year ago, I cobbled together a Chrome extension for exactly this
purpose: screen grabs from any webpage. Releasing it on the webstore has been on
my backburner ever since. Over the last few weeks, I've spent a bit of time
improving it and today, I'm ready to release it for broader testing. I call it
<a href="https://inspirata.xyz">Inspirata</a>. Inspirata can be downloaded from the <a href="https://chrome.google.com/webstore/detail/oaddmiclfpjkcehhbhhojmphflhlompo">Web Store</a>, and it
works like this:</p>

<ol>
<li>Click the Inspirata icon button.</li>
<li>Select part of the page to save.</li>
<li>Enter an optional caption, et voila!</li>
</ol>

<p><img src="/inspirata/inspirata.gif" alt="How Inspirata works video" /></p>

<!--more-->

<h1>Bookmarks and breadcrumbs</h1>

<p>Bookmarks are like breadcrumbs. Hansel and Gretel left a trail of them to follow
home. But GPS made this application of breadcrumbs obsolete! In a similar
fashion, search engines killed bookmarking. Rather than browsing your curated
bookmarks to find your way to content, you can just search for it.</p>

<p>Bookmarks, like breadcrumbs, go stale quickly. When a website goes down, the
bookmark becomes useless, just like Hansel's breadcrumbs which got eaten by
birds. When a page does dark, your bookmark leaves no record of what used to be
there. And when your bookmarking service gets turned down, say bye bye to your
carefully curated archive!</p>

<p>Bookmarks, like breadcrumbs may be <a href="http://del.icio.us/">delicious</a> but aren't very appealing. A
URL has no appeal in itself, only the content at that URL does. And the bookmark
does not capture anything about that content: neither the content itself, nor
the presentation, nor a deep link into which part of that content spoke to you.
A pile of breadcrumbs, like a pile of bookmarks, is pretty nondescript.</p>

<h1>Why?</h1>

<p>I'm an avid user of little paper notebooks that I carry around in my pocket,
along with a trusty black pen. Sometimes while strolling down Valencia St on an
errand, I'll have an idea, stop and jot down it down using a wall or street
light for support. This way I don't get sucked into my phone, and capture
whatever's on my mind. Even if I don't revisit my note, the act of writing
itself has served a purpose. This notion is found explicitly in the <a href="https://fieldnotesbrand.com/">Field
Notes</a> tagline: "I'm not writing it down to remember it later, I'm
writing it down to remember it now". </p>

<p>Inspirata serves a similar purpose for content on the web. If something inspires
me, I want to capture it, not for the purpose of revisiting later or sharing
socially, but for the act itself. Perhaps a utilitarian argument can be made as
well: being on the lookout for inspiration helps to maintain a sharp and active
eye.</p>

<p>I am interested in being more creative in my consumption. Over the last year, I
have developed a habit of writing a summary after finishing each book, as if I
was going to share it with others. This forces me to gather my thoughts on the
subject, sometimes even taking notes while reading or listening to make my
summary more complete. I'm hoping this will increase retention and engagement.
Inspirata can perhaps serve a similar purpose in my web browsing endeavors.</p>

<h1>Ease of use is (always) key</h1>

<p>I continue using notebooks because of their amazing usability. The battery is
never dead, it's quick to turn on (open up, uncap pen, good to go), and writing
is a pleasant experience overall (perfect pen tip tracking, zero latency).
Technology at its finest!</p>

<p>I tried to make Inspirata as minimal and convenient as possible. One click on
the browser action button, select the area of interest, leave an optional
comment, and you're set.</p>

<p>There are a few entry points into capturing content:</p>

<ul>
<li>By clicking the Inspirata browser action (extension button), and clipping part
of the webpage you're currently on.</li>
<li>By right clicking an image and saving the inspiration.</li>
<li>By selecting text and right clicking it as above, or by clicking the extension
button.</li>
</ul>

<p>In all cases, Inspirata ultimately saves an image, even if the content in
question is text. An image of just the text can give interesting additional
context, such as layout and typography which is missing from the content
itself.</p>

<h1>Public by default and presentation agnostic</h1>

<p>Images captured with Inspirata are added to a public Firebase and hosted in the
Firebase bucket storage, which is part of the new <a href="https://firebase.google.com/docs/">Firebase 3.0
platform</a>. As an aside, the new Firebase platform is pretty amazing
once you work out some migration kinks. In terms of security, only you can
publish new Inspirata for yourself, but your Inspirata are publically available. </p>

<p>The Inspirata website <a href="https://inspirata.xyz">https://inspirata.xyz</a> provides a default public gallery
view. To give you a sense of what this looks like, here is <a href="https://inspirata.xyz/?uid=US3UvWWOBhhi21AZgKkyUK0QTHL2">my public
gallery</a>. Each users' feed is <a href="https://project-4121485576010625868.firebaseio.com/users/US3UvWWOBhhi21AZgKkyUK0QTHL2.json">served as JSON</a>, which is how its
stored in Firebase. This means your list of inspirata can be presented in any
way you like on any embedding website (as I have done <a href="http://smus.com/inspiration/">on mine</a>). This, as
far as I know, is not possible in other visual bookmarking services.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Browsing Wikipedia in VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/wikipedia-vr"/>
    
    <updated>2016-05-26T09:00:00-00:00</updated>
    
    <id>https://smus.com/wikipedia-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>WebVR provides a solid technical foundation on which to build compelling VR
experiences. But it does not answer a critical question, which is the topic of
this post:</p>

<blockquote>
  <blockquote>
    <p>What could the web become in a Virtual Reality environment?</p>
  </blockquote>
</blockquote>

<p>Gear VR provides a simple and straightforward answer: same same. The fundamental
unit is still a page, but you use the immersion of VR to increase your effective
screen size. The input constraints result in a worse experience for the user.
Scrolling with your finger on your temple is tiring and head-based typing is a
massive pain. Given the input constraints, we need to beef up the output and
make it better matched to what VR excels at. A responsive design inspired
solution would involve deconstructing the page to better suit the nature of the
immersive environment.</p>

<p>Another approach is to make a clean break from legacy web content. What if
certain web pages had parallel content tailored for virtual reality? In this
post, I'll explore this idea with an example focused on Wikipedia.</p>

<p><a href="https://youtu.be/HcSvBAEXcWA"><img src="/wikipedia-vr/vr-forest.jpg" alt="Video of VR Wikipedia" /></a></p>

<!--more-->

<h1>Navigating the VR Forest</h1>

<p>The web in general and Wikipedia specifically covers a vast amount of
information &mdash; nearly everything. Everything is a daunting place to start,
so we will begin with something specific: a forest.</p>

<p>To begin, navigate to a (fake) <a href="http://borismus.github.io/wikipedia-vr/pages/moose">wikipedia article about a moose</a>, and hit
the VR button. This takes you to a forest meadow, with a life sized moose in front of
you. You are free to move inside the forest (focus on the grass and click) and
interact with other animals in it. Looking at an animal gives you some basic
Wikipedia-inspired information about it.  Clicking it focuses you in on
it and presents options. If you leave VR when focused on an animal, you end up
on the associated Wikipedia article. This closes the navigation loop: you can
start from one webpage, enter VR mode, navigate to another entity inside VR,
leave VR and end up on another webpage.</p>

<p>Try out this <a href="http://borismus.github.io/wikipedia-vr/pages/moose">Wikipedia VR sample</a> on your mobile phone in Cardboard. It
also works on desktop using the spacebar to simulate the Cardboard click.</p>

<p><img src="/wikipedia-vr/navigation.mp4.gif" alt="Video of navigating between pages" /></p>

<p>VR and education are naturally matched. As <a href="https://youtu.be/UuceLtGjDWY?t=1m40s">Ben explains eloquently in an I/O
talk</a>, "VR is a chance to scale experiential learning". Remembering what
you learned in class is much harder than remembering your favorite vacation. In
this Wikipedia example, you immediately get a sense of the animal's grandeur,
which is hard to convey in words and images. You can get a feeling for quickly
it runs, and what it sounds like. </p>

<h1>Closing thoughts</h1>

<p><strong>Tip of the iceberg</strong>. The entities in this Wikipedia demo (in green) represent
a tiny subgraph of Wikipedia:</p>

<p><img src="/wikipedia-vr/knowledge-graph.png" alt="Picture of the subgraph we implement" /></p>

<p>All of the above are positioned in a much bigger subgraph of Wikipedia which
might can be represented in VR. Of course, many Wikipedia pages are really
difficult to imagine in VR. Could <a href="https://en.wikipedia.org/wiki/Philosophy">Wikipedia's Philosophy</a> article have a
compelling VR version?</p>

<p><strong>Changing scale</strong>. The ability to change scale would make it possible to place
every entity from the above graph into VR Wikipedia. Imagine diving into the
hide of the moose, learning about symbiotic insects and hair folicles, then
going deeper to learn about the structure of hair on a molecular level. Or vice
versa, zooming out to look at planet Earth to see where moose live, or going
into an abstract view to explore Family Cervidae. It's easy to lose an hour or
two in Wikipedia's hyperlink maze. One day, it may be even easier to do this in
VR.</p>

<p><img src="/wikipedia-vr/alice-shrinking.jpg" alt="Alice in wonderland shrinking" /></p>

<p><strong>Content is king</strong>. The big open question is how to generate this content. Even a
scoped down project to VR-ify categories of Wikipedia pages (say, only forest
animals), is incredibly ambitious. Where do you get all of the models? How do
you animiate them to run, jump, stand around, sleep, play, eat and be eaten? How
do you place them in a the forest in a meaningful way? Doing this automatically
seems, at a glance, AI-hard.</p>

<p><strong>Limited knowledge graphs</strong>. Even if you imagine that we have a series of animated
models, how do we compose them together? Do moose and canaries live in the same
environment? Can you find fire ants in the bark of a Sequoia? How many? How big
are hyenas, and how quickly do they run? This information is missing from even
the best known knowledge graph.</p>

<p><strong>3D modeling is difficult</strong>. Wikipedia's giant corpus of quality content exists
because it's easy for many people to collaborate. Wikipedians need to be good
writers, well versed in their topic, and motivated to contribute. There is a
technical barrier - learning Wikipedia markup - but it is not incredibly
difficult. For a Wikipedia in VR, the technical barriers are much higher. Even
with a good collaborative editor, it seems inevitable that contributors would
need to have some sense of 3D modelling, and a far more specialized skillset.</p>

<p><strong>Artistic considerations</strong>. One of the challenges for a large community project
like Wikipedia is establishing a consistent style. Imagine if every Wikipedia
image was hand drawn. Artistic abilities vary wildly, and you can imagine a
funny and chaotic result. Aaron Koblin's now classic <a href="http://www.thesheepmarket.com/">Sheep Market</a>
experiment comes to mind:</p>

<p><img src="/wikipedia-vr/sheep-market.png" alt="Sheep market screenshot" /></p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Three approaches to VR lens distortion</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-lens-distortion"/>
    
    <updated>2016-04-20T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-lens-distortion</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Immersion requires a large field of view. This could be achieved by putting a
large curved spherical display on your face, but alas such technology is
prohibitively expensive. A more affordable solution to increasing the field of
view is to look at small ubiquitous rectangular displays through lenses:</p>

<p><img src="/vr-lens-distortion/how-lenses-increase-fov.png" alt="Why VR needs lenses" /></p>

<p>Lenses placed close to your eyes greatly increase your field of view, but there
is a cost: the image becomes spherically distorted. The larger the field of
view, the more distorted the image. This post is a quick summary of three
different approaches to undistorting the image, all of which have been
implemented in JavaScript for various WebVR-related projects.</p>

<!--more-->

<p>Here is a closer look at the lens distortion of a typical head mounted display.
The lenses cause a pincushion effect:</p>

<p><img src="/vr-lens-distortion/pincushion-distortion.png" alt="Pincushion distortion due to lenses" /></p>

<p>The solution is to apply barrel distortion to the image. When we look at it through
the distorting lenses, the image looks neutral:</p>

<p><img src="/vr-lens-distortion/barrel-predistortion.png" alt="Barrel pre-distortion" /></p>

<p>Lens distortion is well understood mathematically, governed by equations <a href="https://en.wikipedia.org/wiki/Distortion_(optics)#Software_correction">like
these</a>, with distortion coefficients corresponding to the particular lens.
To undo the distortion properly, we also need to calculate the centers of the
eyes, which requires knowing a bit about the geometry of the display and the
enclosure itself. This can all be done, even on the web! I summarize a few
implementation options below.</p>

<h1>1. Fragment based solution (bad)</h1>

<p>The simplest way to using two pass rendering. First, we render the left and
right eyes onto a texture, and then process that texture with a fragment (pixel)
shader, moving each pixel inward in relation to the centroid of the eye:</p>

<p><img src="/vr-lens-distortion/dense.png" alt="Per-pixel based distortion" /></p>

<p>This is the first and simplest method, which is also the least efficient, since
each pixel is processed separately. The <a href="https://github.com/borismus/webvr-boilerplate/blob/d91cc2866bd54e65d59022800f62c7e160dc9fee/src/cardboard-distorter.js">first version</a> of the
WebVR Boilerplate implemented this method.</p>

<h1>2. Mesh based solution (better)</h1>

<p>Rather than processing each pixel separately, we distort the vertices of a
relatively sparse mesh (40x20 works well). </p>

<p><img src="/vr-lens-distortion/sparse.png" alt="Mesh based distortion" /></p>

<p>This can save some direct computation and let the GPU do a fair amount of
interpolation. Rather than having to apply to every single pixel (<code>1920 * 1080 ~=
2e6</code>), we do the calculation for every vertex in the mesh (<code>40 * 20 = 800</code>). The
result is a significant reduction (3 magnitudes or so) of computation, and a
nice boost in performance. The <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/cardboard-distorter.js">WebVR Polyfill</a> currently implements
this approach.</p>

<p>Applying distortion isn't the only expensive part in this rendering method. A
lot of time is wasted copying the whole scene to an intermediate texture.</p>

<h1>3. Vertex displacement based solution (best)</h1>

<p>This brings us to the most efficient method of the three, which eliminates the
need to render to an intermediate texture in the first place. In this approach,
the geometry itself is distorted using a custom vertex shader. The idea is that
knowing the position of the camera, we can displace vertices in such a way that
the resulting 2D render is already barrel distorted. In this case, no shader
pass is needed, and we save the expensive step of copying the rendering into a
texture. </p>

<p>This method does require a certain vertex density on every mesh that is being
deformed. Imagine the simple case of a large, 4-vertex rectangle being
rendered very close to the camera. Distorting these vertices would still yield a
4-vertex flat rectangle, and clearly there's no barreling effect. Because of
this, this is method does not generalize without extra work on the
developer's part.</p>

<p><img src="/vr-lens-distortion/cdl.png" alt="Cardboard Design Lab screenshot" /></p>

<p>This approach is used in the <a href="https://github.com/googlesamples/cardboard-unity/tree/master/Samples/CardboardDesignLab">Cardboard Design Lab</a> and in the open sourced
<a href="https://github.com/google/vrview/blob/master/src/vertex-distorter.js">VR View project</a>. Geometry-based distortion can also result in sharper
looking renderings, since the two pass approach can cause aliasing, especially
if the intermediate texture is small. You can read more about this distortion
method in <a href="http://www.gamasutra.com/blogs/BrianKehrer/20160125/264161/VR_Distortion_Correction_using_Vertex_Displacement.php">this helpful explainer</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Embedding VR content on the web</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/vr-views"/>
    
    <updated>2016-03-31T09:00:00-00:00</updated>
    
    <id>https://smus.com/vr-views</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>During a two week trip to India, I took over 1000 shots, including photos,
videos and a few photospheres. A picture is worth one thousand words, but how
many pictures is a photosphere worth? We may never know, but I digress. My
favorite photosphere was of friends posing inside one of the turrets of the
Jaigarh Fort:</p>

<iframe class="vrview" width="100%" height="300px" allowfullscreen frameborder="0" src="//storage.googleapis.com/vrview/index.html?image=//smus.com/vr-views/india-photosphere-4096.jpg&preview=//smus.com/vr-views/india-photosphere-1024.jpg&is_stereo=false"></iframe>

<script>
function DeviceMotionSender(){if(!this.isIOS_()){return}window.addEventListener("devicemotion",this.onDeviceMotion_.bind(this),false);this.iframes=document.querySelectorAll("iframe.vrview")}DeviceMotionSender.prototype.onDeviceMotion_=function(e){var message={type:"DeviceMotion",deviceMotionEvent:this.cloneDeviceMotionEvent_(e)};for(var i=0;i<this.iframes.length;i++){var iframe=this.iframes[i];var iframeWindow=iframe.contentWindow;if(this.isCrossDomainIframe_(iframe)){iframeWindow.postMessage(message,"*")}}};DeviceMotionSender.prototype.cloneDeviceMotionEvent_=function(e){return{acceleration:{x:e.acceleration.x,y:e.acceleration.y,z:e.acceleration.z},accelerationIncludingGravity:{x:e.accelerationIncludingGravity.x,y:e.accelerationIncludingGravity.y,z:e.accelerationIncludingGravity.z},rotationRate:{alpha:e.rotationRate.alpha,beta:e.rotationRate.beta,gamma:e.rotationRate.gamma},interval:e.interval}};DeviceMotionSender.prototype.isIOS_=function(){return/iPad|iPhone|iPod/.test(navigator.userAgent)&&!window.MSStream};DeviceMotionSender.prototype.isCrossDomainIframe_=function(iframe){var html=null;try{var doc=iframe.contentDocument||iframe.contentWindow.document;html=doc.body.innerHTML}catch(err){}return html===null};var dms=new DeviceMotionSender;
</script>

<p>I captured this using the photosphere camera which ships with Android. It's
embedded into my blog using <a href="https://developers.google.com/cardboard/vrview">VR View</a>, which <a href="https://developers.googleblog.com/2016/03/introducing-vr-view-embed-immersive.html">launched
today</a>. The embed above lets you include an interactive photosphere
right in your website, which is especially fun on mobile, where the image reacts
directly to your phone's movements. You can view it in full screen mode, and
even in Cardboard mode (only on mobile).</p>

<p>But you know what's cooler than a photosphere? A stereo photosphere! And
luckily, you can capture stereo photospheres using <a href="https://play.google.com/store/apps/details?id=com.google.vr.cyclops&amp;hl=en">Cardboard Camera</a>, and
then use a VR View to <a href="https://storage.googleapis.com/vrview/examples/pano/index.html">embed them too</a>. You can even embed mono or
<a href="https://storage.googleapis.com/vrview/examples/video/index.html">stereo videos</a>. Check out <a href="https://developers.google.com/cardboard/vrview">the docs</a> for more info. Eager
to hear what you think!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Simulating wealth inequality</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/simulating-wealth-inequality"/>
    
    <updated>2016-01-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/simulating-wealth-inequality</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Economic inequality is rising in the US. A viral video from several years ago
made this abundantly clear:</p>

<p><a href="https://www.youtube.com/watch?v=QPKKQnijnsM"><img src="/simulating-wealth-inequality/video_small.jpg" alt="Wealth inequality in U.S." /></a></p>

<p>The gap between desire, expectation and reality is truly shocking, and inspired
me to learn more. In particular, whether or not inequality is actually a big
problem, and then to better understand issues that the video above did not
address:</p>

<ol>
<li>How did the US become so economically unequal?</li>
<li>How can this inequality be reduced?</li>
</ol>

<p>My answers come in the form of simple simulations. For example, the following
simulation has two agents with different salaries, but the same spending habits.
You can <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">play with it</a> yourself!</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">
<video src="/simulating-wealth-inequality/simulator.mp4" autoplay loop style="width: 100%"></video>
</a></p>

<p>In the first part of this post, I try to provide some background on economic
inequality: how to measure it, various forms of it, and whether or not it's a
problem.  In the last part, I try to explain how we got to the status quo, and
how inequality can potentially be reduced. Rather than just making claims, I use
simulations like the one above to defend my claims. This way, you can see more
clearly where I'm coming from, and if you disagree, you can <a href="http://github.com/borismus/inequality-simulator">make your own
simulation</a> with better assumptions.</p>

<!--more-->

<h2>What is economic inequality?</h2>

<p>There are <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">three main ways</a> to measure economic inequality: income,
consumption, and wealth. Income inequality in particular has become a huge
national issue, with <a href="https://berniesanders.com/issues/income-and-wealth-inequality/">some presidential candidates</a> focusing large
amounts of their time addressing it directly, with policies such as the $15
federal minimum wage.</p>

<p>Inequality can be measured using the Gini coefficient. The greater the Gini, the
more unequal a society is. A Gini of 0 means perfect equality: everybody has the
same. A Gini of 1 means perfect tyranny: the winner takes it all. This metric
can be applied to any distribution: wealth, income, or consumption.</p>

<p>All forms of inequality are unequal, but some are more unequal than others! In
general, wealth is the most unequally distributed of the three indicators,
consumption the least. But which measure of inequality is most important to
consider?</p>

<h3>Types of economic inequality</h3>

<p>Income inequality is difficult to measure. What constitutes income? Obviously a
salary is included, but how about investment income? Unsold stocks? Options?
The <a href="https://en.wikipedia.org/wiki/One-dollar_salary">list of $1 salary CEOs</a> is famously long, but what is their effective
income? Pew Research provides <a href="http://www.pewresearch.org/fact-tank/2015/09/22/the-many-ways-to-measure-economic-inequality/">many more reasons</a> why income is hard to
measure, and may not be a meaningful indicator:</p>

<blockquote>
  <p>Some economists say income data have too many flaws to be the primary measure
  of inequality. For one thing, many income inequality measures use income
  before accounting for the impact of taxes and transfer payments. [...]
  In addition, critics of the income-based approach note that an individual’s
  (or household’s) income can vary considerably over time, and may not reflect
  all available economic resources.</p>
</blockquote>

<p>Income also isn't a great indicator for quality of life. Indeed, many economists
agree that <a href="http://www.aei.org/wp-content/uploads/2012/06/-a-new-measure-of-consumption-inequality_142931647663.pdf">consumption inequality</a> is a better proxy for that.
<a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a> writes:</p>

<blockquote>
  <p>For the purpose of measuring how inequality affects a community [income
  inequality] is also probably the least interesting yardstick of the three.
  Consumption inequality, though harder to measure, provides a better proxy of
  social welfare. This is because people’s living standards depend on the amount
  of goods and services they consume, rather than the number of dollars in their
  wage packet.</p>
</blockquote>

<p>But consumption inequality has its limitations too. For one, it is difficult to
measure directly. More importantly, consumption is an indication of the current
state, but does not reflect ones ability to deal with the future. Wealth and
consumption are tightly linked. When times get tough, only the wealthy can
maintain their lifestyle by dipping into their savings. Having this reserve of
"potential energy" is especially important in inevitable periods of instability.</p>

<p>This leaves us with wealth inequality, which I will focus on from now on.</p>

<h3>Adverse effects of extreme wealth inequality</h3>

<p>There are plenty of arguments to be made for dangers of high wealth inequality.
The common sense reason is the diminishing marginal utility of wealth. For an
unemployed person, suddenly having a job that pays $40K is a game changer. But
for a top-1% income earner already making $500K, the additional $40K makes no
practical difference.</p>

<p>Another economic argument goes something like this. Low wealth causes reduced
purchasing power, which ultimately means less money going to corporations, fewer
jobs, and a slower economy. More people should have spending power, which will
keep our economy running smoothly. Robert Reich makes this point well in his
moving <a href="http://www.pbs.org/newshour/making-sense/why-robert-reich-cares-so-passionately-about-economic-inequality/">Inequality for All</a>. However, the link between low wealth and
reduced spending is somewhat tenuous, given the much less extreme consumption
inequality distribution.</p>

<p>Wealth as potential energy also has a psychological dimension. Wealth gives some
peace of mind that you have a buffer against unforseeable problems, increasing
well being. This is especially important in countries with weak social programs
and relatively small safety nets. In such scenarios, more people feel the need
to accumulate wealth as a personal buffer.</p>

<p>There is also something philosophically unfair about wealth concentration. What
makes a society fair is a matter of opinion, but the <a href="https://en.wikipedia.org/wiki/Original_position">Original Position</a>, a
thought experiment proposed by John Rawls provides an interesting starting
point. In the Original Position, you and your hypothetical countrymen select
principles that will determine the basic structure of the society you will live
in. This choice is made from behind what he calls a veil of ignorance, which
prevents you from knowing your economic status.</p>

<p>Being born into a society with high wealth inequality, you are subject to a
"lottery of birth". Quoth <a href="http://www.economist.com/blogs/freeexchange/2014/07/measuring-inequality">The Economist</a>:</p>

<blockquote>
  <p>Wealth is also an important metric since it can be inherited, unlike income.
  When wealth inequality increases, the lottery of birth becomes an increasingly
  important determinant of living standards. Consequently, a society which wants
  to ensure an equal level of opportunity, in which outcomes are not closely
  linked to surnames, will endeavour to keep wealth inequality at tolerably low
  levels.</p>
</blockquote>

<h3>Some wealth inequality is good</h3>

<p>Yet clearly we don't want complete economic equality. It's important that people
work and create value. The best way to do this is to incentivize them by
rewarding high performing individuals. History has shown socialist societies
like the Soviet Union fail in part because there was no incentive to work. In Soviet
Russia, wealth inequality was low: everybody except the ruling class had the
same amount of the sad little pie. A small piece of a much larger (eg. American)
pie is better than an average slice of a small (eg. Soviet) one. This <a href="https://www.khanacademy.org/economics-finance-domain/macroeconomics/gdp-topic/piketty-capital/v/inequality-good-or-bad">Khan
Academy video</a> makes this point well.</p>

<p>Wealth inequality alone is not a great indicator of prosperity either. Many
Scandinavian countries have very high wealth inequality, potentially because
life is already so good. According to <a href="http://www.businessinsider.com/why-socialist-scandinavia-has-some-of-the-highest-inequality-in-europe-2014-10">Credit Suisse</a>,</p>

<blockquote>
  <p>Strong social security programs, good public pensions, free higher education
  or generous student loans, unemployment and health insurance can greatly
  reduce the need for personal financial assets.</p>
</blockquote>

<p>Wealth acts as a personal safety net. In countries with significant public
safety nets for ailing citizens, accumulating wealth is less important. Compare
a society with high inequality but a solid public safety net, with one with
equality but no public safety net. From the perspective of Rawls' Original
Position, the solid safety net is preferred, since even if you are the poorest
in such a society, you are still guaranteed a standard of living.</p>

<h2>Modeling inequality</h2>

<p>Inspired by <a href="http://worrydream.com/">Bret Victor</a>, <a href="http://ncase.me/">Nicky Case</a>, and chats with <a href="http://mikejohnstn.com/">Mike
Johnston</a>, I've built a visualizer to give an <a href="http://worrydream.com/ExplorableExplanations/">explorable explanation</a>
of how wealth inequality arises, and how policy changes can reduce it.</p>

<p>The simulation itself is very simple, consisting of a set of rules which can be
defined in JSON, and then households that have property bags. Every year, each
rule is applied to each household in order. The result of each rule is some
change in the net worth of the household. Each simulation is defined by a
collection of rules and actors. In the GUI, you can inspect rules and actors by
clicking on them. The visualization itself is implemented in <a href="http://threejs.org/">three.js</a>.
For more information, check out the <a href="http://github.com/borismus/inequality-simulator">github</a> page.</p>

<p>Using these models, let's jump in and explore some factors contributing to our
current state of wealth inequality. Then, some policies that can change the
status quo.</p>

<h3>Cause 1: Income inequality</h3>

<p>In this first simulation, we consider two households: one with low income and
one with high income. They have the same spending habits, but the high income
household has twice the income.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js"><img src="/simulating-wealth-inequality/screenshots/1-world-income-ineq-doesnt-lead-to-wealth-ineq.png" alt="Income inequality not sole cause wealth inequality." /></a></p>

<p>As you can see above (or if you click the image and <a href="http://borismus.github.io/inequality-simulator/?model=1-world-income-ineq-doesnt-lead-to-wealth-ineq.js">run the simulation
yourself</a>), such a scenario does not yield huge differences in wealth.
Even a hundred years later, wealth remains proportional to income, so we look to
other factors to explain the wealth inequality we see today.</p>

<h3>Cause 2: Investors win over the long term</h3>

<p>In addition to salaries, households can also invest money. For simplicity,
assume that the net worth of each household is subject to some investment
return. Most Americans (52%) <a href="http://www.gallup.com/poll/182816/little-change-percentage-americans-invested-market.aspx">avoid the stock market</a> entirely, which
cuts them out from any investment income.</p>

<p>In the next simulation, one household does not invest at all, and another
household invests its whole net worth. We assume that the yield is the average
return of the market, which is about <a href="http://www.marketwatch.com/story/8-lessons-from-80-years-of-market-history-2014-11-19">10% from 1930 to 2013</a>.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=2-investing-ability.js"><img src="/simulating-wealth-inequality/screenshots/2-investing-ability.png" alt="Simulation of investors vs. non-investors" /></a></p>

<p>This is effectively a demonstration of compound interest. Given the <a href="http://www.bloombergview.com/articles/2015-09-25/the-rich-are-different-they-re-better-investors-">correlation
between wealth and investment ability</a>, the impact of investing is huge on
wealth inequality.</p>

<h3>Cause 3: Entrepreneurship can have huge payoff</h3>

<p>Many of the wealthiest people in the world became so by creating new companies.
Most enterprises fail, but it only takes one incredible success to generate
massive amounts of wealth.</p>

<p>I found that modeling this accurately is very difficult, but for the purposes of
illustration, this next simulation includes three households: a
non-entrepreneur, a regular entrepreneur, and a lucky entrepreneur. An
enterprise failure (10% yearly chance) is modeled as a 5% reduction in wealth,
while a success (1% yearly chance, 2% if lucky) is modeled as a 50% increase in
wealth.</p>

<p>The expected wealth of the regular entrepreneur is the same as the
non-entrepreneur, but the lucky entrepreneur has a 2% chance of success, and
thus a higher expected wealth. Entrepreneurship introduces volatility and can
lead to more wealth inequality.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=3-with-entrepreneurship.js"><img src="/simulating-wealth-inequality/screenshots/3-with-entrepreneurship.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<h3>All together: income, investments, and entrepreneurship</h3>

<p>Consider all of these factors together: varying salaries, investment abilities
and entrepreneurial inclinations/luck. Here we have 8 agents with varying
parameters along these dimensions.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=4-income-invest-entrepreneur.js"><img src="/simulating-wealth-inequality/screenshots/4-income-invest-entrepreneur.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<p>We can see that after 50 years, we have a Gini of 0.44. In the real world, the
spread of incomes is much greater than here, the most successful entrepreneurs
make orders of magnitudes more than regular employees, and the best investors
are wildly successful. The <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">real world wealth Gini</a> of the US is
0.8.</p>

<h2>Reducing inequality</h2>

<p>Working within the system, inequality can be reduced through progressive
taxation of the wealthy. However, it's key to avoid becoming a <a href="https://wiki.lesswrong.com/wiki/Paperclip_maximizer">paperclip
maximizer</a> when it comes to the Gini coefficient. Making the
wealthiest slightly less wealthy will certainly reduce the Gini, but will do
little to improve life for actual poor people.</p>

<p>Through additional taxation, the wealthy end up being less wealthy, with the
difference going to the government. Implied is a hope that the government is
capable and sufficiently efficient to use this extra money for good. By
investing in public works, creating relevant jobs, and establishing a more solid
safety net, there is potential to improve lives of those that are less
fortunate.</p>

<h3>Solution 1: Tax capital gains like income</h3>

<p>Compound interest is a powerful force. Once an individual's wealth is large
enough, returns on investing their wealth will exceed even their salary.
However, the US currently imposes a very low capital gains tax, a long-term
capital gain tax rate of 15% for most normal annual incomes.</p>

<p>An easy solution is to increase capital gains taxes, or simply to treat capital
gains like regular income. This would effectively reduce the return rate on
investment and reduce inequality. The following simulation shows what happens
when investment income is taxed at a flat 40%. This is a crude estimate, since
it would actually be subject to a variable tax rate like the income tax, but
gets the point across.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=5-higher-capital-gains-tax.js"><img src="/simulating-wealth-inequality/screenshots/5-higher-capital-gains-tax.png" alt="Simulation of higher capital gains." /></a></p>

<p>As you can see, the Gini at 100 years is much smaller than before.</p>

<h3>Solution 2: Estate taxes</h3>

<p>Estate tax is intended as an effective tool for preventing the concentration of
wealth in the hands of a relatively few powerful families. It also encourages
charitable giving, since the money that is to be bequeathed is subject to the
tax.</p>

<p>Estate tax is collected when the deceased transfers their wealth to the
recipient of their inheritance. The deceased's net worth exceeding a certain
threshold is subject to the estate tax rate. Both the threshold and the tax rate
have varied a surprising <a href="https://docs.google.com/spreadsheets/d/1lWzzz6RlMmxWTGYoU9kxxXqL9Q8Pto4cbzZRozVc8wU/pubhtml">amount over time</a>:</p>

<p><img src="/simulating-wealth-inequality/estate-tax-history.png" alt="Historical estate taxes" /></p>

<p>In a previous simulation, we saw what would have happened with no estate tax (as
was the case in 2010, a good year to die). The following simulation shows the
average tax rate since 2000, which is 41%, with a threshold of 100 units.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=6-estate-tax.js"><img src="/simulating-wealth-inequality/screenshots/6-estate-tax.png" alt="Simulation of entrepreneurs vs. non-entrepreneurs" /></a></p>

<h3>Solution 3: Wealth taxes</h3>

<p><a href="http://www.cnbc.com/2015/03/10/why-we-need-a-global-wealth-tax-piketty.html">Piketty's solution</a> to inequality is a global wealth tax. The idea is
that individuals with over a certain amount of wealth (here, 100 units) be taxed
at some rate for just maintaining that level of wealth. This seems difficult to
enforce, especially since in a global economy, a single neutral country
(say, Switzerland?) that does not impose a wealth tax will end up being a
natural safe haven for the rich.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=7-wealth-tax.js"><img src="/simulating-wealth-inequality/screenshots/7-wealth-tax.png" alt="Simulation of wealth taxes" /></a></p>

<h2>Conclusion</h2>

<p>Geez, you're still here?</p>

<h3>Practical limitations</h3>

<p>Theoretically, inequality is not an insurmountable issue by any stretch. As
shown, by introducing policies like increased capital gains tax, estate tax and
wealth tax, inequality can be reduced. The real question is how much inequality
is actually desirable, and how effective the above policies are in practice.</p>

<p><a href="http://borismus.github.io/inequality-simulator/?model=8-solution.js"><img src="/simulating-wealth-inequality/screenshots/8-solution.png" alt="Estate taxes, wealth taxes and " /></a></p>

<p>In practice, estate tax is often avoided or minimized, <a href="http://www.calculator.net/estate-tax-calculator.html">according to the
Urban-Brookings Tax Policy Center</a>,</p>

<blockquote>
  <p>Among the 3,780 estates that owe any tax, the "effective" tax rate — that is,
  the percentage of the estate's value that is paid in taxes — is 16.6 percent,
  on average. </p>
</blockquote>

<p>A wealth tax is even harder to enforce, since you can simply move your wealth to
a country that does not have a wealth tax.</p>

<h2>Conclusion</h2>

<p><strong>Roll your own</strong>. The good news is that the models above show how inequality
can arise and how inequality can be effectively reduced! The bad news is that I
just made these models up with only a minimal understanding of how the world
works. However, more good news! If you are wise in the ways of economics and/or
have a suggestion for a more accurate, or perhaps more provocative way of
modeling wealth inequality, get in touch! Or if you just want to DIY, simulation
and visualizer are <a href="http://github.com/borismus/inequality-simulator">on the githubs</a>.</p>

<p><strong>Eyes on the prize</strong>. <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Zimbabwe and Denmark</a> both have high
wealth ginis (over 0.8), while <a href="https://docs.google.com/spreadsheets/d/1HZBF47q9DYgFj49Q2ZLBb_3gpsoayAuGF_kUdYIM9nQ/pubhtml?gid=0&amp;single=true">Yemen and Japan</a> both have low
wealth ginis (under 0.6), yet these pairs of countries couldn't be more
different. Wealth inequality in itself is not really the problem, just an
indicator. The Rawlsian goal is not to reduce it arbitrarily, but to make life
actually better for everybody.</p>

<p><strong>Micro to macro</strong>. The simple two household simulations I started with feel
like microeconomics. The more complex simulations we ended with started feeling
more like something from macroeconomics. Put another way, each household starts
with just a couple of bars of wealth, but as the simulation proceeds, the canvas
begins to resemble a bar chart. I found this quantity-to-quality transition
fascinating.</p>

<p><strong>On simulations</strong>. I'm intrigued by simulations as way of explaining
complicated things to non-experts. However, any simulation is inherently
inaccurate, as it approximates the real world in order to have explanatory
power. In other words, there is some continuum between accuracy and
insight. The simulations in this post are more simple than they are realistic,
however, I hope they are at least somewhat informative and interesting.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Sensor fusion and motion prediction</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/sensor-fusion-prediction-webvr"/>
    
    <updated>2015-11-05T09:00:00-00:00</updated>
    
    <id>https://smus.com/sensor-fusion-prediction-webvr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>A major technical challenge for VR is to make head tracking as good as possible.
The metric that matters is called <strong>motion-to-photon latency</strong>. For mobile VR
purposes, this is the time that it takes for a user's head rotation to be fully
reflected in the rendered content.</p>

<p><img src="/sensor-fusion-prediction-webvr/latency-chain.jpg" alt="Motion to photon pipeline" /></p>

<p>The simplest way to get up-and-running with head tracking on the web today is
to use the <code>deviceorientation</code> events, which are generally well supported across
most browsers. However, this approach suffers from several drawbacks which can
be remedied by implementing our own sensor fusion. We can do even better by
predicting head orientation from the gyroscope.</p>

<p>I'll dig into these techniques and their open web implementations.  Everything
discussed in this post is implemented and available open source as part of the
<a href="https://github.com/borismus/webvr-polyfill">WebVR Polyfill</a> project. If you want to skip ahead, check out
the <a href="http://borismus.github.io/webvr-boilerplate/">latest head tracker</a> in action, and play around with this <a href="http://borismus.github.io/sensor-fusion/">motion
sensor visualizer</a>.</p>

<!--more-->

<h2>The trouble with device orientation</h2>

<p>The web provides an easy solution for head tracking through the
<code>deviceorientation</code> event, which gives Euler angles corresponding to your
phone's 3-DOF orientation in space. This orientation is calculated through an
undisclosed algorithm. Until very recently, <a href="http://w3c.github.io/deviceorientation/spec-source-orientation.html">the spec</a> didn't
even specify whether or not these events should give your phone's orientation in
relation to north or not. However, recently <a href="https://github.com/w3c/deviceorientation/pull/22">accepted spec
changes</a> make this behavior more standard across
browsers.</p>

<p>In Android, the JavaScript <code>deviceorientation</code> event was implemented using
<code>Sensor.TYPE_ORIENTATION</code> in Android, which fuses accelerometer, gyroscope and
magnetometer sensors together to give a North-aligned orientation. The trouble
is that the magnetometer's estimate of magnetic North is easily affected by
external metallic objects. On many devices, the North estimate continually
changes, even when you are not looking around. This breaks the correspondence
between motion and display, a recipe for disaster.</p>

<p>Another issue in some implementations is that the <code>deviceorientation</code> sensor
ramps up and down in firing rate depending on the speed of the phone's rotation.
Try opening up <a href="http://jsbin.com/device-inertial-sensor-diagnostics">this diagnostic page</a> on Android. This variation in
sensor update rate is not good for maintaining a reliable head track.</p>

<p>To top it off, a <a href="http://crbug.com/540629">recent regression in Android M</a> broke
<code>deviceorientation</code> for Nexus 5s. Why do bad bugs happen to good people?</p>

<h3>What is to be done?</h3>

<p>We implement our own sensor fusion with <code>devicemotion</code>, which provides lower
level accelerometer &amp; gyroscope events. These fire at a regular rate. When you
search for "sensor fusion", jumping into the rabbit hole will quickly take you
into the realm of Kalman Filters. This is a bit more firepower than we will need
for the moment, although I did finally get a better sense of the concept with
the help of a <a href="https://www.youtube.com/watch?v=18TKA-YWhX0">boring but understandable explanation</a>.</p>

<p>Luckily, there are simpler alternatives such as the Complementary Filter, which
is what we'll talk about next.</p>

<h2>Your sensing smartphone</h2>

<p>Let us start with the basics: sensors. There are three fundamental motion
tracking sensors in your smartphone. </p>

<p>Accelerometers measure any acceleration, returning a vector in the phone's
reference frame. Usually this vector points down, towards the center of the
earth, but other accelerations (eg. linear ones as you move your phone) are also
captured. The output from an accelerometer is quite noisy by virtue of how the
sensor works. Here's a plot of the rotation around the X-axis according to an
accelerometer:</p>

<p><img src="/sensor-fusion-prediction-webvr/accel.gif" alt="Animation of X-axis accelerometer output with a phone turning around the X axis" /></p>

<p>Gyroscopes measure rotations, returning an angular rotation vector also in the
phone's reference frame. Output from the gyro is quite smooth, and very
responsive to small rotations. The gyro can be used to estimate pose by keeping
track of the current pose and adjusting it every timestep, with every new gyro
reading. This integration works well, but suffers from drift. If you were to
place your phone flat and capture it's gyro-based position, then pick it up,
rotate it a bunch, and place it flat again, its integrated gyro position might
be quite different from what it was before due to the accumulation of errors
from the sensor. Rotation around the X-axis according to a gyroscope:</p>

<p><img src="/sensor-fusion-prediction-webvr/gyro.gif" alt="Animation of X-axis gyroscope output with a phone turning around the X axis" /></p>

<p>Magnetometers measure magnetic fields, returning a vector corresponding to the
cumulative magnetic field due to any nearby magnets (including the Earth). This
sensor acts like a compass, giving an orientation estimate of the phone. This is
incredibly useful combined with the accelerometer, which provides no information
about the phone's yaw. Magnetometers are affected not by the Earth, but by
anything with a magnetic field, including <a href="http://smus.com/magnetic-input-mobile-vr/">strategically placed permanent
magnets</a> and also ferromagnetic metals which are often found in substantial
quantities in certain environments.</p>

<h2>Intuition: why do we need sensor fusion?</h2>

<p>Each sensor has its own strengths and weaknesses. Gyroscopes have no idea where
they are in relation to the world, while accelerometers are very noisy and can
never provide a yaw estimate. The idea of sensor fusion is to take readings from
each sensor and provide a more useful result which combines the strengths of
each. The resulting fused stream is greater than the sum of its parts. </p>

<p>There are many ways of fusing sensors into one stream. Which sensors you fuse,
and which algorithmic approach you choose should depend on the usecase.
The accelerometer-gyroscope-magnetometer sensor fusion provided by the
system tries really hard to generate something useful. But as it turns out, it
is not great for VR head tracking. The selected sensors are the wrong ones, and
the output is not sensitive enough to small head movements.</p>

<p>In VR, drifting away from true north is often fine since you aren't looking at
the real world anyway. So there's no need to fuse with magnetometer. Reducing
absolute drift is, of course, still desirable in some cases. If you are sitting
in an armchair, maintaining alignment with the front of your chair is critical,
otherwise you will find yourself having to crank your neck too much just to
continue looking forward in the virtual world. For the time being, we ignore
this problem.</p>

<h2>Building a complementary filter</h2>

<p>The complementary filter takes advantage of the long term accuracy of the
accelerometer, while mitigating the noise in the sensor by relying on the
gyroscope in the short term. The filter is called complementary because
mathematically, it can be expressed as a weighted sum of the two sensor streams:</p>

<p><img src="/sensor-fusion-prediction-webvr/filter-equation.png" class="center" 
    title="Filter equation" /></p>

<p>This approach relies on the gyroscope for angular updates to head orientation,
but corrects for gyro drift by taking into account where measured gravity is
according to the accelerometer.</p>

<p>Initially inspired by <a href="http://www.pieter-jan.com/node/11">Pieter's explanation</a>, I built this filter by
calculating roll and pitch from the accelerometer and gyroscope, but quickly ran
into issues with <a href="https://en.wikipedia.org/wiki/Gimbal_lock">gimbal lock</a>. A better approach is to use quaternions
to represent orientation, which do not suffer from this problem, and are ideal
for thinking about rotations in 3D. Quaternions are complex (ha!) so I won't go
into much detail here beyond linking to a <a href="http://www.3dgep.com/understanding-quaternions/">decent primer</a> on the
topic. Happily, quaternions are a useful tool even without fully understanding
the theory, and many implementations exist. For this filter, I used <a href="http://threejs.org/docs/#Reference/Math/Quaternion">the
one</a> found in THREE.js.</p>

<p>The first task is to express the accelerometer vector as a quaternion rotation,
which we use to initialize the orientation estimate (see
<a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.accelToQuaternion_</code></a>).</p>

<pre><code>quat.setFromUnitVectors(new THREE.Vector3(0, 0, -1), normAccel);
</code></pre>

<p>Every time we get new sensor data, calculate the instantaneous change in
orientation from the gyroscope. Again, we convert to a quaternion, as follows
(see: <a href="https://github.com/borismus/webvr-polyfill/blob/master/src/complementary-filter.js"><code>ComplementaryFilter.gyroToQuaternionDelta_</code></a>):</p>

<pre><code>quat.setFromAxisAngle(gyroNorm, gyro.length() * dt);
</code></pre>

<p>Now we update the orientation estimate with the quaternion delta. This is a
quaternion multiplication:</p>

<pre><code>this.filterQ.copy(this.previousFilterQ);
this.filterQ.multiply(gyroDeltaQ);
</code></pre>

<p>Next, calculate the estimated gravity from the current orientation and compare
it to the gravity from the accelerometer, getting the quaternion delta.</p>

<p><img src="/sensor-fusion-prediction-webvr/complementary-filter.png" class="center" 
    title="Complementary filter visual illustration" /></p>

<pre><code>deltaQ.setFromUnitVectors(this.estimatedGravity, this.measuredGravity);
</code></pre>

<p>Now we can calculate the target orientation based on the measured gravity, and
then perform a <a href="https://en.wikipedia.org/wiki/Slerp">spherical linear interpolation (SLERP)</a>. How much to
slerp depends on that constant I mentioned before. If we don't slerp at all, we
will end up only using the gyroscope. If we slerp all the way to the target, we
will end up ignoring the gyroscope completely and only using the accelerometer.
In THREE parlance:</p>

<pre><code>this.filterQ.slerp(targetQ, 1 - this.kFilter);
</code></pre>

<p>Sanity checking the result, we expect the filter output to be roughly parallel
to the gyroscope readings, but to align with the accelerometer reading over the
long term. Below, you can see the accelerometer and gyroscope (green and blue)
and compare them to the fused output (orange):</p>

<p><img src="/sensor-fusion-prediction-webvr/fusion.gif" alt="Complementary filter output" /></p>

<h2>Predicting the future</h2>

<p>As your program draws each frame of rendered content, there is delay between
the time you move your head and the time the content actually appears on the
screen. It takes time for the sensors to fire, for firmware and software to
process sensor data, and for a scene to be generated based on that sensor data.</p>

<p>In Android, this latency is often on the order of 50-100 ms with sensors firing
on all cylinders (the technical term for 200 Hz) and some nice graphics
optimizations. The web suffers a strictly worse fate since sensors often fire
slower (60 Hz in Safari and Firefox), and there are more hoops of abstraction to
jump through. Reducing motion-to-photon latency can be done by actually reducing
each step in the process, with faster sensor processing, graphics optimizations,
and better algorithms. It can also be reduced by cheating!</p>

<p>We can rely on a <a href="https://en.wikipedia.org/wiki/Dead_reckoning#Directional_dead_reckoning">dead reckoning</a> inspired approach, but rather
than predicting position based on velocity, we predict in the angular domain.
Once we predict the orientation of the head in the (near) future, use that
orientation to render the scene. We predict based on angular velocity, assuming
that your head will keep rotating at the same rate. More complex schemes are
possible to imagine too, using acceleration (2nd order) or Nth order prediction,
but these are more complex, and so more expensive to calculate, and don't
necessarily yield better results.</p>

<pre><code>var deltaT = timestampS - this.previousTimestampS;
var predictAngle = angularSpeed * this.predictionTimeS;
</code></pre>

<p>The way this works is pretty straight forward, using angular speed from the
gyroscope, we can predict a little bit into the future to yield results like
this:</p>

<p><img src="/sensor-fusion-prediction-webvr/prediction.gif" alt="Predicted vs. sensor fusion." /></p>

<p>Notice that the predicted signal (in red) is somewhat ahead of the fused one (in
orange). This is what we'd expect based on the motion prediction approach taken.
The downside of this is that there is noticeable noise, since sometimes we
over-predict, and are forced to return back to the original heading.</p>

<h2>Plotting graphs</h2>

<p>Although still in very active development, <a href="https://gitgud.io/unconed/mathbox/">Mathbox2</a> is already a
formidable visualization toolkit. It is especially well suited to output in 3D,
which I used actively to debug and visualize the filter.</p>

<p>I also used Mathbox2 to generate plots featured earlier in this blog post. I
wrote a live-plotting tool that can compare gyroscope, accelerometer, fused and
predicted streams on each axis, and also let you tweak the filter coefficient
and how far into the future to predict.</p>

<p><img src="/sensor-fusion-prediction-webvr/plot-options.png" class="center"
    title="Preview of the options available in the plot"/></p>

<p>You too can <a href="http://borismus.github.io/sensor-fusion/">try the plots live on your phone</a>. After all, it's just a
mobile webpage! Many thanks to <a href="https://twitter.com/pierregeorgel">Pierre
Fite-Georgel</a> and <a href="https://github.com/jkammerl">Julius
Kammerl</a> for lending their incredible
filter-building skills to this project.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Hot bread: delicious or deadly?</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/hot-bread-delicious-deadly"/>
    
    <updated>2015-09-23T09:00:00-00:00</updated>
    
    <id>https://smus.com/hot-bread-delicious-deadly</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Despite free access to information via the Internet and an increasingly global
world, people still seem to have all sorts of divergent ideas about how the world
works. For example, did you know that eating hot bread and pastries is
incredibly unhealthy? Indeed, it can often even lead to complete bowel
obstruction! I learned this fact as a kid, while growing up in the Soviet Union.
Understandably, I have been very careful to avoid eating hot baked goods.
That is, until recently, when my American girlfriend questioned the validity of my
belief and I began to harbor some doubts. I decided to check if it was actually
true, and asked Google. The results were very clear: I had fallen prey to an old
wives tale. My worldview, shattered.</p>

<p>Incredulous, I searched for the same thing in Russian and arrived at the
opposite conclusion. "What's up with that?" I thought, and wrote this post.</p>

<!--more-->

<h2>Asking in different languages</h2>

<p>I searched Google for "hot bread unhealthy", and tallied up the top 5 results:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>davidwalbert.com</td><td>No</td></tr>
<tr><td>chestofbooks.com</td><td>Maybe</td></tr>
<tr><td>gurumagazine.org</td><td>For some</td></tr>
<tr><td>lthforum.com</td><td>No</td></tr>
<tr><td>answers.yahoo.com</td><td>No</td></tr>
</table>

<p>I then compared it to an equivalent Russian search string: "горячий хлеб
вреден". The following are my results in English:</p>

<table>
<tr><th>Domain</th><th>Unhealthy?</th></tr>
<tr><td>useful-food.ru</td><td>Yes</td></tr>
<tr><td>foodblogger.ru</td><td>Yes</td></tr>
<tr><td>hlebopechka.ru</td><td>Yes</td></tr>
<tr><td>otvet.mail.ru</td><td>Maybe</td></tr>
<tr><td>otvet.mail.ru</td><td>Yes</td></tr>
</table>

<p>My <a href="https://goo.gl/ltPefm">working spreadsheet</a> contains more colorful details
if you are interested.</p>

<h2>Language shapes your... search results?</h2>

<p>No English language site suggested that eating hot bread was unhealthy. Three of
the top five results explicitly point it out as an old wives tale. The first hit,
<a href="http://goo.gl/Cj9jKS">the most skeptical of the bunch</a> even cites articles from
the 18th and 19th centuries which have since been refuted.</p>

<p>In stark contrast, no Russian language site suggested that eating fresh
bread was totally fine. Four of five of the top results explicitly said that it
was unhealthy, suggesting that fresh bread is difficult to digest, encourages
swallowing without chewing, and eating it leads to all sorts of gastrointestinal
trouble like stomach pain, inflammation, constipation and full on bowel
obstruction. Oh my!</p>

<p>One possibility is that the environments of the Russian and English speaker are
in fact completely different. The bread making processes in Russia could differ
from other places in the world. Many Russians favor rye bread, which takes some
effort to find in North America, for example. The main reason for unhealthiness
of fresh bread seems to be related to it being undercooked, with the yeast still
being active until it cools. Maybe rye better protects the yeast, or takes less
time or heat to cook? </p>

<p>This and other theories are possible, though not likely. My intuition suggests a
simpler explanation.</p>

<h2>Nothing Is True and Everything Is Possible</h2>

<p>There is an expression in Russian: "умом Россию не понять", which roughly
translates as "Russia cannot be understood with the mind". There is a certain 
mystery deeply ingrained in the national character which, fascinatingly, has
always been a point of pride.  The heading of this section is actually taken
from the title of a <a href="http://www.amazon.com/Nothing-Is-True-Everything-Possible/dp/1610394550">book about modern Russia</a>, subtitled "The Surreal
Heart of the New Russia". In Russia, rationalism and skepticism is on the
decline, in favor of traditionalism and magical thinking. Given a rich tradition
of <a href="https://en.wikipedia.org/wiki/Russian_traditions_and_superstitions">traditions, superstitions, and beliefs</a> in Russian culture, there is a
large pool of absurdity to pick from.</p>

<p>Given that, and my recent search history, you can imagine what I now believe
about the harmful effects of eating freshly baked bread. I don't much care
whether or not eating fresh bread is healthy, especially since as a card
carrying Celiac, I can't even enjoy the delicious kind. The fascinating
conclusion from my multilingual sojourn is this:</p>

<p><strong>Having searched for the same thing in their native languages, a Russian
speaker and an English speaker would have arrived at a completely different
world-view.</strong></p>

<p>The Russian language <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_Russian_is_an_official_language">maps closely</a> to Russia and Russian culture, certainly
more so than <a href="https://en.wikipedia.org/wiki/List_of_territorial_entities_where_English_is_an_official_language">English does</a> to any particular country and culture. The
result is that queries in Russian are suspect to a very natural echo chamber,
echoing and amplifying deeply held beliefs with the help of our supposedly
normalizing open Internet. </p>

<h2>Translated foreign pages</h2>

<p>There are hundreds of other examples of queries that when translated will yield
dramatically different results much like "hot bread unhealthy"/"горячий хлеб
вреден". There's a simple formula for finding more. Pick a language and write a
query string, translate it into another language, perform both searches and
analyze the top results.</p>

<p>This sounds a lot like something that can be automated. Indeed, Google used to
automatically translate queries, perform searches with translated queries, and
surface them to the user. Unfortunately this "Translated foreign pages" feature
was <a href="https://productforums.google.com/forum/#!topic/websearch/tYo0LpcVobI/discussion">removed several years ago</a>, due to lack of usage. Also, there are
difficulties with automating the process. The Google Translation of "hot bread
unhealthy" is "горячий хлеб нездоровый", which in Russian sounds like the bread
itself is ill, and yields less relevant search results.</p>

<p>It's surprising how clearly this cultural difference can be seen through the
simple example of warm bread and a search engine. The initial surprise can be
easily explained though, since the search engine crawls a naturally insular
corpus of articles in the same language. Many of the search results in English
cite the same sources. The same is true for search results in Russian. The key
point, though, is that there is very little shared linking between the English
and Russian sites, especially since <a href="https://en.wikipedia.org/wiki/List_of_countries_by_English-speaking_population">only 5% of Russians speak English</a>.
The language corpuses seem to be almost completely insulated from one another.
Inevitably, confirmation bias kicks in and you end up with the polarized world
we live in today.</p>

<p>I'd love to see what similar analyses on other search queries. For instance,
there is a Russian gadget called a <a href="http://www.amazon.com/Dark-Blue-Lamp-Minin-Reflector/dp/B00RPG6UTW">Minin Reflector</a>, which consists of a
lamp with a blue filter. You simply shine it onto the part of your body that
ails you, and presto, instant pain relief... sigh!</p>

<p>Wrapping up this blog, I am enjoying some delicious, fresh from the oven, hot
muffins. I'll keep you posted with the definitive truth!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>UbiComp and ISWC 2015</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/ubicomp-iswc-2015"/>
    
    <updated>2015-09-16T09:00:00-00:00</updated>
    
    <id>https://smus.com/ubicomp-iswc-2015</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>I recently returned from ISWC 2015, where I presented the <a href="/magnetic-input-mobile-vr/">Cardboard
Magnet</a> paper. In addition to seeing old friends,
meeting new ones, and being inspired by some interesting research, it was an
excellent excuse to visit Osaka, Japan! This year, ISWC was co-located with
UbiComp, and the combined conference had four tracks. This post is by no means
exhaustive, just some of the more interesting work I got a chance to see.</p>

<!--more-->

<p><strong>Opening keynote: Visualizing and Manipulating Brain Dynamics</strong>. <a href="http://www.cns.atr.jp/~kawato/">Mitsuo
Kawato</a> showed some impressive <a href="https://ieeetv.ieee.org/conference-highlights/cb-exploring-neuroscience-withhumanoid-research-platform?">self-balancing
robots</a>.
It seems that we've <a href="http://tumblr.forgifs.com/post/111425301004/robot-soccer-kick-fail">come a long
way</a>. Most
of what he showed was around deep brain stimulation, artificial cochleas and
retinas, old work but mostly new to me. This is a recent, very impressive and
somewhat terrifying paper on reconstructing low-resolution grayscale <a href="http://neurosurgery.washington.edu/Lectures/science.1234330.full.pdf">imagery
from
dreams</a>.</p>

<h2>Novel input technology</h2>

<p><strong>SoQr: Sonically Quantifying the Content Level inside Containers</strong> is a
convoluted way of determining if you're out of milk. Inspired by acoustically
checking ripeness of watermellons, the idea is to use <a href="https://goo.gl/photos/BVhjSZXyn7MJV2sv7">contact speaker and mic
pair</a> to determine how full a container
is. The method's efficacy depends a lot on the placement of the sensor,
properties of the container, and other environmental factors, like whether or
not any other items are touching the container. Seems overly complex to me, you
could use another approach to reach higher fidelity (eg. a scale). That said,
maybe this can be done very inexpensively?</p>

<p><strong>MagnifiSense: Inferring Device Interaction Using Wrist-Worn Passive
Magneto-Inductive Sensors</strong> is about determining which electronic device is
being used. The idea is to use an inductor coil to detect nearby electromagnetic
radiation. They built their own hardware for the purpose which samples at a very
high frequency (44.1 KHz). They detect <a href="https://goo.gl/photos/RQXQ8fBJpJU4f8wn6">unique EM radiation
patterns</a> for each type of device.
Supposedly they can do the same using a regular smartphone magnetometer, but I'm
very skeptical. They also claim to be able to determine who is using the device,
but that part wasn't very clear from the talk.</p>

<p><strong>DoppleSleep: A Contactless Unobtrusive Sleep Sensing System Using Short-Range
Doppler Radar</strong> uses a 24 GHz doppler radar typically mounted near the bedside
to detect sleep patterns. The benefits are huge: you don't have to wear anything
or instrument the bed. <a href="https://en.wikipedia.org/wiki/Polysomnography">Medical sleep
trackers</a> require
7 electrodes, and <a href="https://en.wikipedia.org/wiki/Actigraphy">consumer ones</a>
don't work well.  They also had a demo where you just sit at your desk and the
doppler tracks your heart rate, breathing rate, as well as more macro
movements. It didn't work as well as I had hoped, but being a research demo, I
remain hopeful!</p>

<p><strong>Activity tracking and indoor positioning with a wearable magnet</strong> was a poster
showing a very cheap way of tracking just by placing magnetometers in strategic
locations and giving the user a magnet. More details <a href="https://goo.gl/photos/2un4nc5nrE7evC4D8">on the
poster</a>.</p>

<p><strong>IDyLL: Indoor Localization using Inertial and Light Sensors on Smartphones</strong>
attempts to solve indoor localization using the light sensor, and lamps as
features for tracking. They extract features from the lights using peak finding,
not absolute intensity. Then they wrote a kalman filter to fuse the IMU and
light-derived features. There's a lot of problems, like needing to have
structured light (eg. in a hallway with a low ceiling), and identify ambiguity
(ie. you're under a light, but which one?).</p>

<p><strong>Monitoring Building Door Events using Barometer Sensor in Smartphones</strong> used
the ubiquitous smartphone barometer, which is currently used to get faster GPS
lock and assist in weather forecasting, to determine if a door opens in a
building. This only works in buildings with HVAC systems, but it was pretty
clever, and they found that it can work reliably, even for multiple doors. Basic
idea <a href="https://goo.gl/photos/yKPbBYPv2RayMrcf6">described in this slide</a>.</p>

<p><strong>ProximityHat - A Head-Worn System for Subtle Sensory Augmentation with Tactile
Stimulation</strong> reminded me of various <a href="http://www.cc.gatech.edu/~acosgun3/papers/cosgun2014guidance.pdf">vibro-tactile belt
projects</a>, and
served a similar purpose: to exploit the sense of touch to give the wearer
another sense. This has many benefits like not blocking other senses. Anyway,
they built a hat and gave it ultrasonic sensors all around and inward-facing
linear actuators, not vibrator motors.  They studied sensitivity around the head
and found high variation around users, and that the forehead was generally less
sensitive. Main application appears to be navigation, and they did some blind
user studies.</p>

<p><strong>Controlling Stiffness with Jamming for Wearable Haptics</strong> makes it easier and
harder to move sliders with the help of a pneumatic bladder, and layered
material. As the bladder inflates, the additional force on the layered material
causes increased friction. Previous layer jamming had low fidelity (binary), so
this is a big improvement. They are currently using sandpaper, so it's unclear
how robust the effect would be over time.</p>

<p><strong>PneuHaptic: Delivering Haptic Cues with a Pneumatic Armband</strong> used a wearable
<a href="https://goo.gl/photos/XkFSKrbRmpiZgzCu9">pneumatic band with 2 pumps and 3
valves</a> to give haptic feedback. This
is a nice alternative to vibrating motors and linear actuators, but not sure how
miniaturizable in practice.</p>

<p><strong>Fast Blur Removal for Wearable QR Code Scanners</strong> is an image processing paper
for improving QR code detection on wearable devices. The proposed method uses
un-blurring techniques which involve predicting the blur direction and applying
de-convolutions. They also use an IMU to better guess the direction of movement.
However <a href="http://picturesofpeoplescanningqrcodes.tumblr.com/">QR codes are dead to
me</a>.</p>

<h2>Gadgets and fads</h2>

<p><strong>Why we use and abandon smart devices</strong> tried to answer the question of why
people abandon their various health and tracking devices so quickly. Basically,
people are motivated by curiosity and novelty, and these health trackers are too
gimmicky. Studies of Fitbit trackers saw majority of them abandoned (65%
abandoned in 2 weeks). Design implications are that encouraging routines
(changing behavior) and minimizing maintenance (charging) are the critical
things.This study had participants come up with a goal, and $1K to buy devices,
so quite contrived given that people didn't even choose to use the devices on
their own, but motivated by a study.</p>

<p>In a less contrived study about the same thing, <strong>No Longer Wearing:
Investigating the Abandonment of Personal Health-Tracking Technologies on
Craigslist</strong> scraped Craigslist for this data. They found that only 25% of
people sell their devices just for abandonment reasons. In many other cases,
they upgrade to something else, or reach their goals. That said, it's very
biased sample, since these people are selling (many just abandon, and don't sell
on CL).</p>

<h2>Machine learning</h2>

<p><strong>DeepEar: Robust Smartphone Audio Sensing in Unconstrained Acoustic
Environments Using Deep Learning</strong> used RNNs to learn whatever sound the user is
interested in. They did an interesting comparison to similar specialized systems
(eg. those that do speaker identification, stress detection, emotion, etc) and
claim to do better. Also, their RNN runs in hardware on a chip, which I thought
was super impressive.</p>

<p>In <strong>Sensor-based Stroke Detection and Stroke Type Classification in Table
Tennis</strong>, the authors instrumented paddles with IMUs and got people to perform
various strokes (in a somewhat controlled environment). They performed stroke
detection through peak recognition and thresholding, and then had a classifier
for stroke type determination. 97% detection and classification rates!
Impressive, but contrived. Wondering how it would do for a full game?</p>

<p><strong>Recognizing New Activities with Limited Training Data</strong> was an interesting
paper about recognizing new activities based on small amounts of labeled data.
Their idea was to leverage "semantic attributes" from core activities to learn
a new activity.  Example: biking is like sitting (body is not changing angle),
running (legs move up and down) and driving (hands are steering). They proposed
an <a href="https://goo.gl/photos/Rn2BbvQjhU3Lhv1K7">Activity-Attribute matrix</a>, and a
cascaded classifier. Problem is that multiple activities can share the same
attributes. So they combine this with a traditional approach.</p>

<p><strong>When Attention is not Scarce - Detecting Boredom from Mobile Phone Usage</strong>
predicted boredom with higher accuracy than I predicted. They collected a ground truth
of boredom data by polling users multiple times a day, asking if they were
bored, and collected activity traces (semantic location, demographics, network
usage, recent number of notifications, sensor data). They managed to detect
boredom with 73% accuracy. They then built an app which sent buzzfeed articles
when bored and compared engagement and click ratio to the random condition.
CTR was 8% for random, 20% when bored, and people were much more engaged.</p>

<h2>Virtual and augmented reality</h2>

<p><strong>Wearing Another Personality: A Human-Surrogate System with a Telepresence
Face</strong> was probably the most bizarre paper at the conference. This work
basically proposes to use a human surrogate instead of a telepresence robot. The
surrogate wears an HMD with pass-through camera feed and a tablet on their face.
The tablet shows the face of the director. The director gets audio and video
feed from the surrogate, and the surrogate gets audio instructions from the
director. They did creepy user studies like going to a city office to get a
public document (friend as surrogate), or meeting your grandmother (mother as
surrogate). Surprisingly, many participants liked being surrogates. The big
technical problem is camera pass through latency. If you go through the whole
Java stack it's something crazy like 300ms. Here's a <a href="https://goo.gl/photos/tZPoR4wvQiDekzg86">video from the
conference</a> to give you a better sense.</p>

<p><strong>Comparing Order Picking Assisted by Head-Up Display versus Pick-by-Light with
Explicit Pick Confirmation</strong> compared two order picking methods in warehouses.
The current method is via digital labels on each tray that count how many items
you're supposed to take from that tray. The new method is to show <a href="https://www.youtube.com/watch?v=yUZFaCP6rP4">which trays
to pick from using augmented
reality</a>. The benefit is that you
don't need an instrumented warehouse, so it's much cheaper. This was interesting
because it was a specific, potentially useful application for a Google
Glass-type device. At the same time, it may be an obsolete problem since aren't
robots supposed to automate that sort of thing pretty soon?</p>

<p><strong>ConductAR: An AR based tool for iterative design of conductive ink circuits</strong>
is a project that validates hand drawn circuits using augmented reality. You
sketch your circuit with a conductive pen, and then the tool takes a picture and
gives you the right voltage drops etc. The presenter showed resistance
calculation (the thicker the line, the more resistive), using <a href="https://goo.gl/photos/wLaMQZqoqf66ap477">a FEM
method</a>. But I wasn't convinced that
this is worthwhile. Sketching circuits should be exploratory and does not need
to be precise, that's sort of the point.</p>

<p><strong>An Approach to User Identification for Head-Mounted Displays</strong> uses blink and
head movements to identify users. They play a particular video and track your
patterns using Google Glass. They extract blinks using IR peaks, and head track
using the IMU. It takes about 30s to verify uniqueness, but not sure how large
their user base is. Results are good: 94% balanced accuracy, and blink features
are most important.</p>

<p><strong>Glass-Physics: Using Google Glass for Physics Experiments</strong> compared using
Google Glass to just a tablet for assisting students doing physics experiments.
The idea is to remove drudgery from data collection. The experiment was to
determine <a href="https://goo.gl/photos/kcxvxouYtDs6CfQv7">effect of fill level in a water glass on frequency of
sound</a> when the vessel was hit with a
fork. A Google Glass app did automatic collection of frequency and of water
level. People liked the wearable version more, but the tablet app involved
manual input. My theory is that a tablet app with AR features to auto-measure
fill level would do as well as an HMD.</p>

<p><strong>WISEglass: Multi-purpose Context-aware Smart Eyeglasses</strong> was like Google
Glass, except without the display. The main contribution was a light sensor on
the bridge of the nose, which could reliably determine when you are at the
computer (from the screen update frequency). Other than that, seems pretty much
the same as wearing an IMU anywhere else (eg. smartwatch).</p>

<h2>E-textiles are impressive</h2>

<p>I saw some nice demos of <a href="https://goo.gl/photos/cui8ucmGYjdXq2tj9">stretch-sensitive fabric
(video)</a>, and <a href="https://goo.gl/photos/HbXp9xK2GtBgcoEo8">pressure/capacitative
fabric (video)</a>. The real question is
where to embed the controller, and what to do about battery life (their stats
were pretty bad). E-textiles are interesting because everybody wears clothing,
which is not true for glasses or watches.</p>

<p><strong>Closing keynote: Behind the scenes</strong> delivered by <a href="http://www.daito.ws/en/">Daito
Manabe</a> was sequentially translated, which was
initially jarring, but the talk was so visually stimulating, it didn't really
matter. Daito walked through a lot of his data arts work, mind-blowingly
impressive art pieces involving drones, 3D graphics, depth cameras, etc. A nice,
if somewhat non-sequitur ending to the conference.</p>

<p>Signing off. Arigatou gozaimasu!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Magnetic Input for Mobile VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/magnetic-input-mobile-vr"/>
    
    <updated>2015-09-07T09:00:00-00:00</updated>
    
    <id>https://smus.com/magnetic-input-mobile-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's easy to do, just follow these steps:</p>

<ol>
<li>Cut two holes in a box</li>
<li>Put your phone in that box</li>
<li>Look inside the box</li>
</ol>

<p><a href="https://www.youtube.com/watch?v=ABrSYqiqvzc&amp;t=1m42s">And that's the way you do it</a>.</p>

<p>Your smartphone is now in a box, so how do you do input? Now that we have a
<a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">paper</a> accepted to <a href="http://iswc.net/">ISWC 2015</a>, I can tell you!</p>

<!--more-->

<h2>It's not easy being in a box</h2>

<p>Let me remind you: your smartphone is still in a box. This means that your
fingers can't reach the touch screen or the volume buttons. Let's consider a
couple of input alternatives:</p>

<ul>
<li>Cameras and microphones require extra app permissions, are inefficient to keep
always on, and may face many false positives.</li>
<li>External electronic devices cost money. Plugging them in and out is a
usability nightmare.</li>
</ul>

<p>Ok, let's nix those. How about permanent magnets? They are inexpensive, robust,
require no power to operate, and do not degrade over time. The vast majority of
smartphones have a magnetometer, which is used for the compass. Intriguing...</p>

<h2>Fun with permanent magnets</h2>

<p>In 2009, Chris Harrison and Scott Hudson published <a href="http://www.chrisharrison.net/index.php/Research/Abracadabra">Abracadabra</a>, a
magnetic ring form factor for finger interactions with small devices:</p>

<p><img src="/magnetic-input-mobile-vr/abra.jpg" alt="Abracadabra" /></p>

<p>In 2010, Hamed Ketabdar and others published <a href="https://www.facebook.com/MagiTact">Magitact</a>, instead using
a magnetic rod for more varied interactions near smartphones:</p>

<p><img src="/magnetic-input-mobile-vr/magitact.jpg" alt="Magitact" /></p>

<p>In 2011, Daniel Ashbrook and others published <a href="http://dl.acm.org/citation.cfm?id=1979238">Nenya</a>, which is similar
to Abracadabra, but focused more on the eyes-free input aspects:</p>

<p><img src="/magnetic-input-mobile-vr/nenya.jpg" alt="Nenya" /></p>

<p>In 2013, Sungjae Hwang and others published <a href="https://www.youtube.com/watch?v=_sSgp0hD-jk">Maggetz</a>, which used
passive magnets to build all sorts of widgets around the device:</p>

<p><img src="/magnetic-input-mobile-vr/maggetz.jpg" alt="Maggetz" /></p>

<h2>Fucking magnets: how do they work?</h2>

<p>Magnets affect the magnetometer in an <a href="https://www.quora.com/Why-does-the-magnetic-field-obey-an-inverse-cube-law">inverse-cubic relationship</a>, so
distance between magnet and magnetometer really makes a dramatic difference in
signal strength. We empirically determined that in most phones, the sensor is
placed at the top of the device, near the earpiece:</p>

<table>
<tr><th>Smartphone Model</th><th>Sensor Location</th></tr>
<tr><td>Moto X</td><td>Top</td></tr>
<tr><td>Nexus 4</td><td>Top</td></tr>
<tr><td>Nexus 5</td><td>Top</td></tr>
<tr><td>Samsung S4</td><td>Top</td></tr>
<tr><td>Galaxy Nexus</td><td>Top</td></tr>
<tr><td>Samsung S3</td><td>Bottom</td></tr>
<tr><td>Moto G</td><td>Bottom</td></tr>
</table>

<p>Some magnetometers are really screwy, like the one found in the first revision
of the HTC M7, or broken, like in some models of the Galaxy Nexus we tested
with. There's little that we can do in these cases, but luckily they are quite
rare.</p>

<p><img src="/magnetic-input-mobile-vr/calibration.png" class="floatright" title="Plot of calibration events"/></p>

<p>The way you access the magnetometer on Android is via the sensor stack,
requesting the <code>TYPE_MAGNETIC_FIELD</code> sensor. This is a calibrated sensor, since
it's primarily used to determine the direction of magnetic north for the
compass. Calibration means that somewhere deep inside Android, software and
hardware periodically calibrates the output of the sensor. When calibration
occurs, magnetometer readings effectively reset to some new coordinate system.</p>

<p>Calibration can happen at any point, and the calibration pattern can look quite
different depending on the device. In some cases, it's a gradual calibration,
not a sudden spike as above. This limitation restricts what we can reliably
detect, which is why we chose a pull-and-release interaction. Android already
has provisions for an uncalibrated magnetometer via
<code>TYPE_MAGNETIC_FIELD_UNCALIBRATED</code>, but this sensor is not nearly as ubiquitous
as its calibrated cousin. Even so, we should be robust to phone insertions and
removals from Cardboard, which can also look like calibration events.</p>

<h2>Magnetic input for VR</h2>

<iframe width="853" height="480" src="//www.youtube.com/embed/a53a-9FLdL8" frameborder="0" allowfullscreen></iframe>

<p><img src="/magnetic-input-mobile-vr/mechanism.png" class="floatright" title="Interaction mechanism"/></p>

<p>As you can see, the interaction involves pulling the magnet downward, and
releasing it. The magnetic ring automatically returns to its rest position
because of the force exerted on it by an internal magnet. The external magnet is
also held in-place by the same force, and while it's possible to pull the magnet
off the cardboard side, it takes concerted effort to do so. The motion of the
magnet is constrained by a cardboard indentation, so it can only move downward.
The thing I find most elegant about this design is that both the digital signal
to the smartphone and the physical mechanism itself relies on the same
principle: magnetism.</p>

<p>We collected a bunch of data for this pull-and-release interaction from many
devices. We found that most devices behave predictably well. Here's a combined
plot of normalized, superimposed positives and negatives from all phones which
we collected data from, with each dimension of the magnetometer vector plotted
separately.</p>

<p><img src="/magnetic-input-mobile-vr/all_features.png" alt="Image of the true positives and negatives from all phones." /></p>

<p>The detector we built was not based on a template learned from all of the data
above, but a simpler state machine based on thresholding. The thresholds
themselves were learned empirically. Here's the simple state machine:</p>

<p><img src="/magnetic-input-mobile-vr/state_machine.png" alt="State machine of the detector" /></p>

<p>The basic idea is that we take a sliding window approach, normalizing all of the
data relative to the last value in the window. For each window, we calculate <code>min_1</code>,
which is the smallest value of the first half of the window, and <code>max_2</code>, the
largest value in the second half. Next, we compare to empirically determined
thresholds perform the appropriate transition in the state machine. I won't bore
you with details of normalizing the data, etc but you can find all of the
details in <a href="/magnetic-input-mobile-vr//magnetic-input-mobile-vr/mimvr-iswc-2015.pdf">the paper</a>. Oh, and all of the code is also available <a href="https://github.com/dodger487/MIST">on
github</a>.</p>

<h2>What's next?</h2>

<p><img src="/magnetic-input-mobile-vr/joystick.jpg" class="floatright" title="Hypothetical magnetic joystick"/></p>

<p>A lot more can be done using passive magnetic input. With uncalibrated
magnetometers, there is no fear of calibration events, so we could implement a
faster detector based on just the down motion of the magnet. We could reliably
detect long presses and double clicks. Alternatively, extensions to the existing
input can be implemented by simply changing the geometry of the physical
constraints, such as a joystick form factor.</p>

<p>I'm incredibly happy that Cardboard has been doing so well. Thanks to the great
team working so hard on it, there are now <a href="http://techcrunch.com/2015/05/28/google-has-shipped-over-1-million-cardboard-vr-units/">over 1 million units shipped</a>.
The press has been happy with it too, with kind reviews from many tech
publications.</p>

<p><a href="http://techcrunch.com/2014/06/25/hands-on-with-googles-incredibly-clever-cardboard-virtual-reality-headset/">Techcrunch</a> said:</p>

<blockquote>
  <p>This funny little cardboard faux-Rift has something even the original Rift
  itself does not: a built-in button. Your phone is able to sense the magnet’s
  movement, allowing it to act as a ridiculously clever little button. Yeesh.</p>
</blockquote>

<p><a href="http://www.techradar.com/news/phone-and-communications/mobile-phones/google-cardboard-everything-you-need-to-know-1277738">Techradar</a> said:</p>

<blockquote>
  <p>What's also somewhat amazing is the magnet on the side. […] The little magnet
  on the side is actually a quite ingenious design aspect of Google Cardboard.
  It's a button!</p>
</blockquote>

<p><a href="http://www.engadget.com/2014/12/10/google-cardboard/">Engadget</a> said:</p>

<blockquote>
  <p>One of the things I liked most was a switch located on the left temple, which
  consists of just a couple of magnets and a metal ring.</p>
</blockquote>

<p><a href="http://www.google.com/get/cardboard/downloads/wwgc_manufacturers_kit_v2.0.zip">Future versions of Cardboard</a> are switching to a different input method
using a conductive button which brings your body's capacitance to the screen,
similar to how a touch stylus works. It's cheaper without them, and the new
input works well, but I'll definitely miss the magnets!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Site redesign, version five</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/design-v5"/>
    
    <updated>2015-06-10T09:00:00-00:00</updated>
    
    <id>https://smus.com/design-v5</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>It's been over three years since the design of this site has been
updated. Time to change that!</p>

<p><img src="/design-v5/0days.jpg" class="floatright"/></p>

<p>This is the fifth revision of this site's design. Looking over <a href="/redesign-2012">previous
designs</a>, I've been happier with minimal designs,
especially <a href="/redesign-2012/v2.png">this one</a> from 2012. I was inspired
by many excellent designs such as <a href="http://practicaltypography.com/typography-in-ten-minutes.html">Butterick's Practical
Typography</a>,
<a href="http://www.teehanlax.com/">Teehan+Lax</a>, <a href="http://www.erikjohanssonphoto.com/">Erik
Johansson</a>,
<a href="https://medium.com/designing-medium/death-to-typewriters-9b7712847639">Medium</a>
and <a href="http://frankchimero.com/writing/other-halves">Frank Chimero</a>.</p>

<p>The new design is visually cleaner. I <a href="http://caniuse.com/#feat=flexbox">use
flexbox</a> in many places, which makes
the CSS far more intuitive. The responsive parts are very simple,
consisting of just ten CSS declarations.</p>

<!--more-->

<p>Rather than subjecting readers to my face on every page, I have a simple
stipple background on the <a href="/about">about page</a>, which I created using the
complex but functional <a href="http://www.evilmadscientist.com/2012/stipplegen2/">StippleGen</a>.</p>

<p>Also, I've started working on a self-hosted visual link blog that you
can check out in under <a href="/inspiration">inspiring clippings</a>. I've
implemented a companion Chrome extension that makes it super easy to
clip inspiring content from anywhere on the web and bring it to that
page.</p>

<p><strong>Typography:</strong> I'm continuing to use Google fonts, which seems to be so
much simpler to use than various competitors. I have not completely
optimized my selection of fonts, but this is satisfactory given my
belief that no design is ever finished. <a href="http://alistapart.com/article/improving-ux-through-front-end-performance">Performance is UX</a> too,
and aesthetic decisions need to be counterbalanced by mundane
considerations like page load time.  Unfortunately <a href="https://www.google.com/fonts/specimen/Dosis">Dosis</a> didn't
make the cut.</p>

<p><strong>Infrastructure:</strong> This site is still built using the <a href="https://github.com/borismus/lightning">lightning static
blog</a> engine, which I'm continuing to improve. On that front,
I've dropped the ambitious goal of being able to edit content from any
device using dropbox, since in practice I always author on my laptop.
Instead, the focus has been on optimizing the edit flow for the local
offline case, and I have built <a href="https://github.com/lepture/python-livereload">livereload</a> into the local preview
server. As far as hosting, I have conceded to GitHub Pages, and have
migrated away from using S3 directly.</p>

<p><strong>Thanks:</strong> to <a href="https://twitter.com/mikemartin604">Mike</a>,
<a href="https://twitter.com/paul_irish">Paul</a>,
<a href="https://twitter.com/smattyang">Seungho</a>,
<a href="https://twitter.com/scottjenson">Scott</a>,
<a href="https://twitter.com/mahemoff">Michael</a>, and other awesome friends that
gave me excellent design suggestions and found bugs!</p>

<p>While I appreciate companies like <a href="http://medium.com">Medium</a> and
<a href="http://svbtle.com/">Svbtle</a> advancing the aesthetics of the web, I
completely <a href="http://practicaltypography.com/billionaires-typewriter.html">agree with Matthew Butterick</a>'s view, and will
continue self-hosting my writings for as long as possible. Long live the
plurality of the web!</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Spatial audio and web VR</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/spatial-audio-web-vr"/>
    
    <updated>2015-03-19T09:00:00-00:00</updated>
    
    <id>https://smus.com/spatial-audio-web-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>Last summer I visited Austria, the capital of classical music. I had the
pleasure of hearing the <a href="http://en.wikipedia.org/wiki/Vespro_della_Beata_Vergine">Vespers of 1610</a> in the great
<a href="http://goo.gl/e3WBB2">Salzburger Dom (photosphere)</a>. The most memorable part of
the piece was that the soloists moved between movements, so their voices
and instruments emanated from surprising parts of the great hall.
Inspired, I returned to the west coast and eventually came around to
building a spatial audio prototypes like this one:</p>

<p><a href="http://borismus.github.io/moving-music"><img src="/spatial-audio-web-vr/collage_small.jpg" alt="Screenshot of a demo" /></a></p>

<p>Spatial audio is an important part of any good VR experience, since the
more senses we simulate, the more compelling it feels to our sense
fusing mind. WebVR, WebGL, and WebAudio all act as complementary specs
to enable this necessary experience. As you would expect, because it
uses the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>, this demo can be viewed on
mobile, desktop, in Cardboard or an Oculus Rift. In all cases, you will
need headphones :)</p>

<!--more-->

<h2>Early spatial music</h2>

<p>One of the things that made my acoustic experience in the Salzburg Dom
so memorable was the beauty of the space in which it was performed. The
potential for awesome sound was staggering, with one massive organ at
the back, and four smaller organs surrounding the nave. During the
performance of the vespers, the thing that struck me the most was that
as the piece transitioned from movement to movement, choreographed
soloists also moved around the cathedral, resulting in haunting acoustic
effects. Sometimes, a voice would appear quietly from the far end of the
cloister, sounding distant and muffled. Other times, it would come from
the balcony behind the audience, full of unexpected reverb. It was a
truly unique acoustic experience that I will never forget, and it made
me wonder about the role of space in music.</p>

<p>As it turns out, there is a rich history on the <a href="http://en.wikipedia.org/wiki/Spatial_music">topic of spatialization
in music</a> going back to the 16th century. For the
purposes of this blog, I am more interested in the present day. In
particular, given the <a href="http://www.w3.org/TR/webaudio/">excellent state of audio APIs</a> on the
web, what follows is a foray into spatial audio with WebVR.</p>

<h2>Experiments in spatial audio</h2>

<p>How does music sound if in addition to pitch, rhythm and timbre, we
could tweak position and velocity as additional expressive dimension?
My demo places you into a virtual listening space, that you look
around into (using whatever means you have available: mouse and
keyboard, mobile phone gyroscope, or cardboard-like device) -- thanks to
<a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a>. Each track is visualized as a blob of
particles. These animate according to the instantaneous amplitude of the
track, serving as a per-track visualizer and indicating where the track
is in space.</p>

<p>There is a surprising amount of multi-track music out there, such as
<a href="http://www.cambridge-mt.com/ms-mtk.htm">Cambridge Music Technology's raw recordings archive</a> for aspiring
audio engineers and <a href="https://soundcloud.com/search/sets?q=stems">soundcloud stems</a> which are typically
recorded separately in a studio and then released publicly for <a href="http://www.remixcomps.com/">remix
contests</a>. In the end, I went with a few different sets just to
get a feeling for spatializing a variety of tracks:</p>

<ul>
<li>Multiple people <a href="http://borismus.github.io/moving-music/?set=speech">speaking (demo)</a> simultaneously (the <a href="http://en.wikipedia.org/wiki/Cocktail_party_effect">cocktail party effect</a>).</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=jazz">live (demo)</a> recording with many mic'ed instruments.</li>
<li>Multi-track <a href="http://borismus.github.io/moving-music/?set=phoenix">studio recording (demo)</a> with cleanly recorded tracks (A <a href="http://en.wikipedia.org/wiki/Phoenix_%28band%29">Phoenix</a> track).</li>
</ul>

<p>In addition to selecting the sounds to spatialize, the demo supports
laying out the tracks in various formations. To cycle between these
modes, hit space on desktop, or tap the screen on mobile:</p>

<ul>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=0">clumped together (demo)</a> in one place.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=1">positioned around the observer (demo)</a>.</li>
<li>Sources are <a href="http://borismus.github.io/moving-music/?mode=2">moving around the observer (demo)</a>.</li>
</ul>

<p>Given that the code is <a href="https://github.com/borismus/moving-music">open sourced on github</a>, it's pretty
easy to try your own tracks, implement new trajectories or change the
visualizer. Please fork away!</p>

<h2>Implementation details</h2>

<p>In an attempt to eat my own dogfood, this project partly serves as a way
to test the <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> to make sure that
it is usable, and provides the functionality that it purports to. I've
made a bunch of changes to the boilerplate in parallel, fixing browser
compatibility issues and resolving bugs. Notable improvements since
inception include the ability to mouselook via <a href="http://www.html5rocks.com/en/tutorials/pointerlock/intro/">pointer
lock</a> in regular desktop mode and improved support for iOS
and Firefox nightly. Thanks to <a href="http://www.antoniocosta.eu/">Antonio</a>, my awesome designer
colleague, the WebVR boilerplate has a new icon!</p>

<p>This project relies heavily on audio, but requires the page to be
running in the foreground for you to enjoy the immersive nature of the
experience. Browsers, especially on mobile devices, can have some weird
behaviors when it comes to backgrounded tabs. It's a safe bet to just
prevent this from happening altogether, so I've been using the <a href="http://www.smashingmagazine.com/2015/01/20/creating-sites-with-the-page-visibility-api/">page
visibility API</a> to mute the music when the tab goes out of
focus, and then resume it when it's back in focus. This works super well
across browsers I've tested in and prevents the page-hunt where you're
trying to find which annoying tab/activity/app is playing!</p>

<p>I toyed a little bit with the doppler effect, but found it to be
terrible for music. Because in the moving case, each track moves with
its own velocity relative to the viewer, frequency shifts are
non-uniform, leading to a cacophany of out-of-tune instruments. For
spoken word, it worked quite well, though. The caveat to all this is that the
current <a href="http://crbug.com/439644">doppler API is deprecated</a>, so I
didn't delve too deeply into doppler until we have a new implementation.</p>

<h2>Pitfalls and workarounds</h2>

<p><strong>Set your listener's up vector properly.</strong> Something you should beware
of is to always set the up vector correctly in the
<code>listener.setOrientation(...)</code> call. Initially, I was only setting the
direction vector, keeping up fixed at <code>(0, 1, 0)</code>, but this yielded
unpredictable results and took a long time to track down.</p>

<p><strong>Streaming is broken in mobile implementations.</strong> A couple of issues
related to loading audio bit me as I was developing, proving to be
nearly show stoppers (please star if you feel strongly):</p>

<ul>
<li>Streaming audio doesn't work on Android (or iOS). This means that
every track we play needs to be first loaded, and then decoded:
<a href="http://crbug.com/419446">http://crbug.com/419446</a></li>
<li>Decoding mp3 on Android takes a very very long time (same in Firefox):
<a href="http://crbug.com/232973">http://crbug.com/232973</a></li>
<li>Though it doesn't directly affect my spatial sound experiments, the
inability to bring in remote WebRTC audio streams into the audio graph
is blocking other ideas: <a href="https://crbug.com/121673">https://crbug.com/121673</a></li>
</ul>

<p>I tried to work around the streaming issue by doing my own chunking
locally and then writing a <code>ChunkedAudioPlayer</code>, but this is harder than
it seems, especially when you want to synchronize multiple chunked
tracks.</p>

<p><strong>Beware of implementation differences.</strong> It's also worth noting that
different browsers have slightly different behaviors when it comes to
PannerNodes. In particular, Firefox spatialization can appear to sound
better, but this is simply because it's louder (the same effect can be
replicated in Chrome by just increasing gain). Also, on iOS, it seems
that the spatialization effect is weaker -- potentially because they are
using a different HRTF, or maybe they are just panning.</p>

<p><strong>Spatialization effect can be subtle.</strong> I found that there wasn't
enough oomph to the effect provided by WebAudio's HRTF. Perhaps it is
acoustically correct, but it just wasn't obvious or compelling enough as
is. I had to fudge the situation slightly, and implement a sound cone
for the observer, so that sources that are within the field of view got
a slight gain boost.</p>

<h2>Parting words and links</h2>

<p><a href="http://www.ee.usyd.edu.au/people/philip.leong/UserFiles/File/papers/errors_hr97.pdf">The nature and distribution of errors in sound localization</a>
is a seminal paper from 1997, giving a thorough psychoacoustic analysis
on our hearing limits. In this web audio context, however, it is unclear
how much of this perceptual accuracy is lost due to variations in
headphone style and quality, and software implementation details.  To
truly bring my Austrian cathedral experience to the web, we would
probably need a personalized HRTF, and also a more sophisticated room
model that could simulate reflections from the walls of the building.
This is concievable on the web in the near future, especially with the
prospect of the <a href="https://plus.sandbox.google.com/+ChrisWilson/posts/QapzKucPp6Y">highly anticipated</a> <a href="http://webaudio.github.io/web-audio-api/#audio-worker-examples">AudioWorker</a>.</p>

<p>Let me conclude by linking you to a couple more spatial audio demos:</p>

<ul>
<li>Ilmari wrote an <a href="http://www.html5rocks.com/en/tutorials/webaudio/positional_audio/">html5rocks post</a> a while back about three.js
and the Web Audio API.</li>
<li>Mozilla built the <a href="https://hacks.mozilla.org/2013/10/songs-of-diridum-pushing-the-web-audio-api-to-its-limits/">Songs of Diridum demo</a>, showing a cute
spatialized jazz band.</li>
<li>Arturo recently emailed me, showing me a <a href="http://inspirit.unboring.net/">pretty compelling WebVR +
Audio project</a>, in the spirit of WebVR.</li>
<li>Not web-based, but a painstakingly recorded and tweaked <a href="https://www.youtube.com/watch?v=IUDTlvagjJA">binaural
haircut simulation</a> just to illustrate the potential.</li>
</ul>


        
      </div>
      ]]>
    </content>
  </entry>
  
  <entry>
    <title>Responsive WebVR, headset optional</title>
    <author><name>Boris Smus</name></author>
    <link href="https://smus.com/responsive-vr"/>
    
    <updated>2015-02-02T09:00:00-00:00</updated>
    
    <id>https://smus.com/responsive-vr</id>
    <content type="html">
      <![CDATA[
      <div>
        <p>VR on the web threatens to cleave the web platform in twain, like mobile
did before it. The solution then and the solution now is <a href="http://en.wikipedia.org/wiki/Responsive_web_design">Responsive Web
Design</a>, which websites to scale well for all form factors.
Similarly, for VR to succeed on the web, we need to figure out how to
make VR experiences that work both in any VR headset, and also without a
VR headset at all.</p>

<p><img src="/responsive-vr/hmds.png" alt="Various head mounted displays." /></p>

<p><a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate</a> is a new starting point for building
responsive web VR experiences that work on popular VR headsets and
degrace gracefully on other platforms. Check out a couple of demos, <a href="http://borismus.github.io/webvr-boilerplate/">a
simple one</a> and one <a href="http://borismus.github.io/sechelt">ported from MozVR</a>.</p>

<!--more-->

<h2>Preview the VR experience for everyone</h2>

<p>Say you visit a webpage, and it opens up in split-screen mode barrel
distortion, chromatic aberration correction, personalized interpupillary
distance, and provides 6 DOF tracking. <a href="https://www.youtube.com/watch?v=PkFyGNjaQ8k">Whoa</a>. You reach for your
VR headset only to find that you forgot it at work! How disappointing!
The vast majority of normal people with no head mounted display lying
around will surely be even more disappointed.</p>

<p>Responsive web design promises content which automatically adapts to
your viewing environment by using fluid layouts, flexible images,
proportional grids; a cocktail of modern web technologies. Similarly,
WebVR experiences need to work even without VR hardware. This has two
obvious advantages:</p>

<ol>
<li>The vast majority of people that don't have VR hardware can still get
a feeling for the experience.</li>
<li>Even if you have VR gear, donning it is a pain. This preview lets you
quickly evaluate whether or not wearing is worth the hassle.</li>
</ol>

<p>What are some reasonable fallbacks to the in-helmet VR experience? The
main question boils down to emulating head tracking without wearing
anything on your head. On mobile phones, the obvious answer is to use
the gyroscope, for a <a href="http://googlespotlightstories.com/">Spotlight Stories</a>-type experience. On
desktop, we use the mouse to free-look, and also support turning using
the arrow keys. This covers enough of the 3DOF orientation that all HMDs
provide. Clearly missing are the three translational degrees of freedom,
but these are provided only by some VR headsets, and we can imagine some
<a href="http://topheman.github.io/parallax/">interesting fallbacks</a> for those too.</p>

<h2>Write once, run in any VR headset</h2>

<p>Remember the old "write once, run anywhere" promise? The web is the
closest thing we have to fulfilling it, but what it actually delivers is
often far from this ideal. The latest VR wave has barely begun and
already the web VR world is fragmented. Case in point,
<a href="http://vr.chromeexperiments.com/">vr.chromeexperiments.com</a> don't work on Oculus, and
<a href="http://mozvr.com/">mozvr.com</a> demos don't work in Cardboard.  The promise of WebVR
is that once it lands, all will be well in the world. However, this
means that we need to wait for WebVR to become fully baked. In other
words, we are blocked on the valiant efforts of our dear <a href="https://twitter.com/vvuk">WebVR</a>
<a href="https://twitter.com/Tojiro">implementers</a> (as well as the heavyweight browser development
process consisting of spec authors, security reviews, binary size, etc).</p>

<p>To speed up the process, we need a polyfill for WebVR which uses web
APIs to provide functionality to the WebVR specification (currently, in
<a href="https://github.com/vvuk/gecko-dev/blob/oculus/dom/webidl/VRDevice.webidl">IDL form</a>). In the absence of a WebVR implementation, the polyfill
kicks in and supports mobile VR headsets like Cardboard and Durovis
Dive, which are passive contraptions that just piggyback on the
smartness found in your smartphone.</p>

<h2>Introducing: WebVR Boilerplate</h2>

<p>The <a href="https://github.com/borismus/webvr-boilerplate">WebVR boilerplate project</a> is on github, and consists of
two parts. Firstly, the <a href="https://github.com/borismus/webvr-polyfill">WebVR polyfill</a> provides WebVR
support for Cardboard-compatible devices, and orientation tracking
fallbacks where no headset is available. The WebVR polyfill can also be
installed from npm (available via <code>npm install webvr-polyfill</code>).</p>

<ol>
<li>A <code>CardboardHMDDevice</code>, provides a reasonable default for
interpupillary distance and field of view for cardboard-like devices.</li>
<li>On mobile devices, a <code>GyroPositionSensorVRDevice</code>, which provides
orientation through the <code>DeviceOrientationEvent</code>.</li>
<li>On PCs, a <code>MouseKeyboardPositionSensorVRDevice</code>, which provides
orientation through keyboard and mouse events.</li>
</ol>

<p>It is designed to be used with <a href="http://threejs.org/">THREE.js</a> plugins that are built
for the WebVR API: <a href="https://github.com/mrdoob/three.js/blob/master/examples/js/controls/VRControls.js">VRControls.js</a> and
<a href="https://github.com/mrdoob/three.js/blob/master/examples/js/effects/VREffect.js">VREffect.js</a>, but of course any valid usage of the WebVR API
should work (modulo bugs).</p>

<p>Secondly, the <a href="https://github.com/borismus/webvr-boilerplate/blob/master/js/webvr-manager.js">WebVR manager</a> surfaces VR compatibility
using consistent iconography and simplifies transitioning in and out of
full VR mode. It also contains some of the best practices for making VR
work on the web, for example, using orientation lock to keep the phone
in landscape orientation, and a means of keeping the phone screen on. If
you're ready to dive in, the <a href="https://github.com/borismus/webvr-boilerplate">github has all of the technical
information</a> available.</p>

<p>WebVR boilerplate is meant to make it easy to develop immersive
experiences that run on all VR hardware, including Oculus and Cardboard,
and also provide reasonable fallbacks when no specialized viewer is
available.</p>

<h2>WebVR boilerplate in action</h2>

<p><img src="/responsive-vr/sechelt.png" alt="Screenshot of the mozvr.com Sechelt demo." /></p>

<p>I really liked Mozilla's <a href="http://mozvr.com/projects/sechelt/">Sechelt demo</a>, inspired by the
eponymous town on British Columbia's beautiful Sunshine Coast. I've
<a href="http://borismus.github.io/sechelt">ported it</a> to WebVR Boilerplate. The result is the same
demo, which works in Cardboard, as well as continuing to work on desktop
and mobile devices, and on the Oculus Rift via <a href="https://nightly.mozilla.org/">Firefox Nightly</a> and
<a href="https://drive.google.com/folderview?id=0BzudLt22BqGRbW9WTHMtOWMzNjQ&amp;usp=sharing#list">Brandon's WebVR Chrome builds</a>. It also yielded less confusing
boilerplate code and a <a href="https://github.com/borismus/sechelt/commit/671cff9fc283b58d2ebce0ae6f0dfa3580050aab">simplified code base</a> since
there is no longer need for an unweildy conditional to determine whether
to use <code>DeviceOrientationControls</code>, <code>OrbitControls</code>, or <code>VRControls</code>,
and decide between <code>VREffect</code> and <code>StereoEffect</code>.</p>

<p>As always, <a href="https://twitter.com/borismus">let me know what you think</a> and feel free to point
out (or fix!) my mistakes on <a href="https://github.com/borismus/webvr-boilerplate">the github</a>.</p>


        
      </div>
      ]]>
    </content>
  </entry>
  
</feed>